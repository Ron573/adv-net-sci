
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Appendix &#8212; Advanced Topics in Network Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'tmp/m09';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.jpg" class="logo__image only-light" alt="Advanced Topics in Network Science - Home"/>
    <script>document.write(`<img src="../_static/logo.jpg" class="logo__image only-dark" alt="Advanced Topics in Network Science - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../home.html">
                    Welcome to SSIE 641 Advanced Topics on Network Science
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Intro</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro/why-networks.html">Why should we care networks?</a></li>

<li class="toctree-l1"><a class="reference internal" href="../intro/zoo-of-networks.html">Zoo of networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intro/setup.html">Trouble shooting</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M01: Euler Tour</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m01-euler_tour/what-to-learn.html">Module 1: Euler Tour</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-euler_tour/puzzle.html">A puzzle</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-euler_tour/euler-path.html">Euler’s solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-euler_tour/how-to-code-network.html">Compute with networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m01-euler_tour/coding-exercise.html">Exercise</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M02: Small World</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/what-to-learn.html">Module 2: Small-world</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/small-world-experiment.html">Small-world experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/wikirace.html">Wikirace</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/pen-and-paper.html">Why is our social network small world?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/connectedness.html">Walks, Trails, Paths, and Connectedness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/which-tools.html">Toolbox for network analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/compressed-sparse-row.html">Efficient representation for large sparse networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/connectedness-hands-on.html">Computing the Shortest Paths and Connected Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/assignment.html">Assignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m02-small-world/appendix.html">Appendix</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M03: Robustness</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m03-robustness/what-to-learn.html">Module 3: Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-robustness/exercise-power-grid.html">Building a cost-effective power grid network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-robustness/minimum-spanning-tree.html">Minimum spanning tree</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-robustness/robustness.html">Network Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-robustness/robustness-hands-on.html">Hands-on: Robustness (Random attack)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-robustness/percolation.html">Percolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m03-robustness/appendix.html">Appendix</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M04: Friendship Paradox</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m04-friendship-paradox/what-to-learn.html">Module 4: Friendship Paradox</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-friendship-paradox/experiment.html">In-class experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-friendship-paradox/friendship-paradox.html">Friendship Paradox</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-friendship-paradox/vaccination-game.html">Vaccination Game</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m04-friendship-paradox/degree-distribution.html">Degree distribution</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M05: Clustering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/what-to-learn.html">Module 5: Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/what-is-community.html">What is community?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/pen-and-paper.html">Pen and Paper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/pattern-matching.html">Community detection (pattern matching)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/graph-cut.html">Graph cut</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/ratio-normalized-cut.html">Balanced cut</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/modularity.html">Modularity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/modularity-02.html">Modularity (Cont.)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/stochastic-block-model.html">Stochastic Block Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m05-clustering/exercise-clustering.html">Hands-on: Clustering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M06: Centrality</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m06-centrality/what-to-learn.html">Module 6: Centrality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m06-centrality/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m06-centrality/degree-distance-based-centrality.html">What is centrality?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m06-centrality/eigencentrality.html">Centralities based on centralities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m06-centrality/hands-on.html">Computing centrality with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m06-centrality/assignment.html">Assignment</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M07: Random Walks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m07-random-walks/what-to-learn.html">Module 7: Random Walks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m07-random-walks/amida-kuji.html">Ladder Lottery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m07-random-walks/random-walks.html">Random Walks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m07-random-walks/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m07-random-walks/random-walks-code.html">Random Walks in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m07-random-walks/random-walks-math.html">Characteristics of Random Walks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m07-random-walks/unifying-centrality-and-communities.html">Random walks unify centrality and communities</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M08: Embedding</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/what-to-learn.html">Module 8: Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/spectral-embedding.html">Spectral Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/word2vec.html">word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/graph-embedding-w-word2vec.html">Graph embedding with word2vec</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/spectral-vs-neural-embedding.html">Spectral vs Neural Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/software.html">Software for Network Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m08-embedding/appendix.html">Appendix</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">M09: Graph Neural Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../m09-graph-neural-networks/what-to-learn.html">Module 9: Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m09-graph-neural-networks/pen-and-paper.html">Pen and paper exercises</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m09-graph-neural-networks/image-processing.html">Preliminaries: Image Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m09-graph-neural-networks/from-image-to-graph.html">From Image to Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m09-graph-neural-networks/graph-convolutional-network.html">Graph Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m09-graph-neural-networks/popular-gnn.html">Popular Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../m09-graph-neural-networks/appendix.html">Appendix</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/skojaku/adv-net-sci/gh-pages?urlpath=tree/docs/lecture-note/tmp/m09.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/skojaku/adv-net-sci" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/skojaku/adv-net-sci/issues/new?title=Issue%20on%20page%20%2Ftmp/m09.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/tmp/m09.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/tmp/m09.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Appendix</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#brunas-spectral-gcn">Bruna’s Spectral GCN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chebnet">ChebNet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#from-image-to-graph">From Image to Graph</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-between-image-and-graph-data">Analogy between image and graph data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-filter-on-graphs">Spectral filter on graphs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-filtering">Spectral Filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-high-pass-filter-increases-the-contrast-of-the-eigenvector-centrality-emphasizing-the-differences-between-nodes-on-the-other-hand-the-low-pass-filter-smooths-out-the-eigenvector-centrality">The high-pass filter increases the contrast of the eigenvector centrality, emphasizing the differences between nodes. On the other hand, the low-pass filter smooths out the eigenvector centrality.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolutional-networks">Graph Convolutional Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-graph-convolutional-networks">Spectral Graph Convolutional Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-spectral-to-spatial">From Spectral to Spatial</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">ChebNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolutional-networks-by-kipf-and-welling">Graph Convolutional Networks by Kipf and Welling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-order-approximation">First-order Approximation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-gcns-can-suffer-from-over-smoothing">Deep GCNs can suffer from over-smoothing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-transform">Fourier Transform</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-for-the-fourier-transform">An example for the Fourier transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-transform-of-images">Fourier Transform of Images</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-of-fourier-transform">An example of Fourier transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-key-lesson-from-image-processing">A key lesson from image processing</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#pen-and-paper-exercises">Pen and paper exercises</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-graph-neural-networks">Popular Graph Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphsage-sample-and-aggregate">GraphSAGE: Sample and Aggregate</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-ideas">Key Ideas</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#neighborhood-sampling">Neighborhood Sampling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aggregation">Aggregation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-attention-networks-gat-differentiate-individual-neighbors">Graph Attention Networks (GAT): Differentiate Individual Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-isomorphism-network-gin-differentiate-the-aggregation">Graph Isomorphism Network (GIN): Differentiate the Aggregation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weisfeiler-lehman-test">Weisfeiler-Lehman Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gin">GIN</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#module-9-graph-neural-networks">Module 9: Graph Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-to-learn-in-this-module">What to learn in this module</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Link to this heading">#</a></h1>
<section id="brunas-spectral-gcn">
<h2>Bruna’s Spectral GCN<a class="headerlink" href="#brunas-spectral-gcn" title="Link to this heading">#</a></h2>
<p>Let’s first implement Bruna’s spectral GCN.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">scipy.sparse.linalg</span> <span class="k">as</span> <span class="nn">slinalg</span>

<span class="k">class</span> <span class="nc">BrunaGraphConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Bruna&#39;s Spectral Graph Convolution Layer</span>

<span class="sd">    This implementation follows the original formulation by Joan Bruna et al.,</span>
<span class="sd">    using the eigendecomposition of the graph Laplacian for spectral convolution.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Bruna Graph Convolution layer</span>

<span class="sd">        Args:</span>
<span class="sd">            in_features (int): Number of input features</span>
<span class="sd">            out_features (int): Number of output features</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BrunaGraphConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span> <span class="o">=</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span>

        <span class="c1"># Learnable spectral filter parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">n_nodes</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Initialize parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize weights using Glorot initialization&quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">get_laplacian_eigenvectors</span><span class="p">(</span><span class="n">adj</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute eigendecomposition of the normalized graph Laplacian</span>

<span class="sd">        Args:</span>
<span class="sd">            adj: Adjacency matrix</span>

<span class="sd">        Returns:</span>
<span class="sd">            eigenvalues, eigenvectors of the normalized Laplacian</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compute normalized Laplacian</span>
        <span class="c1"># Add self-loops</span>
        <span class="n">adj</span> <span class="o">=</span> <span class="n">adj</span> <span class="o">+</span> <span class="n">sp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">adj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Compute degree matrix</span>
        <span class="n">deg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">adj</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">Dsqrt_inv</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">deg</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

        <span class="c1"># Compute normalized Laplacian: D^(-1/2) A D^(-1/2)</span>
        <span class="n">laplacian</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">adj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">Dsqrt_inv</span> <span class="o">@</span> <span class="n">adj</span> <span class="o">@</span> <span class="n">Dsqrt_inv</span>

        <span class="c1"># Compute eigendecomposition</span>
        <span class="c1"># Using k=adj.shape[0]-1 to get all non-zero eigenvalues</span>
        <span class="n">eigenvals</span><span class="p">,</span> <span class="n">eigenvecs</span> <span class="o">=</span> <span class="n">slinalg</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">laplacian</span><span class="o">.</span><span class="n">tocsc</span><span class="p">(),</span> <span class="n">k</span><span class="o">=</span><span class="n">adj</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">which</span><span class="o">=</span><span class="s1">&#39;SM&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">eigenvals</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">eigenvecs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass implementing Bruna&#39;s spectral convolution</span>

<span class="sd">        Args:</span>
<span class="sd">            x: Input features [num_nodes, in_features]</span>
<span class="sd">            eigenvecs: Eigenvectors of the graph Laplacian [num_nodes, num_nodes-1]</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output features [num_nodes, out_features]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Transform to spectral domain</span>
        <span class="n">x_spectral</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">eigenvecs</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># [num_nodes-1, in_features]</span>

        <span class="c1"># Initialize output tensor</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># For each input-output feature pair</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">):</span>
                <span class="c1"># Element-wise multiplication in spectral domain</span>
                <span class="c1"># This is the actual spectral filtering operation</span>
                <span class="n">filtered</span> <span class="o">=</span> <span class="n">x_spectral</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># [num_spectrum]</span>

                <span class="c1"># Transform back to spatial domain and accumulate</span>
                <span class="n">out</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">eigenvecs</span><span class="p">,</span> <span class="n">filtered</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>Next, we will train the model on the karate club network to predict the given node labels indicating nodes’ community memberships. We load the data by</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load karate club network</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">karate_club_graph</span><span class="p">()</span>
<span class="n">adj</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_scipy_sparse_array</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">number_of_nodes</span><span class="p">())</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;club&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;Officer&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">()],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We apply the convolution twice with ReLu activation in between. This can be implemented by preparing two independent <code class="docutils literal notranslate"><span class="pre">BrunaGraphConv</span></code> layers, applying them consecutively, and adding a ReLu activation in between.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a simple GCN model</span>
<span class="k">class</span> <span class="nc">SimpleGCN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleGCN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">BrunaGraphConv</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">BrunaGraphConv</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We then train the model by</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Get eigenvectors of the Laplacian</span>
<span class="n">eigenvals</span><span class="p">,</span> <span class="n">eigenvecs</span> <span class="o">=</span> <span class="n">BrunaGraphConv</span><span class="o">.</span><span class="n">get_laplacian_eigenvectors</span><span class="p">(</span><span class="n">adj</span><span class="p">)</span>

<span class="c1"># Initialize the model</span>
<span class="n">hidden_features</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">input_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">output_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_nodes</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">number_of_nodes</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleGCN</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">)</span>

<span class="c1"># Train the model</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Split the data into training and testing sets</span>
<span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">number_of_nodes</span><span class="p">()),</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">test_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>


<span class="n">n_train</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_features</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">[</span><span class="n">train_idx</span><span class="p">,</span> <span class="p">:])</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Evaluate the model</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_features</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">[</span><span class="n">test_idx</span><span class="p">,</span> <span class="p">:])</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">test_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_train</span><span class="si">}</span><span class="s1">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100, Loss: 0.6933, Accuracy: 0.4286
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 25/100, Loss: 0.1456, Accuracy: 0.4286
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 50/100, Loss: 0.0056, Accuracy: 0.4286
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 75/100, Loss: 0.0020, Accuracy: 0.4286
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 100/100, Loss: 0.0013, Accuracy: 0.4286
</pre></div>
</div>
</div>
</div>
<p>Observe that the accuracy increases as the training progresses. We can use the model to predict the labels.
The model has a hidden layer, and let’s visualize the data in the hidden space.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="c1"># Visualize the learned embeddings</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">xy</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span> <span class="o">=</span> <span class="n">xy</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;tab10&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Learned Node Embeddings&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/3359ff3af1823b935103b8d1d14b9121513923df25a814af2c3c74303af887e3.png" src="../_images/3359ff3af1823b935103b8d1d14b9121513923df25a814af2c3c74303af887e3.png" />
</div>
</div>
</section>
<section id="chebnet">
<h2>ChebNet<a class="headerlink" href="#chebnet" title="Link to this heading">#</a></h2>
<p>Let’s implement the ChebNet layer.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>


<span class="k">def</span> <span class="nf">sparse_mx_to_torch_sparse</span><span class="p">(</span><span class="n">sparse_mx</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert scipy sparse matrix to torch sparse tensor.&quot;&quot;&quot;</span>
    <span class="n">sparse_mx</span> <span class="o">=</span> <span class="n">sparse_mx</span><span class="o">.</span><span class="n">tocoo</span><span class="p">()</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">sparse_mx</span><span class="o">.</span><span class="n">row</span><span class="p">,</span> <span class="n">sparse_mx</span><span class="o">.</span><span class="n">col</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">sparse_mx</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">(</span><span class="n">sparse_mx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo_tensor</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ChebConv</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Chebyshev Spectral Graph Convolutional Layer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ChebConv</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>

        <span class="c1"># Trainable parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">out_channels</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize parameters.&quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_normalize_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute normalized Laplacian L = I - D^(-1/2)AD^(-1/2)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Convert to scipy if it&#39;s not already</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">sp</span><span class="o">.</span><span class="n">isspmatrix</span><span class="p">(</span><span class="n">adj_matrix</span><span class="p">):</span>
            <span class="n">adj_matrix</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">adj_matrix</span><span class="p">)</span>

        <span class="n">adj_matrix</span> <span class="o">=</span> <span class="n">adj_matrix</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>

        <span class="c1"># Compute degree matrix D</span>
        <span class="n">rowsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">adj_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">d_inv_sqrt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">rowsum</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">d_inv_sqrt</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">d_inv_sqrt</span><span class="p">)]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">d_mat_inv_sqrt</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="n">d_inv_sqrt</span><span class="p">)</span>

        <span class="c1"># Compute L = I - D^(-1/2)AD^(-1/2)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">adj_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">d_mat_inv_sqrt</span> <span class="o">@</span> <span class="n">adj_matrix</span> <span class="o">@</span> <span class="n">d_mat_inv_sqrt</span>
        <span class="k">return</span> <span class="n">L</span>

    <span class="k">def</span> <span class="nf">_scale_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Scale Laplacian eigenvalues to [-1, 1] interval</span>
<span class="sd">        L_scaled = 2L/lambda_max - I</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Compute largest eigenvalue</span>
            <span class="n">eigenval</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigsh</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;LM&quot;</span><span class="p">,</span> <span class="n">return_eigenvectors</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">lambda_max</span> <span class="o">=</span> <span class="n">eigenval</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="c1"># Approximate lambda_max = 2 if eigenvalue computation fails</span>
            <span class="n">lambda_max</span> <span class="o">=</span> <span class="mf">2.0</span>

        <span class="n">n</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">L_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">lambda_max</span><span class="p">)</span> <span class="o">*</span> <span class="n">L</span> <span class="o">-</span> <span class="n">sp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">L_scaled</span>

    <span class="k">def</span> <span class="nf">chebyshev_basis</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">L_sparse</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute Chebyshev polynomials basis up to order K.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># List to store Chebyshev polynomials</span>
        <span class="n">cheb_polynomials</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># T_0(L) = I</span>
        <span class="n">cheb_polynomials</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># T_1(L) = L</span>
            <span class="n">X_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">L_sparse</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
            <span class="n">cheb_polynomials</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X_1</span><span class="p">)</span>

        <span class="c1"># Recurrence T_k(L) = 2L·T_{k-1}(L) - T_{k-2}(L)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">):</span>
            <span class="n">X_k</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">L_sparse</span><span class="p">,</span> <span class="n">cheb_polynomials</span><span class="p">[</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
                <span class="o">-</span> <span class="n">cheb_polynomials</span><span class="p">[</span><span class="n">k</span> <span class="o">-</span> <span class="mi">2</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="n">cheb_polynomials</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X_k</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">cheb_polynomials</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [K, num_nodes, in_channels]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">:</span> <span class="n">sp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass.</span>

<span class="sd">        Args:</span>
<span class="sd">            X: Node features tensor of shape [num_nodes, in_channels]</span>
<span class="sd">            adj_matrix: Adjacency matrix in scipy sparse format</span>

<span class="sd">        Returns:</span>
<span class="sd">            Output tensor of shape [num_nodes, out_channels]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Compute normalized and scaled Laplacian</span>
        <span class="n">L_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_normalize_laplacian</span><span class="p">(</span><span class="n">adj_matrix</span><span class="p">)</span>
        <span class="n">L_scaled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_laplacian</span><span class="p">(</span><span class="n">L_norm</span><span class="p">)</span>

        <span class="c1"># Convert to torch sparse tensor</span>
        <span class="n">L_scaled</span> <span class="o">=</span> <span class="n">sparse_mx_to_torch_sparse</span><span class="p">(</span><span class="n">L_scaled</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Compute Chebyshev polynomials basis</span>
        <span class="n">Tx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">chebyshev_basis</span><span class="p">(</span><span class="n">L_scaled</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>  <span class="c1"># [K, num_nodes, in_channels]</span>

        <span class="c1"># Perform convolution using learned weights</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;kni,kio-&gt;no&quot;</span><span class="p">,</span> <span class="n">Tx</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We stack the layers to form a simple GCN model.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ChebNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ChebNet model for node classification</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hidden_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ChebNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">convs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>

        <span class="c1"># First layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ChebConv</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

        <span class="c1"># Hidden layers</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ChebConv</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">hidden_channels</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

        <span class="c1"># Output layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ChebConv</span><span class="p">(</span><span class="n">hidden_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">:</span> <span class="n">sp</span><span class="o">.</span><span class="n">spmatrix</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through all layers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">conv</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Output layer</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">X</span><span class="p">,</span> <span class="n">adj_matrix</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>Let’s train the model on the karate club network.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>

<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Load karate club network</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">karate_club_graph</span><span class="p">()</span>
<span class="n">adj</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">to_scipy_sparse_array</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">number_of_nodes</span><span class="p">())</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[</span><span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;club&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Officer&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">G</span><span class="o">.</span><span class="n">nodes</span><span class="p">()],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span>
<span class="p">)</span>

<span class="c1"># Initialize the model</span>
<span class="n">hidden_features</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">input_features</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">output_features</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_nodes</span> <span class="o">=</span> <span class="n">G</span><span class="o">.</span><span class="n">number_of_nodes</span><span class="p">()</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ChebNet</span><span class="p">(</span>
    <span class="n">input_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Train the model</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="c1"># Split the data into training and testing sets</span>
<span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">number_of_nodes</span><span class="p">()),</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>
<span class="n">train_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">train_idx</span><span class="p">]</span>
<span class="n">test_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
<span class="n">test_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>


<span class="n">n_train</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_train</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">adj</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">train_labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Evaluate the model</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">adj</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="n">test_idx</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">test_labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">n_train</span><span class="si">}</span><span class="s2">, Loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/100, Loss: 0.7020, Accuracy: 0.2857
Epoch 25/100, Loss: 0.1083, Accuracy: 0.8571
Epoch 50/100, Loss: 0.0111, Accuracy: 0.8571
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 75/100, Loss: 0.0019, Accuracy: 0.8571
Epoch 100/100, Loss: 0.0047, Accuracy: 0.8571
</pre></div>
</div>
</div>
</div>
<p>Let’s visualize the learned embeddings.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># Get embeddings from the last hidden layer</span>
    <span class="n">X_hidden</span> <span class="o">=</span> <span class="n">features</span>
    <span class="k">for</span> <span class="n">conv</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">convs</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">X_hidden</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">X_hidden</span><span class="p">,</span> <span class="n">adj</span><span class="p">)</span>
        <span class="n">X_hidden</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">X_hidden</span><span class="p">)</span>

<span class="c1"># Reduce dimensionality for visualization</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_hidden</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">y</span><span class="o">=</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
    <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;tab10&quot;</span><span class="p">,</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Learned Node Embeddings&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/093cd4874473623cf246a1a83d33d870318b4cb88037f77b321c759af0ad536a.png" src="../_images/093cd4874473623cf246a1a83d33d870318b4cb88037f77b321c759af0ad536a.png" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3">
<h2>jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3<a class="headerlink" href="#jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3" title="Link to this heading">#</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="from-image-to-graph">
<h1>From Image to Graph<a class="headerlink" href="#from-image-to-graph" title="Link to this heading">#</a></h1>
<section id="analogy-between-image-and-graph-data">
<h2>Analogy between image and graph data<a class="headerlink" href="#analogy-between-image-and-graph-data" title="Link to this heading">#</a></h2>
<p>We can think of a convolution of an image from the perspective of networks.
In the convolution of an image, a pixel is convolved with its <em>neighbors</em>. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.</p>
<p><img alt="" src="https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp" /></p>
<p>Building on this analogy, we can extend the idea of convolution to general graph data.
Each node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph.
This is the key idea of graph convolutional networks.
But, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the “kernel” for graph convolution.</p>
</section>
<section id="spectral-filter-on-graphs">
<h2>Spectral filter on graphs<a class="headerlink" href="#spectral-filter-on-graphs" title="Link to this heading">#</a></h2>
<p>Just like we can define a convolution on images in the frequency domain, we can also define a ‘‘frequency domain’’ for graphs.</p>
<p>Consider a network of <span class="math notranslate nohighlight">\(N\)</span> nodes, where each node has a feature variable <span class="math notranslate nohighlight">\({\mathbf x}_i \in \mathbb{R}\)</span>. We are interested in:</p>
<div class="math notranslate nohighlight">
\[
J = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(A_{ij}\)</span> is the adjacency matrix of the graph. The quantity <span class="math notranslate nohighlight">\(J\)</span> represents <em>the total variation</em> of <span class="math notranslate nohighlight">\(x\)</span> between connected nodes; a small <span class="math notranslate nohighlight">\(J\)</span> means that connected nodes have similar <span class="math notranslate nohighlight">\(x\)</span> (low variation; low frequency), while a large <span class="math notranslate nohighlight">\(J\)</span> means that connected nodes have very different <span class="math notranslate nohighlight">\(x\)</span> (high variation; high frequency).</p>
<p>We can rewrite <span class="math notranslate nohighlight">\(J\)</span> as</p>
<div class="math notranslate nohighlight">
\[
J = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\bf x}^\top {\bf L} {\bf x},
\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf L}\)</span> is the Laplacian matrix of the graph given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{ij} = \begin{cases}
-1 &amp; \text{if } i \text{ and } j \text{ are connected} \\
k_i &amp; \text{if } i = j \\
0 &amp; \text{otherwise}
\end{cases}.
\end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\({\bf x} = [x_1,x_2,\ldots, x_N]^\top\)</span> is a column vector of feature variables.</p>
<div class="dropdown admonition">
<p class="admonition-title">Detailed derivation</p>
<p>The above derivation shows that the total variation of <span class="math notranslate nohighlight">\(x\)</span> between connected nodes is proportional to <span class="math notranslate nohighlight">\({\bf x}^\top {\bf L} {\bf x}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
J &amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\
&amp;= \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N \underbrace{A_{ij}\left( x_i^2 +x_j^2\right)}_{\text{symmetric}} - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j \\
&amp;= \sum_{i=1}^Nx_i^2\underbrace{\sum_{j=1}^N A_{ij}}_{\text{degree of node } i, k_i} - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j \\
&amp;= \sum_{i=1}^Nx_i^2 k_i - \sum_{i=1}^N\sum_{j=1}^N A_{ij}x_ix_j \\
&amp;= \underbrace{[x_1,x_2,\ldots, x_N]}_{{\bf x}} \underbrace{\begin{bmatrix} k_1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; k_2 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; k_N \end{bmatrix}}_{{\bf D}} \underbrace{\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix}}_{{\bf x}} - 2\underbrace{\sum_{i=1}^N\sum_{j=1}^N A_{ij}}_{{\bf x}^\top {\mathbf A} {\bf x}} {\bf x} \\
&amp;= {\bf x}^\top {\bf D} {\bf x} - {\bf x}^\top {\mathbf A} {\bf x} \\
&amp;= {\bf x}^\top {\bf L} {\bf x},
\end{aligned}
\end{split}\]</div>
</div>
<p>Let us showcase the analogy between the Fourier transform and the Laplacian matrix.
In the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation <span class="math notranslate nohighlight">\(J\)</span> into eigenvector bases.</p>
<div class="math notranslate nohighlight">
\[
J = \sum_{i=1}^N \lambda_i  {\bf x}^\top {\mathbf u}_i {\mathbf u}_i^\top {\bf x} = \sum_{i=1}^N \lambda_i  ||{\bf x}^\top {\mathbf u}_i||^2.
\]</div>
<p>where <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span> is the eigenvector corresponding to the eigenvalue <span class="math notranslate nohighlight">\(\lambda_i\)</span>.</p>
<ul class="simple">
<li><p>The term <span class="math notranslate nohighlight">\(({\bf x}^\top {\mathbf u}_i)\)</span> is a dot-product between the feature vector <span class="math notranslate nohighlight">\({\bf x}\)</span> and the eigenvector <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span>, which measures how much <span class="math notranslate nohighlight">\({\bf x}\)</span> <em>coheres</em> with eigenvector <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span>, similar to how Fourier coefficients measure coherency with sinusoids.</p></li>
<li><p>Each <span class="math notranslate nohighlight">\(||{\bf x}^\top {\mathbf u}_i||^2\)</span> is the ‘‘strength’’ of <span class="math notranslate nohighlight">\({\bf x}\)</span> with respect to the eigenvector <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span>, and the total variation <span class="math notranslate nohighlight">\(J\)</span> is a weighted sum of these strengths.</p></li>
</ul>
<p>Some eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation <span class="math notranslate nohighlight">\(J\)</span> for an eigenvector <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
J = \frac{1}{2} \sum_{j}\sum_{\ell} A_{j\ell}(u_{ij} - u_{i\ell})^2 = {\mathbf u}_i^\top {\mathbf L} {\mathbf u}_i = \lambda_i.
\]</div>
<p>This equation provides key insight into the meaning of eigenvalues:</p>
<ol class="arabic simple">
<li><p>For an eigenvector <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span>, its eigenvalue <span class="math notranslate nohighlight">\(\lambda_i\)</span> measures the total variation for <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span>.</p></li>
<li><p>Large eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).</p></li>
</ol>
<p>Thus, if <span class="math notranslate nohighlight">\({\bf x}\)</span> aligns well with <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span> with a large <span class="math notranslate nohighlight">\(\lambda_i\)</span>, then <span class="math notranslate nohighlight">\({\bf x}\)</span> has a strong high-frequency component; if <span class="math notranslate nohighlight">\({\bf x}\)</span> aligns well with <span class="math notranslate nohighlight">\({\mathbf u}_i\)</span> with a small <span class="math notranslate nohighlight">\(\lambda_i\)</span>, then <span class="math notranslate nohighlight">\({\bf x}\)</span> has strong low-frequency component.</p>
<section id="spectral-filtering">
<h3>Spectral Filtering<a class="headerlink" href="#spectral-filtering" title="Link to this heading">#</a></h3>
<p>Eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span> can be thought of as a <em>filter</em> that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter <span class="math notranslate nohighlight">\(h(\lambda_i)\)</span> to control which frequency components pass through. This leads to the idea of <em>spectral filtering</em>. Two common filters are:</p>
<ol class="arabic simple">
<li><p><strong>Low-pass Filter</strong>:
$<span class="math notranslate nohighlight">\(h_{\text{low}}(\lambda) = \frac{1}{1 + \alpha\lambda}\)</span>$</p>
<ul class="simple">
<li><p>Preserves low frequencies (small λ)</p></li>
<li><p>Suppresses high frequencies (large λ)</p></li>
<li><p>Results in smoother signals</p></li>
</ul>
</li>
<li><p><strong>High-pass Filter</strong>:
$<span class="math notranslate nohighlight">\(h_{\text{high}}(\lambda) = \frac{\alpha\lambda}{1 + \alpha\lambda}\)</span>$</p>
<ul class="simple">
<li><p>Preserves high frequencies</p></li>
<li><p>Suppresses low frequencies</p></li>
<li><p>Emphasizes differences between neighbors</p></li>
</ul>
</li>
</ol>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/4f8ad2e95bb844b5887d5d55053435d248c98042abf4727d6784dc191e07bf7f.png" src="../_images/4f8ad2e95bb844b5887d5d55053435d248c98042abf4727d6784dc191e07bf7f.png" />
</div>
</div>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>Let us showcase the idea of spectral filtering with a simple example with the karate club network.</p>
<div class="cell tag_remove-input docutils container">
</div>
<p>We will first compute the laplacian matrix and its eigendecomposition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute Laplacian matrix</span>
<span class="n">deg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">diags</span><span class="p">(</span><span class="n">deg</span><span class="p">)</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">D</span> <span class="o">-</span> <span class="n">A</span>

<span class="c1"># Compute eigendecomposition</span>
<span class="n">evals</span><span class="p">,</span> <span class="n">evecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>

<span class="c1"># Sort eigenvalues and eigenvectors</span>
<span class="n">order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">evals</span><span class="p">)</span>
<span class="n">evals</span> <span class="o">=</span> <span class="n">evals</span><span class="p">[</span><span class="n">order</span><span class="p">]</span>
<span class="n">evecs</span> <span class="o">=</span> <span class="n">evecs</span><span class="p">[:,</span> <span class="n">order</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s create a low-pass and high-pass filter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">L_low</span> <span class="o">=</span> <span class="n">evecs</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">evals</span><span class="p">))</span> <span class="o">@</span> <span class="n">evecs</span><span class="o">.</span><span class="n">T</span>
<span class="n">L_high</span> <span class="o">=</span> <span class="n">evecs</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">evals</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">evals</span><span class="p">))</span> <span class="o">@</span> <span class="n">evecs</span><span class="o">.</span><span class="n">T</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of low-pass filter:&quot;</span><span class="p">,</span> <span class="n">L_low</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of high-pass filter:&quot;</span><span class="p">,</span> <span class="n">L_high</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size of low-pass filter: (34, 34)
Size of high-pass filter: (34, 34)
</pre></div>
</div>
</div>
</div>
<p>Notice that the high-pass filter and low-pass filter are matrices of the same size as the adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>, which defines a ‘convolution’ on the graph as follows:</p>
<div class="math notranslate nohighlight">
\[
{\bf x}' = {\bf L}_{\text{low}} {\bf x} \quad \text{or} \quad {\bf x}' = {\bf L}_{\text{high}} {\bf x}.
\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf L}_{\text{low}}\)</span> and <span class="math notranslate nohighlight">\({\bf L}_{\text{high}}\)</span> are the low-pass and high-pass filters, respectively, and <span class="math notranslate nohighlight">\({\bf x}'\)</span> is the convolved feature vector.</p>
<p>Now, let’s see how these filters work. Our first example is a random feature vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Random feature vector</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Convolve with low-pass filter</span>
<span class="n">x_low</span> <span class="o">=</span> <span class="n">L_low</span> <span class="o">@</span> <span class="n">x</span>

<span class="c1"># Convolve with high-pass filter</span>
<span class="n">x_high</span> <span class="o">=</span> <span class="n">L_high</span> <span class="o">@</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<p>Let us visualize the results.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=-</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Original</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">values</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="p">[</span><span class="n">palette</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">],</span> <span class="n">bbox</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original&quot;</span><span class="p">)</span>

<span class="c1"># Low-pass filter applied</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">L_low</span> <span class="o">@</span> <span class="n">x</span>
<span class="n">values</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="p">[</span><span class="n">palette</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">],</span> <span class="n">bbox</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Low-pass filter&quot;</span><span class="p">)</span>

<span class="c1"># High-pass filter applied</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">L_high</span> <span class="o">@</span> <span class="n">x</span>
<span class="n">values</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="p">[</span><span class="n">palette</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">],</span> <span class="n">bbox</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;High-pass filter&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/1a2d54b7b3485c5d6463901775a67ebef46d5289bfaec0077e7a686e07424348.png" src="../_images/1a2d54b7b3485c5d6463901775a67ebef46d5289bfaec0077e7a686e07424348.png" />
</div>
</div>
<p>We observe that the low-pass filter results in smoother <span class="math notranslate nohighlight">\({\bf x}\)</span> between connected nodes (i.e., neighboring nodes have similar <span class="math notranslate nohighlight">\({\bf x}\)</span>).
The original <span class="math notranslate nohighlight">\({\bf x}\)</span> and <span class="math notranslate nohighlight">\({\bf x}'_{\text{low}}\)</span> are very similar because random variables are high-frequency components. In contrast, when we apply the high-pass filter, <span class="math notranslate nohighlight">\({\bf x}'_{\text{high}}\)</span> is similar to <span class="math notranslate nohighlight">\({\bf x}\)</span> because the high-frequency components are not filtered.</p>
<p>Let’s now use an eigenvector as our feature vector <span class="math notranslate nohighlight">\({\bf x}\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eigen_centrality</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">G</span><span class="o">.</span><span class="n">eigenvector_centrality</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">low_pass_eigen</span> <span class="o">=</span> <span class="n">L_low</span> <span class="o">@</span> <span class="n">eigen_centrality</span>
<span class="n">high_pass_eigen</span> <span class="o">=</span> <span class="n">L_high</span> <span class="o">@</span> <span class="n">eigen_centrality</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">palette</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">norm</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=-</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">eigen_centrality</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="c1"># high_pass_random.reshape(-1)</span>
<span class="n">values</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="p">[</span><span class="n">palette</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">],</span> <span class="n">bbox</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original&quot;</span><span class="p">)</span>

<span class="n">values</span> <span class="o">=</span> <span class="n">low_pass_eigen</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">values</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="p">[</span><span class="n">palette</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">],</span> <span class="n">bbox</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Low-pass filter&quot;</span><span class="p">)</span>

<span class="n">values</span> <span class="o">=</span> <span class="n">high_pass_eigen</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">values</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="n">ig</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">vertex_color</span><span class="o">=</span><span class="p">[</span><span class="n">palette</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">values</span><span class="p">],</span> <span class="n">bbox</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">),</span> <span class="n">vertex_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;High-pass filter&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/890b05c4dc1b5105df9b05be8233cd7e6d1d5f5092e8ee4c9b6c798dffba07b4.png" src="../_images/890b05c4dc1b5105df9b05be8233cd7e6d1d5f5092e8ee4c9b6c798dffba07b4.png" />
</div>
</div>
</section>
</section>
<section id="the-high-pass-filter-increases-the-contrast-of-the-eigenvector-centrality-emphasizing-the-differences-between-nodes-on-the-other-hand-the-low-pass-filter-smooths-out-the-eigenvector-centrality">
<h2>The high-pass filter increases the contrast of the eigenvector centrality, emphasizing the differences between nodes. On the other hand, the low-pass filter smooths out the eigenvector centrality.<a class="headerlink" href="#the-high-pass-filter-increases-the-contrast-of-the-eigenvector-centrality-emphasizing-the-differences-between-nodes-on-the-other-hand-the-low-pass-filter-smooths-out-the-eigenvector-centrality" title="Link to this heading">#</a></h2>
</section>
<section id="id1">
<h2>jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="graph-convolutional-networks">
<h1>Graph Convolutional Networks<a class="headerlink" href="#graph-convolutional-networks" title="Link to this heading">#</a></h1>
<p>We have seen that spectral filters give us a principled way to think about “convolution” on irregular graph structures, and controlling the frequency components brings out different aspects of the data. We now go one step further: instead of designing filters by hand, we can learn them from data for specific tasks.</p>
<section id="spectral-graph-convolutional-networks">
<h2>Spectral Graph Convolutional Networks<a class="headerlink" href="#spectral-graph-convolutional-networks" title="Link to this heading">#</a></h2>
<p>A simplest form of learnable spectral filter is given by</p>
<div class="math notranslate nohighlight">
\[
{\bf L}_{\text{learn}} = \sum_{k=1}^K \theta_k {\mathbf u}_k {\mathbf u}_k^\top,
\]</div>
<p>where <span class="math notranslate nohighlight">\({\mathbf u}_k\)</span> are the eigenvectors and <span class="math notranslate nohighlight">\(\theta_k\)</span> are the learnable parameters. The variable <span class="math notranslate nohighlight">\(K\)</span> is the number of eigenvectors used (i.e., the rank of the filter). The weight <span class="math notranslate nohighlight">\(\theta_k\)</span> is learned to maximize the performance of the task at hand.</p>
<p>Building on this idea, <a class="footnote-reference brackets" href="#footcite-bruna2014spectral" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN) by</p>
<div class="math notranslate nohighlight">
\[
{\bf x}^{(\ell+1)} = h\left( L_{\text{learn}} {\bf x}^{(\ell)}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(h\)</span> is an activation function, and <span class="math notranslate nohighlight">\({\bf x}^{(\ell)}\)</span> is the feature vector of the <span class="math notranslate nohighlight">\(\ell\)</span>-th convolution. They further extend this idea to convolve on multidimensional feature vectors, <span class="math notranslate nohighlight">\({\bf X} \in \mathbb{R}^{N \times f_{\text{in}}}\)</span> to produce new feature vectors of different dimensionality, <span class="math notranslate nohighlight">\({\bf X}' \in \mathbb{R}^{N \times f_{\text{out}}}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
{\bf X}^{(\ell+1)}_i &amp;= h\left( \sum_j L_{\text{learn}}^{(i,j)} {\bf X}^{(\ell)}_j\right),\quad \text{where} \quad L^{(i,j)}_{\text{learn}} = \sum_{k=1}^K \theta_{k, (i,j)} {\mathbf u}_k {\mathbf u}_k^\top,
\end{aligned}
\]</div>
<p>Notice that the learnable filter <span class="math notranslate nohighlight">\(L_{\text{learn}}^{(i,j)}\)</span> is defined for each pair of input <span class="math notranslate nohighlight">\(i\)</span> and output <span class="math notranslate nohighlight">\(j\)</span> dimensions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Many GCNs simple when it comes to implementation despite the complicated formula. And this is one of my ways to learn GNNs. Check out the <a class="reference internal" href="#appendix.md"><span class="xref myst">Appendix for the Python implementation</span></a>.</p>
</div>
</section>
<section id="from-spectral-to-spatial">
<h2>From Spectral to Spatial<a class="headerlink" href="#from-spectral-to-spatial" title="Link to this heading">#</a></h2>
<p>Spectral GCNs are mathematically elegant but have two main limitations:</p>
<ol class="arabic simple">
<li><p><strong>Computational Limitation</strong>: Computing the spectra of the Laplacian is expensive <span class="math notranslate nohighlight">\({\cal O}(N^3)\)</span> and prohibitive for large graphs</p></li>
<li><p><strong>Spatial Locality</strong>: The learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.</p></li>
</ol>
<p>These two limitations motivate the development of spatial GCNs.</p>
<section id="id3">
<h3>ChebNet<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>ChebNet <a class="footnote-reference brackets" href="#footcite-defferrard2016convolutional" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> is one of the earliest spatial GCNs that bridges the gap between spectral and spatial domains.
The key idea is to leverage Chebyshev polynomials to approximate <span class="math notranslate nohighlight">\({\bf L}_{\text{learn}}\)</span> by</p>
<div class="math notranslate nohighlight">
\[
{\bf L}_{\text{learn}} \approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{{\bf L}}), \quad \text{where} \quad \tilde{{\bf L}} = \frac{2}{\lambda_{\text{max}}}{\bf L} - {\bf I},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{{\bf L}}\)</span> is the scaled and normalized Laplacian matrix in order to have eigenvalues in the range of <span class="math notranslate nohighlight">\([-1,1]\)</span>. The Chebyshev polynomials <span class="math notranslate nohighlight">\(T_k(\tilde{{\bf L}})\)</span> transforms the eigenvalues <span class="math notranslate nohighlight">\(\tilde{{\bf L}}\)</span> to the following recursively:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
T_0(\tilde{{\bf L}}) &amp;= {\bf I} \\
T_1(\tilde{{\bf L}}) &amp;= \tilde{{\bf L}} \\
T_k(\tilde{{\bf L}}) &amp;= 2\tilde{{\bf L}} T_{k-1}(\tilde{{\bf L}}) - T_{k-2}(\tilde{{\bf L}})
\end{aligned}
\end{split}\]</div>
<p>We then replace <span class="math notranslate nohighlight">\({\bf L}_{\text{learn}}\)</span> in the original spectral GCN with the Chebyshev polynomial approximation:</p>
<div class="math notranslate nohighlight">
\[
{\bf x}^{(\ell+1)} = h\left( \sum_{k=0}^{K-1} \theta_k T_k(\tilde{{\bf L}}){\bf x}^{(\ell)}\right),
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T_k(\tilde{{\bf L}})\)</span> applies the k-th Chebyshev polynomial to the scaled Laplacian matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_k\)</span> are the learnable parameters</p></li>
<li><p>K is the order of the polynomial (typically small, e.g., K=3)</p></li>
</ul>
</section>
<section id="graph-convolutional-networks-by-kipf-and-welling">
<h3>Graph Convolutional Networks by Kipf and Welling<a class="headerlink" href="#graph-convolutional-networks-by-kipf-and-welling" title="Link to this heading">#</a></h3>
<p>While ChebNet offers a principled way to approximate spectral convolutions, Kipf and Welling (2017) <a class="footnote-reference brackets" href="#footcite-kipf2017semi" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> proposed an even simpler and highly effective variant called <strong>Graph Convolutional Networks (GCN)</strong>.</p>
<section id="first-order-approximation">
<h4>First-order Approximation<a class="headerlink" href="#first-order-approximation" title="Link to this heading">#</a></h4>
<p>The key departure is to use the first-order approximation of the Chebyshev polynomials.</p>
<div class="math notranslate nohighlight">
\[
g_{\theta'} * x \approx \theta'_0x + \theta'_1(L - I_N)x = \theta'_0x - \theta'_1D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x
\]</div>
<p>This is crude approximation but it leads to a much simpler form, leaving only two learnable parameters, instead of <span class="math notranslate nohighlight">\(K\)</span> parameters in the original ChebNet.</p>
<p>Additionally, they further simplify the formula by using the same <span class="math notranslate nohighlight">\(\theta\)</span> for both remaining parameters (i.e., <span class="math notranslate nohighlight">\(\theta_0 = \theta\)</span> and <span class="math notranslate nohighlight">\(\theta_1 = -\theta\)</span>). The result is the following convolutional filter:</p>
<div class="math notranslate nohighlight">
\[
g_{\theta} * x \approx \theta(I_N + D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x
\]</div>
<p>While this is a very simple filter, one can stack multiple layers of convolutions to perform high-order graph convolutions.</p>
</section>
<section id="deep-gcns-can-suffer-from-over-smoothing">
<h4>Deep GCNs can suffer from over-smoothing<a class="headerlink" href="#deep-gcns-can-suffer-from-over-smoothing" title="Link to this heading">#</a></h4>
<p>GCN models can be deep, and when they are too deep, they start suffering from an ill-posed problem called <em>gradient vanishing/exploding</em>, where the gradients of the loss function becomes too small or too large to update the model parameters. It is a common problem in deep learning.</p>
<p>To facilitate the training of deep GCNs, the authors introduce a very simple trick called <em>renormalization</em>. The idea is to add self-connections to the graph:</p>
<div class="math notranslate nohighlight">
\[
\tilde{A} = A + I_N, \quad \text{and} \quad \tilde{D}_{ii} = \sum_j \tilde{A}_{ij}
\]</div>
<p>And use <span class="math notranslate nohighlight">\(\tilde{A}\)</span> and <span class="math notranslate nohighlight">\(\tilde{D}\)</span> to form the convolutional filter.</p>
<p>Altogether, this leads to the following layer-wise propagation rule:</p>
<div class="math notranslate nohighlight">
\[X^{(\ell+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X^{(\ell)}W^{(\ell)})\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X^{(\ell)}\)</span> is the matrix of node features at layer <span class="math notranslate nohighlight">\(\ell\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(W^{(\ell)}\)</span> is the layer’s trainable weight matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span> is a nonlinear activation function (e.g., ReLU)</p></li>
</ul>
<p>These simplifications offer several advantages:</p>
<ul class="simple">
<li><p><strong>Efficiency</strong>: Linear complexity in number of edges</p></li>
<li><p><strong>Localization</strong>: Each layer only aggregates information from immediate neighbors</p></li>
<li><p><strong>Depth</strong>: Fewer parameters allow building deeper models</p></li>
<li><p><strong>Performance</strong>: Despite (or perhaps due to) its simplicity, it often outperforms more complex models</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">Exercise</p>
<p>Let’s implement a simple GCN model for node classification.
<span class="xref myst">Coding Exercise</span></p>
</div>
<div class="docutils container" id="id6">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-bruna2014spectral" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In Yoshua Bengio and Yann LeCun, editors, <em>2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings</em>. 2014. URL: <a class="reference external" href="http://arxiv.org/abs/1312.6203">http://arxiv.org/abs/1312.6203</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-defferrard2016convolutional" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. <em>Advances in neural information processing systems</em>, 2016.</p>
</aside>
<aside class="footnote brackets" id="footcite-kipf2017semi" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">3</a><span class="fn-bracket">]</span></span>
<p>Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In <em>International Conference on Learning Representations (ICLR)</em>. 2017.</p>
</aside>
</aside>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In the previous example, we used a <span class="math notranslate nohighlight">\(3 \times 3\)</span> kernels called the Prewitt operator, which in terms of <span class="math notranslate nohighlight">\(K\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K_h = \begin{bmatrix}
-1 &amp; 0 &amp; 1 \\
-1 &amp; 0 &amp; 1 \\
-1 &amp; 0 &amp; 1
\end{bmatrix}
\quad \text{or} \quad
K_v = \begin{bmatrix}
-1 &amp; -1 &amp; -1 \\
0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 1
\end{bmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(K_h\)</span> is the horizontal Prewitt operator and <span class="math notranslate nohighlight">\(K_v\)</span> is the vertical Prewitt operator.</p>
</div>
<p>A kernel represents a local pattern we want to detect. The new pixel value after the convolution is maximized when the pattern is most similar to the kernel in terms of the inner product. This can be confirmed by:</p>
<div class="math notranslate nohighlight">
\[
\nabla Z_{22} = \sum_{i=-1}^1 \sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} Z_{2+i, 2+j} = \langle \hat K, Z \rangle
\]</div>
<p>where <span class="math notranslate nohighlight">\(\langle \cdot, \cdot \rangle\)</span> is the inner product, and <span class="math notranslate nohighlight">\(\hat K\)</span> is the order-reversed kernel.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check out this awesome interactive demo to see how different kernels work: <a class="reference external" href="https://setosa.io/ev/image-kernels/">Demo</a></p>
</div>
</section>
</section>
</section>
<section id="fourier-transform">
<h2>Fourier Transform<a class="headerlink" href="#fourier-transform" title="Link to this heading">#</a></h2>
<p><img alt="" src="https://miro.medium.com/v2/resize:fit:1400/1*D6iRfzDkz-sEzyjYoVZ73w.gif" /></p>
<p>Convolution computes the new pixel values by sliding a kernel over an image. How is the resulting image related to the original image?</p>
<p>To answer this question, let us consider a row of an image and convolve it with a kernel <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
X &amp;= \begin{bmatrix}
X_1 &amp; X_2 &amp; X_3 &amp; X_4 &amp; X_5 &amp; X_6
\end{bmatrix} \\
K &amp;= \begin{bmatrix}
K_1 &amp; K_2 &amp; K_3
\end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>The convolution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(K\)</span> is</p>
<div class="math notranslate nohighlight">
\[
X * K = \begin{bmatrix}
X_1 K_3 + X_2 K_2 + X_3 K_1 &amp; X_2 K_3 + X_3 K_2 + X_4 K_1 &amp; X_3 K_3 + X_4 K_2 + X_5 K_1 &amp; X_4 K_3 + X_5 K_2 + X_6 K_1
\end{bmatrix}
\]</div>
<p>…which is complicated, right? 😅 So let’s make it simple by using a useful theorem called <strong>the convolution theorem</strong>.</p>
<p>The convolution theorem gives us a simpler way to think about convolution. Instead of doing the complex sliding window operation in the original domain (like pixel values), we can:</p>
<ol class="arabic simple">
<li><p>Transform both signals to the frequency domain using Fourier transform</p></li>
<li><p>Multiply them together (much simpler!)</p></li>
<li><p>Transform back to get the same result</p></li>
</ol>
<p>Mathematically, the above steps can be written as:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{F}(X), \mathcal{F}(K)\)</span> - Transform both signals to frequency domain (Fourier transform)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{F}(X) \cdot \mathcal{F}(K)\)</span> - Multiply the transformed signals</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{F}^{-1}(\mathcal{F}(X) \cdot \mathcal{F}(K))\)</span> - Transform back to get <span class="math notranslate nohighlight">\(X * K\)</span></p></li>
</ol>
<p>where <span class="math notranslate nohighlight">\(\mathcal{F}^{-1}\)</span> is the inverse Fourier transform that brings us back to the original domain. This is much easier than computing the convolution directly!</p>
<p>For a discrete signal <span class="math notranslate nohighlight">\(x[n]\)</span> with <span class="math notranslate nohighlight">\(N\)</span> points, the Fourier transform <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{F}(x)[k] = \sum_{n=0}^{N-1} x[n] \cdot e^{-2\pi i \frac{nk}{N}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> is the imaginary unit. Or equivalently,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{F}(x)[k] = \sum_{n=0}^{N-1} x[n] \cdot \left[ \cos\left(2\pi \frac{nk}{N}\right) - i \sin\left(2\pi \frac{nk}{N}\right) \right]
\]</div>
<p>using Euler’s formula <span class="math notranslate nohighlight">\(e^{ix} = \cos(x) + i\sin(x)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Complex number can be thought of as a way to represent a 2D vector using a single value (which is a computer science perspective; mathematically, it is a bit more subtle). For example, <span class="math notranslate nohighlight">\(e^{i\pi/2} = \cos(\pi/2) + i\sin(\pi/2)\)</span> represents the 2D vector <span class="math notranslate nohighlight">\((\cos(\pi/2), \sin(\pi/2))\)</span>. In the context of Fourier transform, we interpret <span class="math notranslate nohighlight">\(e^{-2\pi i \frac{nk}{N}}\)</span> as two <em>base waves</em>, i.e., sine and cosine, with phase <span class="math notranslate nohighlight">\(\frac{2\pi k}{N}\)</span>.</p>
<p><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Euler%27s_formula.svg/360px-Euler%27s_formula.svg.png" /></p>
</div>
<p>In simple terms, <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> takes a signal (like our row of pixel values) and breaks it down into sine and cosine waves of different frequencies. Each frequency component <span class="math notranslate nohighlight">\(k\)</span> tells us “how much” of that frequency exists in our original signal.
Don’t worry too much about the complex math. The key idea is that <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> represents a signal as a sum of multiple waves with different frequencies, so we can understand the signal in terms of its frequencies rather than its original values.</p>
<p><img alt="" src="https://devincody.github.io/Blog/post/an_intuitive_interpretation_of_the_fourier_transform/img/FFT-Time-Frequency-View_hu24c1c8fe894ecd0dad24174b2bed08c9_99850_800x0_resize_lanczos_2.png" /></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>3Blue1Brown makes a beautiful video explaining Fourier transform: <a class="reference external" href="https://www.youtube.com/watch?v=spUNpyF58BY">Video</a>. Here is a great interactive demo on Fourier transform by Jez Swanson: <a class="reference external" href="https://www.jezzamon.com/fourier/">Demo</a>.</p>
</div>
<section id="an-example-for-the-fourier-transform">
<h3>An example for the Fourier transform<a class="headerlink" href="#an-example-for-the-fourier-transform" title="Link to this heading">#</a></h3>
<p>Now, let’s perform the convolution using the Fourier transform using an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Let us first perform the convolution directly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pad X with zeros on both sides to handle boundary</span>
<span class="n">n_conv</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Now we get full length output</span>
<span class="n">XKconv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_conv</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_conv</span><span class="p">):</span>
    <span class="n">XKconv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">K</span><span class="p">))]</span> <span class="o">*</span> <span class="n">K</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># Reverse the kernel and take element-wise product and sum up</span>
<span class="n">XKconv</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-70.,   0.,  70.,   0.])
</pre></div>
</div>
</div>
</div>
<p>Let us now perform the convolution using the Fourier transform. We compute the Fourier transform of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(K\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1: Transform X and K to frequency domain</span>
<span class="n">FX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># Pad K with zeros to match the length of X before FFT</span>
<span class="n">K_padded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">K</span><span class="p">)),</span> <span class="s1">&#39;constant&#39;</span><span class="p">)</span> <span class="c1"># [-1  0  1  0  0  0]</span>
<span class="n">FK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">K_padded</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FX:&quot;</span><span class="p">,</span> <span class="n">FX</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FX: [130.+0.00000000e+00j -35.-6.06217783e+01j -35.+6.06217783e+01j
  70.+7.10542736e-15j -35.-6.06217783e+01j -35.+6.06217783e+01j]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>We add zeros to <span class="math notranslate nohighlight">\(K\)</span> to make it the same length as <span class="math notranslate nohighlight">\(X\)</span> before applying the Fourier transform. This is necessary because the convolution theorem requires the signals to have the same length.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FX</span></code> is the Fourier transform of <span class="math notranslate nohighlight">\(X\)</span>, which is a complex number. Each entry <span class="math notranslate nohighlight">\(FX[k]\)</span> represents the weight of the cosine wave in its real part and the weight of the sine wave in its imaginary part, with phase <span class="math notranslate nohighlight">\(2\pi k / N\)</span>. Similarly for <code class="docutils literal notranslate"><span class="pre">FK</span></code>.</p></li>
</ul>
<p>Next, we multiply the transformed signals.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FXKconv</span> <span class="o">=</span> <span class="n">FX</span> <span class="o">*</span> <span class="n">FK</span>
</pre></div>
</div>
</div>
</div>
<p>This is the convolution in the frequency domain. Finally, we transform back to get the convolution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">XKconv_ft</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">ifft</span><span class="p">(</span><span class="n">FXKconv</span><span class="p">))</span>
<span class="n">XKconv_ft</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 2.84957243e-15,  3.05036092e-15, -7.00000000e+01,  1.88737914e-15,
        7.00000000e+01, -1.05195948e-15])
</pre></div>
</div>
</div>
</div>
<ul>
<li><p>We take the real part. The imaginary part is due to numerical artifacts that do not matter in practice.</p></li>
<li><p>The Fourier transform convolution produces a longer output than direct convolution because it includes partial overlaps between K and X at the boundaries. Since we only want the full overlaps, we need to truncate the first two elements of <code class="docutils literal notranslate"><span class="pre">XKconv_ft</span></code> (as K has length 3) to match the length of the direct convolution result.</p></li>
<li><p>For example, let’s look at what happens at the beginning of the convolution:</p>
<ul class="simple">
<li><p>At position -2: Only the last element of K overlaps with X: <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">10]</span> <span class="pre">*</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]</span> <span class="pre">=</span> <span class="pre">10</span></code></p></li>
<li><p>At position -1: Two elements of K overlap with X: <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">10,</span> <span class="pre">10]</span> <span class="pre">*</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]</span> <span class="pre">=</span> <span class="pre">10</span></code></p></li>
<li><p>At position 0: Full overlap begins: <code class="docutils literal notranslate"><span class="pre">[10,</span> <span class="pre">10,</span> <span class="pre">80]</span> <span class="pre">*</span> <span class="pre">[-1,</span> <span class="pre">0,</span> <span class="pre">1]</span> <span class="pre">=</span> <span class="pre">70</span></code></p></li>
</ul>
<p>The Fourier transform method gives us all these positions (-2, -1, 0, …), but we only want the full overlaps starting from position 0, which is why we truncate the first two elements.</p>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">XKconv_ft</span> <span class="o">=</span> <span class="n">XKconv_ft</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
<span class="n">XKconv_ft</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-7.00000000e+01,  1.88737914e-15,  7.00000000e+01, -1.05195948e-15])
</pre></div>
</div>
</div>
</div>
<p>This gives us the same result as the direct convolution up to numerical errors.</p>
</section>
</section>
<section id="fourier-transform-of-images">
<h2>Fourier Transform of Images<a class="headerlink" href="#fourier-transform-of-images" title="Link to this heading">#</a></h2>
<p>Let’s extend the above example to an image which is a 2D matrix.
The idea is the same: we take the Fourier transform of each row and column of the image, and then multiply them together to get the convolution in the frequency domain.
More specifically, for an image <span class="math notranslate nohighlight">\(X\)</span> with size <span class="math notranslate nohighlight">\(H \times W\)</span>, the Fourier transform of <span class="math notranslate nohighlight">\(X\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{F}(X)[h, w] &amp;= \sum_{k=0}^{H-1} \sum_{\ell=0}^{W-1} X[k, \ell] \cdot e^{-2\pi i \frac{hk}{H}} \cdot e^{-2\pi i \frac{w\ell}{W}} \\
&amp;= \sum_{k=0}^{H-1} \sum_{\ell=0}^{W-1} X[k, \ell] e^{-2\pi i \left(\frac{hk}{H} + \frac{w\ell}{W}\right)}
\end{aligned}
\end{split}\]</div>
<p>Comparing with the 1D case, we see that the 2D Fourier transform is <em>functionally</em> the same as the 1D Fourier transform, except that we now have two indices <span class="math notranslate nohighlight">\(h\)</span> and <span class="math notranslate nohighlight">\(w\)</span> to represent the frequency in both dimensions.
The basis waves are 2D waves as shown below.</p>
<p><strong>Cosine waves</strong></p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="k">def</span> <span class="nf">basis_function</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  img_size : square size of image f(x,y)</span>
<span class="sd">  u,v : spatial space indice</span>
<span class="sd">  &#39;&#39;&#39;</span>
  <span class="n">N</span> <span class="o">=</span> <span class="n">img_size</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="n">x_</span><span class="p">,</span> <span class="n">y_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">bf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">x_</span><span class="o">/</span><span class="n">N</span><span class="o">+</span><span class="n">v</span><span class="o">*</span><span class="n">y_</span><span class="o">/</span><span class="n">N</span><span class="p">))</span>
  <span class="k">if</span> <span class="n">u</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">v</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">bf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">bf</span><span class="p">)</span>
  <span class="n">real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">bf</span><span class="p">)</span> <span class="c1"># The cosine part</span>
  <span class="n">imag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">bf</span><span class="p">)</span> <span class="c1"># The sine part</span>
  <span class="k">return</span> <span class="n">real</span><span class="p">,</span> <span class="n">imag</span>

<span class="n">size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">bf_arr_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="o">*</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>
<span class="n">bf_arr_imag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="o">*</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>
<span class="n">ind</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="n">re</span><span class="p">,</span><span class="n">imag</span> <span class="o">=</span> <span class="n">basis_function</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="n">row</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
    <span class="n">bf_arr_real</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">re</span>
    <span class="n">bf_arr_imag</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">imag</span>
    <span class="n">ind</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># real part</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bf_arr_real</span><span class="p">,</span> <span class="n">axs</span><span class="p">):</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/7cca20a71b573173bea31dcfcb10f8cdecabdaaa5d4c6f11ff925c06a1b3c2cc.png" src="../_images/7cca20a71b573173bea31dcfcb10f8cdecabdaaa5d4c6f11ff925c06a1b3c2cc.png" />
</div>
</div>
<p><strong>Sine waves</strong></p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># imaginary part</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bf_arr_imag</span><span class="p">,</span> <span class="n">axs</span><span class="p">):</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/9230f042871a78717654aacec06d315ea432b2ac99ffb92f6f00a9f6c0d9c2fb.png" src="../_images/9230f042871a78717654aacec06d315ea432b2ac99ffb92f6f00a9f6c0d9c2fb.png" />
</div>
</div>
<p>The Fourier transform of an image is a decomposition of an image into the sum of these basis waves.</p>
<section id="an-example-of-fourier-transform">
<h3>An example of Fourier transform<a class="headerlink" href="#an-example-of-fourier-transform" title="Link to this heading">#</a></h3>
<p>Let us apply the Fourier transform to an image.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Read image from URL</span>
<span class="k">def</span> <span class="nf">read_jpeg_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">))</span>
    <span class="c1"># Convert to RGB mode if needed (in case it&#39;s RGBA)</span>
    <span class="k">if</span> <span class="n">img</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;RGB&#39;</span><span class="p">:</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;RGB&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">img</span>

<span class="k">def</span> <span class="nf">image_to_numpy</span><span class="p">(</span><span class="n">img</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">to_gray_scale</span><span class="p">(</span><span class="n">img_np</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">img_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># URL of the image</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://www.binghamton.edu/news/images/uploads/features/20180815_peacequad02_jwc.jpg&quot;</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">read_jpeg_from_url</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
<span class="n">img_np</span> <span class="o">=</span> <span class="n">image_to_numpy</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img_gray</span> <span class="o">=</span> <span class="n">to_gray_scale</span><span class="p">(</span><span class="n">img_np</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x32b9bce50&gt;
</pre></div>
</div>
<img alt="../_images/7927e2f1f22bdaa4d0ddf7f94aae5e77d51e60a9b63838936b6a5413101e5dcd.png" src="../_images/7927e2f1f22bdaa4d0ddf7f94aae5e77d51e60a9b63838936b6a5413101e5dcd.png" />
</div>
</div>
<p>Take the Fourier transform of the image.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ft_img_gray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">img_gray</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This decomposes the image into a sum of basis waves. Let’s see the weights of the basis waves.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ft_img_gray</span><span class="p">)</span>

<span class="c1"># real part</span>
<span class="n">fig1</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">LogNorm</span><span class="p">(),</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">fig1</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">ax1</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s1">&#39;horizontal&#39;</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;Fourier transform magnitude&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/416680d1085c0676391fada6ca85def781df895e56bab6f91e62fc3829dd8cf9.png" src="../_images/416680d1085c0676391fada6ca85def781df895e56bab6f91e62fc3829dd8cf9.png" />
</div>
</div>
<p>The corresponding basis waves look like this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">bf_arr_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="o">*</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>
<span class="n">bf_arr_imag</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">size</span><span class="o">*</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">))</span>
<span class="n">ind</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">//</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">re</span><span class="p">,</span><span class="n">imag</span> <span class="o">=</span> <span class="n">basis_function</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="n">row</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">col</span><span class="p">)</span>
    <span class="n">bf_arr_real</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">re</span>
    <span class="n">bf_arr_imag</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">imag</span>
    <span class="n">ind</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># real part</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bf_arr_real</span><span class="p">,</span> <span class="n">axs</span><span class="p">):</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Real Part of Basis Functions&#39;</span><span class="p">)</span>


<span class="c1"># imaginary part</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="k">for</span> <span class="n">img</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">bf_arr_imag</span><span class="p">,</span> <span class="n">axs</span><span class="p">):</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_axis_off</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Imaginary Part of Basis Functions&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0.98, &#39;Imaginary Part of Basis Functions&#39;)
</pre></div>
</div>
<img alt="../_images/645492f09c0bf94e6a82a74c818c5210efa2d53a22e3bdaab58ff80e5d785240.png" src="../_images/645492f09c0bf94e6a82a74c818c5210efa2d53a22e3bdaab58ff80e5d785240.png" />
<img alt="../_images/ac6ec7dc40856a758f659ad17a1c0db1613c91d766ee2468bf9ab9fc3458759b.png" src="../_images/ac6ec7dc40856a758f659ad17a1c0db1613c91d766ee2468bf9ab9fc3458759b.png" />
</div>
</div>
<p>Now, let’s see the convolution of the image with a Prewitt operator.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span> <span class="c1"># Prewitt operator</span>

<span class="n">K_padd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">img_gray</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">img_gray</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">K_padd</span><span class="p">[:</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">:</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">K</span>

<span class="c1"># convolution</span>
<span class="n">FK</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">K_padd</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The Fourier transform of the Prewitt operator looks like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">FK</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/39e592bce745302859e04d98ec98c277cf98d12b1dc603da40dec635bd0c1d61.png" src="../_images/39e592bce745302859e04d98ec98c277cf98d12b1dc603da40dec635bd0c1d61.png" />
</div>
</div>
<p>We can think of the frequency domain of the kernel as a <strong>filter</strong> that suppresses some frequencies and allows others to pass through. In the example of the Prewitt operator, the kernel <code class="docutils literal notranslate"><span class="pre">FK</span></code> has a low value around the center of the image. The product <span class="math notranslate nohighlight">\(FX \cdot FK\)</span> then suppresses the low-frequency components of the image, and we are left with the high-frequency components that correspond to the horizontal edges. We can think of this as a high-pass filter that only allows high-frequency components to pass through.</p>
<p>Let’s see the convolution result.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">img_gray</span><span class="p">)</span>
<span class="n">conv_img_gray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">ifft2</span><span class="p">(</span><span class="n">FX</span> <span class="o">*</span> <span class="n">FK</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">conv_img_gray</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x335a73690&gt;
</pre></div>
</div>
<img alt="../_images/a9a1cd9bfc3d3a1e4de867d76fc4b87116c075b0e104bb569a6a08f90455c0ae.png" src="../_images/a9a1cd9bfc3d3a1e4de867d76fc4b87116c075b0e104bb569a6a08f90455c0ae.png" />
</div>
</div>
<p>We observe that the horizontal edges are highlighted.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A widespread application of the 2D Fourier transform is JPEG format. Here’s how it works:</p>
<p>(1) It first breaks the image into small 8x8 squares.
(2) It converts each square into frequencies using the Discrete Cosine Transform. The sine part is discarded for compression.
(3) It keeps the important low frequencies that our eyes can see well.
(4) It throws away most of the high frequencies that our eyes don’t notice much.</p>
<p>These steps make the file much smaller while still looking good to us.</p>
</div>
</section>
</section>
<section id="a-key-lesson-from-image-processing">
<h2>A key lesson from image processing<a class="headerlink" href="#a-key-lesson-from-image-processing" title="Link to this heading">#</a></h2>
<p>We have seen an equivalence between convolution in the pixel (spatial) domain and multiplication in the frequency domain.
Using the Fourier transform, an image is decomposed into a sum of basis waves.
The <em>kernel</em> can be thought of as <em>a filter</em> that suppresses some basis waves and allows others to pass through.</p>
<p>This idea is the key to understand graph convolutional networks we will see in the next page.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="pen-and-paper-exercises">
<h1>Pen and paper exercises<a class="headerlink" href="#pen-and-paper-exercises" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="#pen-and-paper/exercise.pdf"><span class="xref myst">✍️ Pen and paper exercises</span></a></p></li>
</ul>
<hr class="docutils" />
<section id="id7">
<h2>jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="popular-graph-neural-networks">
<h1>Popular Graph Neural Networks<a class="headerlink" href="#popular-graph-neural-networks" title="Link to this heading">#</a></h1>
<p>In this note, we will introduce three popular GNNs: GraphSAGE, Graph Attention Networks (GAT), and Graph Isomorphism Network (GIN).</p>
<section id="graphsage-sample-and-aggregate">
<h2>GraphSAGE: Sample and Aggregate<a class="headerlink" href="#graphsage-sample-and-aggregate" title="Link to this heading">#</a></h2>
<p>GraphSAGE <a class="footnote-reference brackets" href="#footcite-hamilton2017graphsage" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> introduced a different GCN that can be <em><strong>generalized to unseen nodes</strong></em> (they called it “inductive”). While previous approaches like ChebNet and GCN operate on the entire graph, GraphSAGE proposes an inductive framework that generates embeddings by sampling and aggregating features from a node’s neighborhood.</p>
<p><img alt="" src="https://theaisummer.com/static/02e23adc75fe68e5dd249a94f3c1e8cc/c483d/graphsage.png" /></p>
<section id="key-ideas">
<h3>Key Ideas<a class="headerlink" href="#key-ideas" title="Link to this heading">#</a></h3>
<p>GraphSAGE involves two key ideas: (1) sampling and (2) aggregation.</p>
<section id="neighborhood-sampling">
<h4>Neighborhood Sampling<a class="headerlink" href="#neighborhood-sampling" title="Link to this heading">#</a></h4>
<p>The key idea is the <em>neighborhood sampling</em>. Instead of using all neighbors, GraphSAGE samples a fixed-size set of neighbors for each node. This controls memory complexity, a key limitation of the previous GNNs.</p>
<p>Another key advantage of neighborhood sampling is that it enables GraphSAGE to handle dynamic, growing networks. Consider a citation network where new papers (nodes) are continuously added. Traditional GCNs would need to recompute filters for the entire network with each new addition. In contrast, GraphSAGE can immediately generate embeddings for new nodes by simply sampling their neighbors, without any retraining or recomputation.</p>
</section>
<section id="aggregation">
<h4>Aggregation<a class="headerlink" href="#aggregation" title="Link to this heading">#</a></h4>
<p>Another key idea is the <em>aggregation</em>. GraphSAGE makes a distinction between self-information and neighborhood information. While previous GNNs treat them equally and aggregate them, GraphSAGE treats them differently. Specifically, GraphSAGE introduces an additional step: it concatenates the self-information and the neighborhood information as the input of the convolution.</p>
<div class="math notranslate nohighlight">
\[
Z_v = \text{CONCAT}(X_v, X_{\mathcal{N}(v)})
\]</div>
<p>where <span class="math notranslate nohighlight">\(X_v\)</span> is the feature of the node itself and <span class="math notranslate nohighlight">\(X_{\mathcal{N}(v)}\)</span> is the aggregation of the features of its neighbors. GraphSAGE introduces different ways to aggregate information from neighbors:</p>
<div class="math notranslate nohighlight">
\[X_{\mathcal{N}(v)} = \text{AGGREGATE}_k(\{X_u, \forall u \in \mathcal{N}(v)\})\]</div>
<p>Common aggregation functions include:</p>
<ul class="simple">
<li><p>Mean aggregator: <span class="math notranslate nohighlight">\(\text{AGGREGATE} = \text{mean}(\{h_u, \forall u \in \mathcal{N}(v)\})\)</span></p></li>
<li><p>Max-pooling: <span class="math notranslate nohighlight">\(\text{AGGREGATE} = \max(\{\sigma(W_{\text{pool}}h_u + b), \forall u \in \mathcal{N}(v)\})\)</span></p></li>
<li><p>LSTM aggregator: Apply LSTM to randomly permuted neighbors</p></li>
</ul>
<p>The concatenated feature <span class="math notranslate nohighlight">\(Z_v\)</span> is normalized by the L2 norm.</p>
<div class="math notranslate nohighlight">
\[
\hat{Z}_v = \frac{Z_v}{\|Z_v\|_2}
\]</div>
<p>and then fed into the convolution.</p>
<div class="math notranslate nohighlight">
\[
X_v^k = \sigma(W^k \hat{Z}_v + b^k)
\]</div>
</section>
</section>
</section>
<section id="graph-attention-networks-gat-differentiate-individual-neighbors">
<h2>Graph Attention Networks (GAT): Differentiate Individual Neighbors<a class="headerlink" href="#graph-attention-networks-gat-differentiate-individual-neighbors" title="Link to this heading">#</a></h2>
<p>A key innovation of GraphSAGE is to treat the self and neighborhood information differently. But should all neighbors be treated equally? Graph Attention Networks (GAT) address this by letting the model learn which neighbors to pay attention to.</p>
<section id="attention-mechanism">
<h3>Attention Mechanism<a class="headerlink" href="#attention-mechanism" title="Link to this heading">#</a></h3>
<p><img alt="" src="https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_7.55.32_PM_vkdDcDx.png" /></p>
<p>The core idea is beautifully simple: instead of using fixed weights like GCN, let’s learn attention weights <span class="math notranslate nohighlight">\(\alpha_{ij}\)</span> that determine how much node <span class="math notranslate nohighlight">\(i\)</span> should attend to node <span class="math notranslate nohighlight">\(j\)</span>. These weights are computed dynamically based on node features:</p>
<div class="math notranslate nohighlight">
\[
\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})}
\]</div>
<p>where <span class="math notranslate nohighlight">\(e_{ij}\)</span> represents the importance of the edge between node <span class="math notranslate nohighlight">\(i\)</span> and node <span class="math notranslate nohighlight">\(j\)</span>. Variable <span class="math notranslate nohighlight">\(e_{ij}\)</span> is a <em>learnable</em> parameter and can be negative, and the exponential function is applied to transform it to a non-negative value, with the normalization term <span class="math notranslate nohighlight">\(\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})\)</span> to ensure the weights sum to 1.</p>
<p>How to compute <span class="math notranslate nohighlight">\(e_{ij}\)</span>? One simple choice is to use a neural network with a shared weight matrix <span class="math notranslate nohighlight">\(W\)</span> and a LeakyReLU activation function. Specifically:</p>
<ol class="arabic simple">
<li><p>Let’s focus on computing <span class="math notranslate nohighlight">\(e_{ij}\)</span> for node <span class="math notranslate nohighlight">\(i\)</span> and its neighbor <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>We use a shared weight matrix <span class="math notranslate nohighlight">\(W\)</span> to transform the features of node <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>.
$<span class="math notranslate nohighlight">\(
\mathbf{\tilde h}_i  = \mathbf{h}_i, \quad \mathbf{\tilde h}_j  = W\mathbf{h}_j
\)</span>$</p></li>
<li><p>We concatenate the transformed features and apply a LeakyReLU activation function.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
e_{ij} = \text{LeakyReLU}(\mathbf{a}^T[\mathbf{\tilde h}_i, \mathbf{\tilde h}_j])
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is a trainable parameter vector that sums the two transformed features.</p>
<p>Once we have these attention weights, the node update is straightforward - just a weighted sum of neighbor features:</p>
<div class="math notranslate nohighlight">
\[\mathbf{h}'_i = \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij}{\bf W}_{\text{feature}}\mathbf{h}_j\right)\]</div>
<p>where <span class="math notranslate nohighlight">\({\bf W}_{\text{feature}}\)</span> is a trainable weight matrix. To stabilize training, GAT uses multiple attention heads and concatenates their outputs:</p>
<div class="math notranslate nohighlight">
\[\mathbf{h}'_i = \parallel_{k=1}^K \sigma\left(\sum_{j \in \mathcal{N}(i) \cup \{i\}} \alpha_{ij}^k{\bf W}^k_{\text{feature}}\mathbf{h}_j\right)\]</div>
</section>
</section>
<section id="graph-isomorphism-network-gin-differentiate-the-aggregation">
<h2>Graph Isomorphism Network (GIN): Differentiate the Aggregation<a class="headerlink" href="#graph-isomorphism-network-gin-differentiate-the-aggregation" title="Link to this heading">#</a></h2>
<p>Graph Isomorphism Networks (GIN) is another popular GNN that born out of a question: what is the maximum discriminative power achievable by Graph Neural Networks? The answer lies in its theoretical connection to <strong>the Weisfeiler-Lehman (WL) test</strong>, a powerful algorithm for graph isomorphism testing.</p>
<section id="weisfeiler-lehman-test">
<h3>Weisfeiler-Lehman Test<a class="headerlink" href="#weisfeiler-lehman-test" title="Link to this heading">#</a></h3>
<p>Are two graphs structurally identical? Graph isomorphism testing determines if two graphs are structurally identical, with applications in graph classification, clustering, and other tasks.</p>
<p><img alt="" src="https://i.sstatic.net/j5sGu.png" /></p>
<p>While the general problem has no known polynomial-time solution, the WL test is an efficient heuristic that works well in practice. The WL test iteratively refines node labels by hashing the multiset of neighboring labels</p>
<p><img alt="" src="../_images/weisfeiler-lehman-test.jpg" /></p>
<p>The WL test works as follows:</p>
<ol class="arabic simple">
<li><p>Assign all nodes the same initial label.</p></li>
<li><p>For each node, collect the labels of all its neighbors and <em>aggregate them</em> into a hash (e.g., new label). For example, the top node gets {0} from its neighbors, resulting in a collection {0,0}. A new label is created via a hash function <span class="math notranslate nohighlight">\(h\)</span> that maps {0, {0, 0}} to a new label 1.</p></li>
<li><p>Repeat the process for a fixed number of iterations or until convergence.</p></li>
</ol>
<p>Here is the implementation of the WL test in Python:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>

<span class="k">def</span> <span class="nf">weisfeiler_lehman_test</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>
    <span class="n">n_nodes</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">color_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">hash_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">color_map</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color_map</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>

        <span class="c1"># Go through each node</span>
        <span class="n">labels_old</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">):</span>

            <span class="c1"># Collect the labels of all neighbors</span>
            <span class="n">neighbors</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">neighbor_labels</span> <span class="o">=</span> <span class="n">labels_old</span><span class="p">[</span><span class="n">neighbors</span><span class="p">]</span>

            <span class="c1"># Count the frequency of each label</span>
            <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">neighbor_labels</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Create a hash key by converting the frequency dictionary to a string</span>
            <span class="n">hash_key</span> <span class="o">=</span> <span class="nb">str</span><span class="p">({</span><span class="n">unique</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span> <span class="n">counts</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">unique</span><span class="p">))})</span>

            <span class="c1"># Create a new label by hashing the frequency dictionary</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">hash_fn</span><span class="p">(</span><span class="n">hash_key</span><span class="p">)</span>
            <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>

        <span class="c1"># Check convergence</span>
        <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">unique_old</span><span class="p">,</span> <span class="n">counts_old</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels_old</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">counts</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">counts_old</span><span class="p">)):</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">labels</span>


<span class="n">edge_list</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">sparse</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span>
    <span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">edge_list</span><span class="p">),</span> <span class="p">([</span><span class="n">e</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">edge_list</span><span class="p">],</span> <span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">edge_list</span><span class="p">])),</span>
    <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span>
<span class="n">A</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>

<span class="n">weisfeiler_lehman_test</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 0, 0, 0, 0, 0])
</pre></div>
</div>
</div>
</div>
<p>After these iterations:</p>
<ul class="simple">
<li><p>Nodes with the same label are structurally identical, meaning that they are indistinguishable unless we label them differently.</p></li>
<li><p>Two graphs are structurally identical if and only if they have the same node labels after the WL test.</p></li>
</ul>
<p>The WL test is a heuristic and can fail on some graphs. For example, it cannot distinguish regular graphs with the same number of nodes and edges.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The WL test above is called the 1-WL test. There are higher-order WL tests that can distinguish more graphs, which are the basis of advanced GNNs.
Check out <a class="reference external" href="https://www.moldesk.net/blog/weisfeiler-lehman-isomorphism-test/">this note</a></p>
</div>
</section>
<section id="gin">
<h3>GIN<a class="headerlink" href="#gin" title="Link to this heading">#</a></h3>
<p>GIN <a class="footnote-reference brackets" href="#footcite-xu2018how" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> is a GNN that is based on the WL test.
The key idea is to focus on the parallel between the WL test and the GNN update rule.</p>
<ul class="simple">
<li><p>In the WL test, we iteratively collect the labels of neighbors and aggregate them through a <em>hash function</em>.</p></li>
<li><p>In the GraphSAGE and GAT, the labels are the nodes’ features, and the aggregation is some arithmetic operations such as mean or max.</p></li>
</ul>
<p>The key difference is that the hash function in the WL test always distinguishes different sets of neighbors’ labels, while the aggregation in GraphSAGE and GAT does not always do so. For example, if all nodes have the same feature (e.g., all 1), the aggregation by the mean or max will result in the same value for all nodes, whereas the hash function in the WL test can still distinguish different sets of neighbors’ labels by <em>the count of each label</em>.</p>
<p>The resulting convolution update rule is:</p>
<div class="math notranslate nohighlight">
\[
h_v^{(k+1)} = \text{MLP}^{(k)}\left((1 + \epsilon^{(k)}) \cdot h_v^{(k)} + \sum_{u \in \mathcal{N}(v)} h_u^{(k)}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{MLP}^{(k)}\)</span> is a multi-layer perceptron (MLP) with <span class="math notranslate nohighlight">\(k\)</span> layers, and <span class="math notranslate nohighlight">\(\epsilon^{(k)}\)</span> is a fixed or trainable parameter.</p>
<div class="docutils container" id="id10">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-hamilton2017graphsage" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">4</a><span class="fn-bracket">]</span></span>
<p>William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, NIPS’17, 1025–1035. Red Hook, NY, USA, 2017. Curran Associates Inc.</p>
</aside>
<aside class="footnote brackets" id="footcite-xu2018how" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">5</a><span class="fn-bracket">]</span></span>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In <em>International Conference on Learning Representations</em>. 2019. URL: <a class="reference external" href="https://openreview.net/forum?id=ryGs6iA5Km">https://openreview.net/forum?id=ryGs6iA5Km</a>.</p>
</aside>
</aside>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="module-9-graph-neural-networks">
<h1>Module 9: Graph Neural Networks<a class="headerlink" href="#module-9-graph-neural-networks" title="Link to this heading">#</a></h1>
<section id="what-to-learn-in-this-module">
<h2>What to learn in this module<a class="headerlink" href="#what-to-learn-in-this-module" title="Link to this heading">#</a></h2>
<p>In this module, we will learn how to use neural networks to learn representations of graphs. We will learn:</p>
<ul class="simple">
<li><p>Fourier transform on image</p></li>
<li><p>Fourier transform on graph</p></li>
<li><p>Spectral filters</p></li>
<li><p>Graph convolutional networks</p></li>
<li><p>Popular GNNs (GCN, GAT, GraphSAGE, and GIN)</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "skojaku/adv-net-sci",
            ref: "gh-pages",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./tmp"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Appendix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#brunas-spectral-gcn">Bruna’s Spectral GCN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chebnet">ChebNet</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#jupytext-formats-md-myst-text-representation-extension-md-format-name-myst-kernelspec-display-name-python-3-language-python-name-python3">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#from-image-to-graph">From Image to Graph</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#analogy-between-image-and-graph-data">Analogy between image and graph data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-filter-on-graphs">Spectral filter on graphs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-filtering">Spectral Filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-high-pass-filter-increases-the-contrast-of-the-eigenvector-centrality-emphasizing-the-differences-between-nodes-on-the-other-hand-the-low-pass-filter-smooths-out-the-eigenvector-centrality">The high-pass filter increases the contrast of the eigenvector centrality, emphasizing the differences between nodes. On the other hand, the low-pass filter smooths out the eigenvector centrality.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolutional-networks">Graph Convolutional Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spectral-graph-convolutional-networks">Spectral Graph Convolutional Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-spectral-to-spatial">From Spectral to Spatial</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">ChebNet</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolutional-networks-by-kipf-and-welling">Graph Convolutional Networks by Kipf and Welling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-order-approximation">First-order Approximation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-gcns-can-suffer-from-over-smoothing">Deep GCNs can suffer from over-smoothing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-transform">Fourier Transform</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-for-the-fourier-transform">An example for the Fourier transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fourier-transform-of-images">Fourier Transform of Images</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-of-fourier-transform">An example of Fourier transform</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-key-lesson-from-image-processing">A key lesson from image processing</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#pen-and-paper-exercises">Pen and paper exercises</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">jupytext:
formats: md:myst
text_representation:
extension: .md
format_name: myst
kernelspec:
display_name: Python 3
language: python
name: python3</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#popular-graph-neural-networks">Popular Graph Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graphsage-sample-and-aggregate">GraphSAGE: Sample and Aggregate</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-ideas">Key Ideas</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#neighborhood-sampling">Neighborhood Sampling</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#aggregation">Aggregation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-attention-networks-gat-differentiate-individual-neighbors">Graph Attention Networks (GAT): Differentiate Individual Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-mechanism">Attention Mechanism</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-isomorphism-network-gin-differentiate-the-aggregation">Graph Isomorphism Network (GIN): Differentiate the Aggregation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weisfeiler-lehman-test">Weisfeiler-Lehman Test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gin">GIN</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#module-9-graph-neural-networks">Module 9: Graph Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-to-learn-in-this-module">What to learn in this module</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Sadamori Kojaku
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>