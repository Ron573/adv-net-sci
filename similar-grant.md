
https://www.nsf.gov/awardsearch/simpleSearchResult?queryText=entity+recognition&ActiveAwards=true

# Knowledge Extraction and Discovery from Massive Text Corpora via Extremely Weak Supervision
Automated knowledge extraction and discovery methods can address the diverse needs of different users (e.g., governments for decision making and scientists for literature summary). A fundamental open problem is how much user effort automated methods require to obtain useful knowledge. This project aims to minimize such required user effort with a newly proposed paradigm, extremely weak supervision ? It includes only brief natural-language user input to define the task (e.g., a list of topics when classifying news articles; location names when classifying events), guidance similar to task-specific guidelines that might be provided to human annotators. By using brief natural-language input instead of labor-intensive annotated training samples, this new paradigm will help democratize knowledge extraction and discovery, and extend its application beyond rich companies to ordinary, relatively untrained users with a broad range of needs (e.g., domain scientists and small business owners). Project outcomes will be disseminated via top conferences and scholarly publications and integrated into new courses. This project will also support a diverse set of graduate, undergraduate, and high school students.

This project focuses on four fundamental, interconnected knowledge extraction and discovery tasks, i.e., text classification, phrase mining, named entity recognition, and relation extraction. Following the extremely weak supervision paradigm, this project will develop a series of novel methods, including (1) an unsupervised phrase tagging method for both multi-gram and unigram (emerging) phrases, (2) a text classification method that can take only the most popular (e.g., top-50%) class names as input to discover novel classes (i.e., new classes are not explicitly defined by the user) and build a classifier for all the classes; (3) a named entity recognition method that can take a few popular entity types and mentions of interest to recognize (emerging) entity mentions of the same/similar types; and (4) a relation extraction method that can take a few popular relation types and tuples of interest to discover relations of similar semantics and extract relevant tuples. All these methods, by design, will be agnostic to domains and languages and require only the availability of pre-trained neural language models in a particular domain and language.

This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.


# Scalable Entity Resolution for Massive and Streaming Data Contexts
This research project will develop the first model-based statistical approach to perform entity
resolution for streaming data contexts and will address the issue of scalability in both online and
offline scenarios. With the ubiquity of data, linking multiple data sets is a crucial first step in
many types of inference for myriad applications including healthcare, official statistics, ecology,
and fraud detection and national security. Entity resolution is the task of resolving duplicates
in two or more partially overlapping sets of records, or files, from noisy data sources without a
unique identifier. Statistical approaches to entity resolution are advantageous because they provide
interpretable parameters and quantify uncertainty in the linked records. Linking becomes more
challenging when the data update over time, termed streaming or online data, or when the data
scale is massive. Currently no statistically model-based approaches exist to resolve entities in an
online way. Relatedly, the nature of streaming data exacerbates the challenge of scalability in
that the number of records to be linked accumulates. The methods will be made accessible to
practitioners and other researchers through open-source software and the project will additionally
provide educational and professional training and mentoring to graduate students.

This project will expand model-based entity resolution into the streaming data space through
the formulation of new models and novel computational algorithms. Specifically, this project aims to
improve scalability of Bayesian entity resolution models through approximate sampling techniques,
such as variational inference, and develop fast updating of a Bayesian entity resolution model in a
streaming data context, resulting in the first Bayesian entity resolution model updating strategy
that can handle streaming data contexts for massive data sets. A limitation of existing methods for
streaming inference with Bayesian models is that the pool of samples to be updated will converge to
a degenerate distribution as the process is repeated many times, which guarantees poor quality of
model fits in a streaming setting. This issue will be addressed with introduction of a novel Markov
chain Monte Carlo sampler for streaming Bayesian inference, which will improve existing methods
by combining filtering ideas with a highly parallelizable transition kernel.


# CAREER: Learning to Extract Consistent Event Graphs from Long and Complex Documents
Documents about real-world events are published daily. The large number of such documents makes it very hard for people to read and absorb them all, a phenomenon known as ?information overload". Applying computer algorithms that can automatically extract events is a promising solution because they can transform large amounts of text into smaller summaries in the form of structured event knowledge graphs that reveal the relationships between the people, places, and times in the events. Current deep learning-based event extraction techniques mainly focus on extracting event knowledge at the level of individual sentences and are unable to extract a knowledge graph spanning multiple sentences with sufficient accuracy or efficiency. For example, existing techniques would struggle with events described in a long document having multiple sections. Moreover, these extraction techniques do not capture accurate information regarding real-life events because they typically include nuanced attributes such as causes and effects. The research goal of this CAREER award is to build information extraction (IE) methods with natural language processing methods, using the latest deep learning-based techniques, to construct an event knowledge graph for storing knowledge and improving the ability of people to track rapidly evolving event information. In the short term, the project will improve the quality and comprehensiveness of event knowledge graphs. In the long run, the project will entirely transform people's experiences and habits in acquiring event knowledge from various sources. The system to be developed through this award will better support numerous event-oriented tasks that people need to perform, such as future event prediction, event factuality verification, and risk event prevention, all of which have profound impacts on society. Moreover, our work would make fundamental contributions to a wide range of interdisciplinary applications such as statutory reasoning based on legal documents, prediction of disease outbreaks, and biomedical document understanding, all of which currently rely on extremely slow and high-cost methods.

The general technical goal of this project is to address the knowledge gap of event extraction from long and complex documents (as compared to the traditional sentence-level extraction) and to do so in an efficient manner. The general goal is divided into three sub-research goals. First, to extract the entirety of event attributes, which is not possible for current models trained on a dataset with a predefined schema, the project introduces a new question-answer generation paradigm that enables a novel representation of events from clusters of documents discussing the same events. The project will leverage document hierarchy information for extracting events, which enforces the validity and broad coverage of event information. Motivated by the fact that current event knowledge construction is inefficient and is impaired by pairwise event-event relation predictions, the second research goal is to develop novel techniques enabling the construction of the event knowledge graph. For this purpose, the investigators propose interleaving targeted retrieval and joint modeling of event arguments and entity-entity relations. This not only enables efficient updating of graphs, but also ensures its global consistency. Finally, the third goal is to adapt to individual information-seeking needs, which is not considered by current methods. The project will study schema induction strategies and schema matching algorithms for adapting the event knowledge graph to user preferences.ggi
