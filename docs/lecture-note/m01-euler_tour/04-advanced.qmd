---
title: "Sparse Matrices for Large-Scale Networks"
filters:
  - marimo-team/marimo
jupyter: python3
---

# Sparse Matrices: Scaling Network Analysis to the Real World

While the basic adjacency matrix representation works well for small networks like Königsberg's 4 nodes, real-world networks can have millions or billions of nodes. Facebook has 3 billion users, the internet has billions of web pages, and the human brain has roughly 86 billion neurons. For such networks, dense adjacency matrices become computationally prohibitive.

::: {.callout-note}
## Why This Matters

A dense adjacency matrix for Facebook's network would require $(3 \times 10^9)^2 \times 8$ bytes ≈ 72 exabytes of memory—more than all the data storage in the world combined! Clearly, we need smarter approaches.
:::

This notebook explores **sparse matrix** representations that make large-scale network analysis feasible by exploiting a fundamental property of real networks: they are **sparse** (most pairs of nodes are not connected).

## The Sparsity of Real Networks

Most real-world networks are extremely sparse. Let's quantify this:

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy import sparse
import time

# Examples of real network sparsity
networks = {
    "Facebook (2012)": {"nodes": 1e9, "edges": 140e9},
    "Twitter (2010)": {"nodes": 40e6, "edges": 1.47e9},
    "Internet AS-level": {"nodes": 65000, "edges": 120000},
    "Human Brain": {"nodes": 86e9, "edges": 100e12},
    "Academic Collaboration": {"nodes": 1.7e6, "edges": 11e6}
}

print("Network Sparsity Analysis:")
print("=" * 50)
for name, data in networks.items():
    n_nodes = data["nodes"]
    n_edges = data["edges"]
    max_edges = n_nodes * (n_nodes - 1) / 2  # Undirected network
    sparsity = n_edges / max_edges
    
    print(f"{name}:")
    print(f"  Nodes: {n_nodes:,.0f}")
    print(f"  Edges: {n_edges:,.0f}")
    print(f"  Sparsity: {sparsity:.2e} ({sparsity*100:.8f}%)")
    print(f"  Memory (dense): {n_nodes**2 * 8 / 1e9:.2f} GB")
    print(f"  Memory (sparse): {n_edges * 16 / 1e9:.2f} GB")
    print()
```

::: {.column-margin}
**Sparsity** = (actual edges) / (maximum possible edges). Real networks typically have sparsity < 0.001, meaning over 99.9% of potential connections don't exist.
:::

The key insight: since most matrix entries are zero, we only need to store the non-zero entries!

## Sparse Matrix Formats

SciPy provides several sparse matrix formats, each optimized for different operations:

### 1. COO (Coordinate) Format

The coordinate format stores three arrays: row indices, column indices, and values. This is conceptually closest to an edge list.

```{python}
# Create a simple example network
edges = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 4), (3, 4)]
n_nodes = 5

# Build COO matrix from edge list
row_indices = []
col_indices = []
values = []

for i, j in edges:
    # Add edge in both directions (undirected)
    row_indices.extend([i, j])
    col_indices.extend([j, i])
    values.extend([1, 1])

# Create COO sparse matrix
coo_matrix = sparse.coo_matrix((values, (row_indices, col_indices)), 
                               shape=(n_nodes, n_nodes))

print("COO Matrix representation:")
print(f"Data: {coo_matrix.data}")
print(f"Row indices: {coo_matrix.row}")
print(f"Col indices: {coo_matrix.col}")
print(f"Shape: {coo_matrix.shape}")
print(f"Non-zero entries: {coo_matrix.nnz}")
```

### 2. CSR (Compressed Sparse Row) Format

CSR is optimized for fast row-wise operations—perfect for computing node degrees and many graph algorithms.

```{python}
# Convert to CSR format
csr_matrix = coo_matrix.tocsr()

print("CSR Matrix representation:")
print(f"Data: {csr_matrix.data}")
print(f"Indices: {csr_matrix.indices}")
print(f"Index pointers: {csr_matrix.indptr}")
print(f"Shape: {csr_matrix.shape}")

# The dense representation for comparison
print("\nDense matrix:")
print(csr_matrix.toarray())
```

::: {.column-margin}
**CSR encoding**: `indptr[i]` to `indptr[i+1]` tells you where node i's neighbors are stored in the `indices` array. The `data` array contains the corresponding edge weights.
:::

### 3. CSC (Compressed Sparse Column) Format

CSC is optimized for column-wise operations.

```{python}
# Convert to CSC format
csc_matrix = coo_matrix.tocsc()

print("CSC Matrix representation:")
print(f"Data: {csc_matrix.data}")
print(f"Indices: {csc_matrix.indices}")
print(f"Index pointers: {csc_matrix.indptr}")
```

## Performance Comparison: Dense vs Sparse

Let's create a realistic example to demonstrate the performance differences:

```{python}
def create_sparse_network(n_nodes, sparsity=0.001):
    """Create a sparse random network"""
    n_edges = int(n_nodes * (n_nodes - 1) * sparsity / 2)
    
    # Generate random edges
    edges = set()
    while len(edges) < n_edges:
        i, j = np.random.choice(n_nodes, 2, replace=False)
        edges.add((min(i, j), max(i, j)))
    
    return list(edges)

# Create networks of different sizes
sizes = [100, 500, 1000, 2000]
results = []

for n in sizes:
    print(f"\nTesting network with {n} nodes...")
    
    edges = create_sparse_network(n, sparsity=0.01)  # 1% sparsity
    
    # Create dense matrix
    start_time = time.time()
    dense_matrix = np.zeros((n, n))
    for i, j in edges:
        dense_matrix[i, j] = 1
        dense_matrix[j, i] = 1
    dense_time = time.time() - start_time
    
    # Create sparse matrix
    start_time = time.time()
    row_idx, col_idx = zip(*[(i, j) for i, j in edges] + [(j, i) for i, j in edges])
    values = [1] * (2 * len(edges))
    sparse_matrix = sparse.csr_matrix((values, (row_idx, col_idx)), shape=(n, n))
    sparse_time = time.time() - start_time
    
    # Memory usage
    dense_memory = dense_matrix.nbytes / 1024  # KB
    sparse_memory = (sparse_matrix.data.nbytes + 
                    sparse_matrix.indices.nbytes + 
                    sparse_matrix.indptr.nbytes) / 1024  # KB
    
    # Degree computation performance
    start_time = time.time()
    dense_degrees = dense_matrix.sum(axis=1)
    dense_degree_time = time.time() - start_time
    
    start_time = time.time()
    sparse_degrees = np.array(sparse_matrix.sum(axis=1)).flatten()
    sparse_degree_time = time.time() - start_time
    
    results.append({
        'nodes': n,
        'edges': len(edges),
        'dense_memory': dense_memory,
        'sparse_memory': sparse_memory,
        'memory_ratio': dense_memory / sparse_memory,
        'dense_degree_time': dense_degree_time,
        'sparse_degree_time': sparse_degree_time,
        'time_ratio': dense_degree_time / sparse_degree_time
    })
    
    print(f"  Edges: {len(edges)}")
    print(f"  Dense memory: {dense_memory:.1f} KB")
    print(f"  Sparse memory: {sparse_memory:.1f} KB")
    print(f"  Memory savings: {dense_memory/sparse_memory:.1f}x")
    print(f"  Degree computation speedup: {dense_degree_time/sparse_degree_time:.1f}x")
```

## Visualizing Performance Gains

```{python}
# Extract data for plotting
nodes = [r['nodes'] for r in results]
memory_ratios = [r['memory_ratio'] for r in results]
time_ratios = [r['time_ratio'] for r in results]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Memory efficiency
ax1.plot(nodes, memory_ratios, 'bo-', linewidth=2, markersize=8)
ax1.set_xlabel('Number of Nodes')
ax1.set_ylabel('Memory Savings (Dense/Sparse)')
ax1.set_title('Memory Efficiency of Sparse Matrices')
ax1.grid(True, alpha=0.3)
ax1.set_yscale('log')

# Computational efficiency
ax2.plot(nodes, time_ratios, 'ro-', linewidth=2, markersize=8)
ax2.set_xlabel('Number of Nodes')
ax2.set_ylabel('Speed Improvement (Dense/Sparse)')
ax2.set_title('Computational Efficiency for Degree Calculation')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"Average memory savings: {np.mean(memory_ratios):.1f}x")
print(f"Average speed improvement: {np.mean(time_ratios):.1f}x")
```

## Advanced Operations with Sparse Matrices

### Matrix-Vector Multiplication

One of the most common operations in network analysis is matrix-vector multiplication, used in algorithms like PageRank.

```{python}
# Create a larger sparse network for demonstration
n = 10000
edges = create_sparse_network(n, sparsity=0.0005)
print(f"Created network with {n} nodes and {len(edges)} edges")

# Build sparse matrix
row_idx = [i for i, j in edges] + [j for i, j in edges]
col_idx = [j for i, j in edges] + [i for i, j in edges]
data = [1.0] * (2 * len(edges))
A_sparse = sparse.csr_matrix((data, (row_idx, col_idx)), shape=(n, n))

# Normalize rows (for PageRank-style computation)
row_sums = np.array(A_sparse.sum(axis=1)).flatten()
row_sums[row_sums == 0] = 1  # Avoid division by zero
A_normalized = sparse.diags(1.0 / row_sums) @ A_sparse

print(f"Sparse matrix: {A_sparse.shape}, {A_sparse.nnz} non-zeros")
print(f"Sparsity: {A_sparse.nnz / (n*n):.6f}")

# Matrix-vector multiplication benchmark
x = np.random.random(n)

# Sparse multiplication
start_time = time.time()
y_sparse = A_normalized @ x
sparse_matvec_time = time.time() - start_time

print(f"Sparse matrix-vector multiplication: {sparse_matvec_time:.4f} seconds")
print(f"Result norm: {np.linalg.norm(y_sparse):.6f}")
```

### Finding Connected Components

Sparse matrices excel at graph traversal algorithms:

```{python}
from scipy.sparse.csgraph import connected_components

# Find connected components
n_components, labels = connected_components(A_sparse, directed=False)

print(f"Number of connected components: {n_components}")

# Analyze component sizes
component_sizes = np.bincount(labels)
print(f"Largest component size: {np.max(component_sizes)}")
print(f"Component size distribution: {sorted(component_sizes, reverse=True)[:10]}")
```

### Shortest Path Calculations

```{python}
from scipy.sparse.csgraph import shortest_path

# Calculate shortest paths from node 0 (for a smaller subgraph)
if A_sparse.shape[0] > 1000:
    # Use subgraph for demonstration
    subgraph = A_sparse[:1000, :1000]
else:
    subgraph = A_sparse

distances = shortest_path(subgraph, indices=0, directed=False)
finite_distances = distances[np.isfinite(distances)]

print(f"Shortest paths from node 0:")
print(f"  Reachable nodes: {len(finite_distances)}")
print(f"  Average distance: {np.mean(finite_distances):.2f}")
print(f"  Maximum distance: {np.max(finite_distances):.0f}")
```

## Best Practices for Sparse Network Analysis

### 1. Choose the Right Format

```{python}
# Format selection guide
def recommend_format(operation_type):
    recommendations = {
        "row_operations": "CSR (fast row slicing, matrix-vector multiply)",
        "column_operations": "CSC (fast column slicing, vector-matrix multiply)", 
        "construction": "COO (easy to build from edge lists)",
        "arithmetic": "CSR or CSC (efficient arithmetic operations)",
        "random_access": "DOK or LIL (but convert to CSR/CSC for computation)"
    }
    return recommendations.get(operation_type, "CSR (general purpose)")

operations = ["row_operations", "column_operations", "construction", "arithmetic"]
for op in operations:
    print(f"{op}: {recommend_format(op)}")
```

### 2. Memory-Efficient Construction

```{python}
def build_sparse_efficiently(edges, n_nodes):
    """Efficiently build sparse matrix from edge list"""
    
    # Method 1: Direct COO construction (recommended)
    row_idx = [i for i, j in edges] + [j for i, j in edges]  
    col_idx = [j for i, j in edges] + [i for i, j in edges]
    data = [1] * (2 * len(edges))
    
    matrix = sparse.coo_matrix((data, (row_idx, col_idx)), 
                              shape=(n_nodes, n_nodes))
    return matrix.tocsr()  # Convert to CSR for efficient operations

# Demonstrate with timing
edges = create_sparse_network(5000, sparsity=0.002)

start_time = time.time()
efficient_matrix = build_sparse_efficiently(edges, 5000)
efficient_time = time.time() - start_time

print(f"Efficient construction: {efficient_time:.4f} seconds")
print(f"Matrix shape: {efficient_matrix.shape}")
print(f"Non-zeros: {efficient_matrix.nnz}")
```

### 3. Avoiding Dense Operations

```{python}
# DON'T do this - creates dense matrix!
# bad_result = sparse_matrix.toarray() @ vector

# DO this - keeps everything sparse
A = A_sparse[:1000, :1000]  # Use smaller matrix for demo
x = np.random.random(1000)

# Good: sparse matrix operations
start_time = time.time()
good_result = A @ x  # Stays sparse
good_time = time.time() - start_time

print(f"Sparse operation time: {good_time:.6f} seconds")
print(f"Result type: {type(good_result)}")
print(f"Memory efficient: True")
```

## Real-World Example: Analyzing a Large Network

Let's simulate analyzing a network similar to a social media platform:

```{python}
# Simulate a social network with realistic properties
def create_realistic_network(n_users=50000):
    """Create a network with power-law degree distribution"""
    
    # Generate degrees following power law (realistic for social networks)
    degrees = np.random.zipf(2, n_users) + 1  # Zipf distribution
    degrees = np.minimum(degrees, n_users - 1)  # Cap at n-1
    
    # Make sum even (required for simple graph)
    if np.sum(degrees) % 2 == 1:
        degrees[0] += 1
    
    edges = []
    node_degrees = degrees.copy()
    
    # Simple degree sequence realization (not perfect, but demonstrates concept)
    for i in range(n_users):
        target_degree = min(degrees[i], 50)  # Limit for computational feasibility
        candidates = [j for j in range(n_users) if j != i and node_degrees[j] > 0]
        
        for _ in range(min(target_degree, len(candidates))):
            if not candidates:
                break
            j = np.random.choice(candidates)
            edges.append((i, j))
            node_degrees[j] -= 1
            if node_degrees[j] == 0:
                candidates.remove(j)
            if len(edges) > 100000:  # Limit for demonstration
                break
        
        if len(edges) > 100000:
            break
    
    return edges[:100000]  # Return first 100k edges

print("Creating realistic large network...")
large_edges = create_realistic_network(20000)
n_large = 20000

print(f"Generated {len(large_edges)} edges for {n_large} nodes")

# Build sparse representation
print("Building sparse matrix...")
start_time = time.time()
large_sparse = build_sparse_efficiently(large_edges, n_large)
construction_time = time.time() - start_time

print(f"Construction time: {construction_time:.2f} seconds")
print(f"Matrix size: {large_sparse.shape}")
print(f"Non-zero entries: {large_sparse.nnz:,}")
print(f"Sparsity: {large_sparse.nnz / (n_large**2):.6f}")
print(f"Memory usage: {(large_sparse.data.nbytes + large_sparse.indices.nbytes + large_sparse.indptr.nbytes) / 1024**2:.1f} MB")

# Network analysis
print("\nNetwork analysis:")
degrees = np.array(large_sparse.sum(axis=1)).flatten()
print(f"Average degree: {np.mean(degrees):.2f}")
print(f"Degree standard deviation: {np.std(degrees):.2f}")
print(f"Max degree: {np.max(degrees)}")

# Connected components
n_comp, comp_labels = connected_components(large_sparse, directed=False)
comp_sizes = np.bincount(comp_labels)
print(f"Connected components: {n_comp}")
print(f"Largest component: {np.max(comp_sizes):,} nodes ({100*np.max(comp_sizes)/n_large:.1f}%)")
```

## Integration with NetworkX

For more advanced analysis, sparse matrices integrate seamlessly with NetworkX:

```{python}
import networkx as nx

# Convert sparse matrix to NetworkX graph
G = nx.from_scipy_sparse_array(large_sparse)

print(f"NetworkX graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

# Some quick NetworkX analysis
print(f"Is connected: {nx.is_connected(G)}")
if nx.is_connected(G):
    print(f"Average clustering: {nx.average_clustering(G):.4f}")
    print(f"Average path length: {nx.average_shortest_path_length(G):.2f}")
else:
    # Analyze largest component
    largest_cc = max(nx.connected_components(G), key=len)
    G_large = G.subgraph(largest_cc)
    print(f"Largest component: {G_large.number_of_nodes()} nodes")
    print(f"Average clustering (largest): {nx.average_clustering(G_large):.4f}")
```

## Summary: When and How to Use Sparse Matrices

### Use Sparse Matrices When:
- **Network has > 1000 nodes** with typical sparsity
- **Memory is limited** and network is large
- **Performing many matrix operations** (multiplication, linear algebra)
- **Working with real-world networks** (they're almost always sparse)

### Format Selection Guide:
- **CSR**: Row operations, matrix-vector multiplication, general purpose
- **CSC**: Column operations, vector-matrix multiplication  
- **COO**: Building matrices, format conversion
- **CSR/CSC**: Arithmetic operations, most algorithms

### Performance Tips:
1. **Build once, use many times**: Construction is expensive, operations are fast
2. **Stay sparse**: Avoid `.toarray()` unless absolutely necessary
3. **Use appropriate formats**: CSR for most operations, COO for construction
4. **Leverage SciPy**: Use `scipy.sparse.csgraph` for graph algorithms

Sparse matrices transform network analysis from impossible to practical for real-world scales. They're essential tools for any serious network scientist working with large datasets!

::: {.callout-tip}
## Next Steps

Try applying these techniques to real network datasets:
- Download a network from [SNAP](https://snap.stanford.edu/data/) or [Network Repository](https://networkrepository.com/)
- Practice building sparse representations
- Experiment with different formats and operations
- Compare performance with dense approaches
:::