{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Image to Graph\n",
        "\n",
        "## Analogy between image and graph data\n",
        "We can think of a convolution of an image from the perspective of networks.\n",
        "In the convolution of an image, a pixel is convolved with its *neighbors*. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.\n",
        "\n",
        "![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp)\n",
        "\n",
        "Building on this analogy, we can extend the idea of convolution to general graph data.\n",
        "Each node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph.\n",
        "This is the key idea of graph convolutional networks.\n",
        "But, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the \"kernel\" for graph convolution.\n",
        "\n",
        "## Spectral filter on graphs\n",
        "Just like we can define a convolution on images in the frequency domain, we can also define a ''frequency domain'' for graphs.\n",
        "\n",
        "Consider a network of $N$ nodes, where each node has a feature variable ${\\mathbf x}_i \\in \\mathbb{R}$. We are interested in:\n",
        "\n",
        "$$\n",
        "J = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2,\n",
        "$$\n",
        "\n",
        "where $A_{ij}$ is the adjacency matrix of the graph. The quantity $J$ represents *the total variation* of $x$ between connected nodes; a small $J$ means that connected nodes have similar $x$ (low variation; low frequency), while a large $J$ means that connected nodes have very different $x$ (high variation; high frequency).\n",
        "\n",
        "We can rewrite $J$ as\n",
        "\n",
        "$$\n",
        "J = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\\bf x}^\\top {\\bf L} {\\bf x},\n",
        "$$\n",
        "\n",
        "where ${\\bf L}$ is the Laplacian matrix of the graph given by\n",
        "\n",
        "$$\n",
        "L_{ij} = \\begin{cases}\n",
        "-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\n",
        "k_i & \\text{if } i = j \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}.\n",
        "$$\n",
        "\n",
        "and ${\\bf x} = [x_1,x_2,\\ldots, x_N]^\\top$ is a column vector of feature variables.\n",
        "\n",
        "\n",
        "::: {.callout-note title=\"Detailed derivation\"}\n",
        ":tag: note\n",
        ":class: dropdown\n",
        "\n",
        "The above derivation shows that the total variation of $x$ between connected nodes is proportional to ${\\bf x}^\\top {\\bf L} {\\bf x}$.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "J &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\\\\n",
        "&= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\underbrace{A_{ij}\\left( x_i^2 +x_j^2\\right)}_{\\text{symmetric}} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n",
        "&= \\sum_{i=1}^Nx_i^2\\underbrace{\\sum_{j=1}^N A_{ij}}_{\\text{degree of node } i, k_i} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n",
        "&= \\sum_{i=1}^Nx_i^2 k_i - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n",
        "&= \\underbrace{[x_1,x_2,\\ldots, x_N]}_{{\\bf x}} \\underbrace{\\begin{bmatrix} k_1 & 0 & \\cdots & 0 \\\\ 0 & k_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & k_N \\end{bmatrix}}_{{\\bf D}} \\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}}_{{\\bf x}} - 2\\underbrace{\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}}_{{\\bf x}^\\top {\\mathbf A} {\\bf x}} {\\bf x} \\\\\n",
        "&= {\\bf x}^\\top {\\bf D} {\\bf x} - {\\bf x}^\\top {\\mathbf A} {\\bf x} \\\\\n",
        "&= {\\bf x}^\\top {\\bf L} {\\bf x},\n",
        "\\end{aligned}\n",
        "$$\n",
        ":::\n",
        "\n",
        "Let us showcase the analogy between the Fourier transform and the Laplacian matrix.\n",
        "In the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation $J$ into eigenvector bases.\n",
        "\n",
        "$$\n",
        "J = \\sum_{i=1}^N \\lambda_i  {\\bf x}^\\top {\\mathbf u}_i {\\mathbf u}_i^\\top {\\bf x} = \\sum_{i=1}^N \\lambda_i  ||{\\bf x}^\\top {\\mathbf u}_i||^2.\n",
        "$$\n",
        "\n",
        "where ${\\mathbf u}_i$ is the eigenvector corresponding to the eigenvalue $\\lambda_i$.\n",
        "- The term $({\\bf x}^\\top {\\mathbf u}_i)$ is a dot-product between the feature vector ${\\bf x}$ and the eigenvector ${\\mathbf u}_i$, which measures how much ${\\bf x}$ *coheres* with eigenvector ${\\mathbf u}_i$, similar to how Fourier coefficients measure coherency with sinusoids.\n",
        "- Each $||{\\bf x}^\\top {\\mathbf u}_i||^2$ is the ''strength'' of ${\\bf x}$ with respect to the eigenvector ${\\mathbf u}_i$, and the total variation $J$ is a weighted sum of these strengths.\n",
        "\n",
        "Some eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation $J$ for an eigenvector ${\\mathbf u}_i$ is given by\n",
        "\n",
        "$$\n",
        "J = \\frac{1}{2} \\sum_{j}\\sum_{\\ell} A_{j\\ell}(u_{ij} - u_{i\\ell})^2 = {\\mathbf u}_i^\\top {\\mathbf L} {\\mathbf u}_i = \\lambda_i.\n",
        "$$\n",
        "\n",
        "This equation provides key insight into the meaning of eigenvalues:\n",
        "\n",
        "1. For an eigenvector ${\\mathbf u}_i$, its eigenvalue $\\lambda_i$ measures the total variation for ${\\mathbf u}_i$.\n",
        "2. Large eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).\n",
        "\n",
        "Thus, if ${\\bf x}$ aligns well with ${\\mathbf u}_i$ with a large $\\lambda_i$, then ${\\bf x}$ has a strong high-frequency component; if ${\\bf x}$ aligns well with ${\\mathbf u}_i$ with a small $\\lambda_i$, then ${\\bf x}$ has strong low-frequency component.\n",
        "\n",
        "### Spectral Filtering\n",
        "\n",
        "Eigenvalues $\\lambda_i$ can be thought of as a *filter* that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter $h(\\lambda_i)$ to control which frequency components pass through. This leads to the idea of *spectral filtering*. Two common filters are:\n",
        "\n",
        "1. **Low-pass Filter**:\n",
        "   $$h_{\\text{low}}(\\lambda) = \\frac{1}{1 + \\alpha\\lambda}$$\n",
        "   - Preserves low frequencies (small λ)\n",
        "   - Suppresses high frequencies (large λ)\n",
        "   - Results in smoother signals\n",
        "\n",
        "2. **High-pass Filter**:\n",
        "   $$h_{\\text{high}}(\\lambda) = \\frac{\\alpha\\lambda}{1 + \\alpha\\lambda}$$\n",
        "   - Preserves high frequencies\n",
        "   - Suppresses low frequencies\n",
        "   - Emphasizes differences between neighbors"
      ],
      "id": "817c4394"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        ":tags: [remove-input]\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "alpha = 1\n",
        "lambdas = np.linspace(0, 10, 100)\n",
        "h_low = 1 / (1 + alpha * lambdas)\n",
        "h_high = (alpha * lambdas) / (1 + alpha * lambdas)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "sns.lineplot(x=lambdas, y=h_low, label=\"Low-pass filter\", ax=axes[0])\n",
        "axes[0].legend(frameon=False).remove()\n",
        "sns.lineplot(x=lambdas, y=h_high, label=\"High-pass filter\", ax=axes[1])\n",
        "axes[1].legend(frameon=False).remove()\n",
        "axes[0].set_title(\"Low-pass filter\")\n",
        "axes[1].set_title(\"High-pass filter\")\n",
        "fig.text(0.5, 0.01, \"Eigenvalue $\\lambda$\", ha=\"center\")\n",
        "axes[0].set_ylabel(\"Filter response $h(\\lambda)$\")\n",
        "sns.despine()\n",
        "plt.tight_layout()"
      ],
      "id": "006a865e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example\n",
        "\n",
        "Let us showcase the idea of spectral filtering with a simple example with the karate club network."
      ],
      "id": "63c487ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        ":tags: [remove-input]\n",
        "import igraph as ig\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "import matplotlib as mpl\n",
        "\n",
        "G = ig.Graph.Famous(\"Zachary\")\n",
        "A = G.get_adjacency_sparse()"
      ],
      "id": "e78932fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will first compute the laplacian matrix and its eigendecomposition."
      ],
      "id": "20c131ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute Laplacian matrix\n",
        "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
        "D = sparse.diags(deg)\n",
        "L = D - A\n",
        "\n",
        "# Compute eigendecomposition\n",
        "evals, evecs = np.linalg.eigh(L.toarray())\n",
        "\n",
        "# Sort eigenvalues and eigenvectors\n",
        "order = np.argsort(evals)\n",
        "evals = evals[order]\n",
        "evecs = evecs[:, order]"
      ],
      "id": "06bafbaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's create a low-pass and high-pass filter."
      ],
      "id": "fc386cb4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "alpha = 2\n",
        "L_low = evecs @ np.diag(1 / (1 + alpha * evals)) @ evecs.T\n",
        "L_high = evecs @ np.diag(alpha * evals / (1 + alpha * evals)) @ evecs.T\n",
        "\n",
        "print(\"Size of low-pass filter:\", L_low.shape)\n",
        "print(\"Size of high-pass filter:\", L_high.shape)"
      ],
      "id": "6d8a83dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the high-pass filter and low-pass filter are matrices of the same size as the adjacency matrix $A$, which defines a 'convolution' on the graph as follows:\n",
        "\n",
        "$$\n",
        "{\\bf x}' = {\\bf L}_{\\text{low}} {\\bf x} \\quad \\text{or} \\quad {\\bf x}' = {\\bf L}_{\\text{high}} {\\bf x}.\n",
        "$$\n",
        "\n",
        "where ${\\bf L}_{\\text{low}}$ and ${\\bf L}_{\\text{high}}$ are the low-pass and high-pass filters, respectively, and ${\\bf x}'$ is the convolved feature vector.\n",
        "\n",
        "Now, let's see how these filters work. Our first example is a random feature vector."
      ],
      "id": "3b0bf8d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Random feature vector\n",
        "x = np.random.randn(A.shape[0], 1)\n",
        "\n",
        "# Convolve with low-pass filter\n",
        "x_low = L_low @ x\n",
        "\n",
        "# Convolve with high-pass filter\n",
        "x_high = L_high @ x"
      ],
      "id": "71edc0fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us visualize the results."
      ],
      "id": "8a0fd569"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        ":tags: [hide-input]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "norm = mpl.colors.Normalize(vmin=-0.3, vmax=0.3)\n",
        "\n",
        "# Original\n",
        "values = x.reshape(-1)\n",
        "values /= np.linalg.norm(values)\n",
        "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[0])\n",
        "axes[0].set_title(\"Original\")\n",
        "\n",
        "# Low-pass filter applied\n",
        "values = L_low @ x\n",
        "values /= np.linalg.norm(values)\n",
        "values = values.reshape(-1)\n",
        "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[1])\n",
        "axes[1].set_title(\"Low-pass filter\")\n",
        "\n",
        "# High-pass filter applied\n",
        "values = L_high @ x\n",
        "values /= np.linalg.norm(values)\n",
        "values = values.reshape(-1)\n",
        "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[2])\n",
        "axes[2].set_title(\"High-pass filter\")\n",
        "fig.tight_layout()"
      ],
      "id": "b7bb2ef8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the low-pass filter results in smoother ${\\bf x}$ between connected nodes (i.e., neighboring nodes have similar ${\\bf x}$).\n",
        "The original ${\\bf x}$ and ${\\bf x}'_{\\text{low}}$ are very similar because random variables are high-frequency components. In contrast, when we apply the high-pass filter, ${\\bf x}'_{\\text{high}}$ is similar to ${\\bf x}$ because the high-frequency components are not filtered.\n",
        "\n",
        "Let's now use an eigenvector as our feature vector ${\\bf x}$."
      ],
      "id": "91fea03d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        ":tags: [hide-input]\n",
        "eigen_centrality = np.array(G.eigenvector_centrality()).reshape(-1, 1)\n",
        "low_pass_eigen = L_low @ eigen_centrality\n",
        "high_pass_eigen = L_high @ eigen_centrality\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "\n",
        "norm = mpl.colors.Normalize(vmin=-0, vmax=0.3)\n",
        "values = eigen_centrality.reshape(-1)# high_pass_random.reshape(-1)\n",
        "values /= np.linalg.norm(values)\n",
        "values = values.reshape(-1)\n",
        "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[0])\n",
        "axes[0].set_title(\"Original\")\n",
        "\n",
        "values = low_pass_eigen.reshape(-1)\n",
        "values /= np.linalg.norm(values)\n",
        "values = values.reshape(-1)\n",
        "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[1])\n",
        "axes[1].set_title(\"Low-pass filter\")\n",
        "\n",
        "values = high_pass_eigen.reshape(-1)\n",
        "values /= np.linalg.norm(values)\n",
        "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[2])\n",
        "axes[2].set_title(\"High-pass filter\")\n",
        "fig.tight_layout()"
      ],
      "id": "a21ddba6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The high-pass filter increases the contrast of the eigenvector centrality, emphasizing the differences between nodes. On the other hand, the low-pass filter smooths out the eigenvector centrality."
      ],
      "id": "dbd9c45a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "netdatasci",
      "language": "python",
      "display_name": "netdatasci",
      "path": "/Users/skojaku-admin/Library/Jupyter/kernels/netdatasci"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}