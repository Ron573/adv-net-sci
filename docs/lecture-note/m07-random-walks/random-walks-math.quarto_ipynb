{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Characteristics of Random Walks\n",
        "\n",
        "## Stationary State\n",
        "\n",
        "Let's dive into the math behind random walks in a way that's easy to understand.\n",
        "\n",
        "Imagine you're at node $i$ at time $t$. You randomly move to a neighboring node $j$. The probability of this move, called the transition probability $p_{ij}$, is:\n",
        "\n",
        "$$\n",
        "p_{ij} = \\frac{A_{ij}}{k_i},\n",
        "$$\n",
        "\n",
        "Here, $A_{ij}$ is an element of the adjacency matrix, and $k_i$ is the degree of node $i$. For a network with $N$ nodes, we can represent all transition probabilities in a transition probability matrix $P$:\n",
        "\n",
        "$$\n",
        "\\mathbf{P} = \\begin{pmatrix}\n",
        "p_{11} & p_{12} & \\cdots & p_{1N} \\\\\n",
        "p_{21} & p_{22} & \\cdots & p_{2N} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "p_{N1} & p_{N2} & \\cdots & p_{NN}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "This matrix $P$ encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:\n",
        "\n",
        "- After one step: $P_{ij} = p_{ij}$\n",
        "- After two steps: $\\left(\\mathbf{P}^{2}\\right)_{ij} = \\sum_{k} P_{ik} P_{kj}$\n",
        "- After $T$ steps: $\\left(\\mathbf{P}^{T}\\right)_{ij}$\n",
        "\n",
        "```{note}\n",
        "Let's explore why $\\mathbf{P}^2$ represents the transition probabilities after two steps.\n",
        "\n",
        "First, recall that $\\mathbf{P}_{ij}$ is the probability of moving from node $i$ to node $j$ in one step. Now, consider a two-step walk from $i$ to $j$. We can express this as:\n",
        "\n",
        "$$(\\mathbf{P}^2)_{ij} = \\sum_k \\mathbf{P}_{ik} \\mathbf{P}_{kj}$$\n",
        "\n",
        "This equation encapsulates a key idea: to go from $i$ to $j$ in two steps, we must pass through some intermediate node $k$. Let's break this down step by step:\n",
        "\n",
        "1. The probability of the first step ($i$ to $k$) is $\\mathbf{P}_{ik}$.\n",
        "2. The probability of the second step ($k$ to $j$) is $\\mathbf{P}_{kj}$.\n",
        "3. The probability of this specific path ($i$ → $k$ → $j$) is the product $\\mathbf{P}_{ik} \\mathbf{P}_{kj}$.\n",
        "4. We sum over all possible intermediate nodes $k$ to get the total probability.\n",
        "\n",
        "Likewise, for three steps, we have:\n",
        "\n",
        "$$(\\mathbf{P}^3)_{ij} = \\sum_k \\left( \\mathbf{P}\\right)^2_{ik} \\mathbf{P}_{kj}$$\n",
        "\n",
        "where:\n",
        "1. The probability of going from $i$ to $k$ in two steps is $\\left( \\mathbf{P}\\right)^2_{ik}$.\n",
        "2. The probability of going from $k$ to $j$ in one step is $\\mathbf{P}_{kj}$.\n",
        "3. The probability of this specific path ($i$ →...→$k$ → $j$) is the product $\\left( \\mathbf{P}\\right)^2_{ik} \\mathbf{P}_{kj}$.\n",
        "4. We sum over all possible intermediate nodes $k$ to get the total probability.\n",
        "\n",
        "And we can extend this reasoning for any number of steps $t$.\n",
        "\n",
        "In summary, for any number of steps $t$, $\\left( \\mathbf{P}^t \\right)_{ij}$ gives the probability of being at node $j$ after $t$ steps, starting from node $i$.\n",
        "\n",
        "```\n",
        "\n",
        "As $T$ becomes very large, the probability distribution of being at each node, $\\mathbf{x}(t)$, approaches a constant value:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}(t+1) =\\mathbf{x}(t) \\mathbf{P}\n",
        "$$\n",
        "\n",
        "This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}(\\infty) = \\mathbb{\\pi}, \\; \\mathbf{\\pi} = [\\pi_1, \\ldots, \\pi_N]\n",
        "$$\n",
        "\n",
        "For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:\n",
        "\n",
        "$$\n",
        "\\pi_j = \\frac{k_j}{\\sum_{\\ell} k_\\ell} \\propto k_j\n",
        "$$\n",
        "\n",
        "This means the probability of being at node $j$ in the long run is proportional to the degree of node $j$. The normalization ensures that the sum of all probabilities is 1, i.e., $\\sum_{j=1}^N \\pi_j = 1$.\n",
        "\n",
        "\n",
        "## Experiment\n",
        "\n",
        "Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:"
      ],
      "id": "dd09daba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import igraph as ig\n",
        "import numpy as np\n",
        "edge_list = []\n",
        "for i in range(5):\n",
        "    for j in range(i+1, 5):\n",
        "        edge_list.append((i, j))\n",
        "        edge_list.append((i+5, j+5))\n",
        "edge_list.append((0, 6))\n",
        "\n",
        "g = ig.Graph(edge_list)\n",
        "ig.plot(g, vertex_size=20, vertex_label=np.arange(g.vcount()))"
      ],
      "id": "7012d284",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transition probability matrix $P$ is given by:"
      ],
      "id": "73d24568"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy.sparse as sparse\n",
        "\n",
        "A = g.get_adjacency_sparse()\n",
        "deg = np.array(A.sum(axis=1)).flatten()\n",
        "Dinv = sparse.diags(1/deg)\n",
        "P = Dinv @ A\n",
        "P.toarray()"
      ],
      "id": "021203f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us compute the stationary distribution by using the power method."
      ],
      "id": "348c6e04"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "x = np.zeros(g.vcount())\n",
        "x[1] = 1 # Start from node 1\n",
        "T = 100\n",
        "xt = []\n",
        "for t in range(T):\n",
        "    x = x.reshape(1, -1) @ P\n",
        "    xt.append(x)\n",
        "\n",
        "xt = np.vstack(xt) # Stack the results vertically\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "palette = sns.color_palette().as_hex()\n",
        "for i in range(g.vcount()):\n",
        "    sns.lineplot(x=range(T), y=xt[:, i], label=f\"Node {i}\", ax=ax, color=palette[i])\n",
        "ax.set_xlabel(\"Time\")\n",
        "ax.set_ylabel(\"Probability\")\n",
        "ax.set_title(\"Stationary distribution of a random walk\")\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "3cb0c5ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the distributions of the walker converges, and there are three characteristic features in the convergence:\n",
        "1. The distribution of the walker occilates with a decying amplitude and eventually converges.\n",
        "2. Nodes of the same degree converge to the same stationary probability.\n",
        "3. Nodes with higher degree converge to the higher stationary probability.\n",
        "\n",
        "To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes."
      ],
      "id": "1ffcf332"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "n_edges = np.sum(deg) / 2\n",
        "expected_stationary_dist = deg / (2 * n_edges)\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"Expected stationary distribution\": expected_stationary_dist,\n",
        "    \"Stationary distribution of a random walk\": xt[-1].flatten()\n",
        "}).style.format(\"{:.4f}\").set_caption(\"Comparison of Expected and Observed Stationary Distributions\").background_gradient(cmap='cividis', axis = None)"
      ],
      "id": "cfb9cd14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time to reach the stationary state\n",
        "\n",
        "Let's explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.\n",
        "\n",
        "The mixing time, denoted as $t_{\\text{mix}}$, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than $\\epsilon$:\n",
        "\n",
        "$$t_{\\text{mix}} = \\min\\{t : \\max_{{\\bf x}(0)} \\|{\\bf x}(t) - {\\bf \\pi}\\|_{1} \\leq \\epsilon\\}$$\n",
        "\n",
        "where $\\|{\\bf x}(t) - {\\bf \\pi}\\|_{1} = 2\\max_{i} |x_i(t) - \\pi_i|$ represents the L1 distance between two probability distributions. The choice of $\\epsilon$ is arbitrary.\n",
        "\n",
        "We know that the distribution of a walker after $t$ steps is given by:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}(t) =  \\mathbf{x}(0) \\mathbf{P} ^{t}\n",
        "$$\n",
        "\n",
        "To find this distribution, we need to compute $\\mathbf{P}^t$. However, we face a challenge: $\\mathbf{P}$ is not diagonalizable.\n",
        "\n",
        "A diagonalizable matrix $\\mathbf{S}$ can be written as $\\mathbf{S} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}$, where $\\mathbf{\\Lambda}$ is a diagonal matrix and $\\mathbf{Q}$ is an orthogonal matrix. Visually, it looks like this:\n",
        "\n",
        "![](../figs/diagonalizable.jpg)\n",
        "\n",
        "It is useful because we can then compute the power of the matrix as follows:\n",
        "\n",
        "$$\n",
        "\\mathbf{S}^t = \\mathbf{Q} \\mathbf{\\Lambda}^t \\mathbf{Q}^{-1}\n",
        "$$\n",
        "\n",
        "And it is easy to find ${\\bf Q}$ and ${\\bf \\Lambda}$ by using eigenvalue decomposition if ${\\bf S}$ is symmetric and consists only of real values. Namely, the eigenvectors form ${\\cal Q}$ and the eigenvalues form the diagonal matrix ${\\cal \\Lambda}$.\n",
        "\n",
        "```{note}\n",
        "Let us demonstrate the above relation by calculating $\\mathbf{S}^2$.\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{S}^2 &= \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1} \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1} \\\\\n",
        "&= \\mathbf{Q} \\mathbf{\\Lambda}^2 \\mathbf{Q}^{-1}.\n",
        "\\end{align}\n",
        "$$\n",
        "(Note that $\\mathbf{Q} \\mathbf{Q}^{-1} = {\\bf I}$.)\n",
        "\n",
        "![](../figs/diagonalizable-squared.jpg)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "$\\mathbf{P}$ is also diagonalizable but not symmetric like $\\mathbf{\\overline A}$ so that we cannot use the above relation directly. So we do a trick by rewriteing $\\mathbf{P}$ as:\n",
        "\n",
        "$$\n",
        "\\mathbf{P} = \\mathbf{D}^{-1} \\mathbf{A} = \\mathbf{D}^{-\\frac{1}{2}} \\overline {\\bf A} \\mathbf{D}^{\\frac{1}{2}}\n",
        "$$\n",
        "\n",
        "where $\\overline{\\bf A} = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}$ is the normalized adjacency matrix.\n",
        "\n",
        "The advantage is that $\\overline{\\bf A}$ is diagonalizable: $\\overline{\\bf A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top$. Using this, we can compute $\\mathbf{P}^t$:\n",
        "\n",
        "$$\n",
        "\\mathbf{P}^t = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{Q} \\mathbf{\\Lambda}^t \\mathbf{Q}^\\top \\mathbf{D}^{\\frac{1}{2}} = \\mathbf{Q}_L \\mathbf{\\Lambda}^t \\mathbf{Q}_R ^\\top\n",
        "$$\n",
        "\n",
        "where $\\mathbf{Q}_L = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{Q}$ and $\\mathbf{Q}_R = \\mathbf{D}^{\\frac{1}{2}} \\mathbf{Q}$.\n",
        "\n",
        "\n",
        "```{note}\n",
        "Let us demonstrate the above relation by calculating $\\mathbf{P}^2$.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{P}^2 &= \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} \\mathbf{D}^{\\frac{1}{2}} \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} \\mathbf{D}^{\\frac{1}{2}}\\\\\n",
        "&=  \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} ^2 \\mathbf{D}^{\\frac{1}{2}}\\\\\n",
        "&= \\mathbf{Q}_L \\mathbf{\\Lambda}^2 \\mathbf{Q}_R ^\\top\n",
        "\\end{align}\n",
        "```\n",
        "\n",
        "The probability distribution after $t$ steps is then:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}(t) = \\mathbf{x}(0) \\mathbf{Q}_L \\mathbf{\\Lambda}^t \\mathbf{Q}_R ^\\top\n",
        "$$\n",
        "\n",
        "We can rewrite this in a more intuitive form:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "x_1(t) \\\\\n",
        "x_2(t) \\\\\n",
        "\\vdots \\\\\n",
        "x_N(t)\n",
        "\\end{pmatrix}\n",
        " =\n",
        " \\sum_{\\ell=1}^N\n",
        " \\left[\n",
        " \\lambda_\\ell^t\n",
        " \\begin{pmatrix}\n",
        " q^{(L)}_{\\ell 1} \\\\\n",
        " q^{(L)}_{\\ell 2} \\\\\n",
        " \\vdots \\\\\n",
        " q^{(L)}_{\\ell N}\n",
        " \\end{pmatrix}\n",
        " \\langle\\mathbf{q}^{(R)}_{\\ell},  \\mathbf{x}(0) \\rangle\n",
        " \\right]\n",
        "$$\n",
        "\n",
        "```{note}\n",
        "Visualize the above equation by using the following figure.\n",
        "\n",
        "![](../figs/diagonalizable-sum.jpg)\n",
        "\n",
        "```\n",
        "\n",
        "The term $\\lambda_\\ell^t$ represents the contribution of each eigenvalue to the stationary distribution over time. As $t$ increases, all terms decay exponentially except for the largest eigenvalue ($\\lambda_1 = 1$). This explains how the random walk converges to the stationary distribution:\n",
        "\n",
        "$$\n",
        "\\pi_i = \\lim_{t\\to\\infty} x_i(t) = \\begin{pmatrix} q^{(L)}_{1 1} \\\\ q^{(L)}_{1 2} \\\\ \\vdots \\\\ q^{(L)}_{1 N} \\end{pmatrix} \\langle\\mathbf{q}^{(R)}_{1},  \\mathbf{x}(0) \\rangle\n",
        "$$\n",
        "\n",
        "The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.\n",
        "\n",
        "Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as\n",
        "\n",
        "$$\n",
        "t_{\\text{mix}} < \\tau \\log \\left( \\frac{1}{\\epsilon \\min_{i} \\pi_i} \\right), \\quad \\tau = \\frac{1}{1-\\lambda_2}\n",
        "$$\n",
        "\n",
        "where $\\lambda_2$ is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as\n",
        "\n",
        "More commonly, it is expressed using the second smallest eigenvalue $\\mu$ of the normalized laplacian matrix as\n",
        "\n",
        "$$\n",
        "t_{\\text{mix}} \\leq \\frac{1}{\\mu}\n",
        "$$\n",
        "\n",
        "where $\\mu = 1-\\lambda_2$.\n",
        "\n",
        "\n",
        "### Compute the mixing time\n",
        "\n",
        "Let us demonstrate the above math by using the network of two cliques.\n",
        "\n",
        "### Normalized Adjacency Matrix\n",
        "\n",
        "First, let us construct the normalized adjacency matrix $\\overline{\\bf A}$ of the network."
      ],
      "id": "2d64d998"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Dinv_sqrt = sparse.diags(1.0/np.sqrt(deg))\n",
        "A_norm = Dinv_sqrt @ A @ Dinv_sqrt"
      ],
      "id": "b7f5ce1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix."
      ],
      "id": "26b75e0c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "evals, evecs = np.linalg.eigh(A_norm.toarray())"
      ],
      "id": "73949af7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{note}\n",
        "`evals` and `evecs` are sorted in descending order of the eigenvalues. `evecs[:, 0]` is the eigenvector corresponding to the largest eigenvalue, which is always 1.\n",
        "```\n",
        "\n",
        "```{warning}\n",
        "There is a similar function called `np.linalg.eig` which returns the eigenvalues and eigenvectors. It can be used for any matrices, while `np.linalg.eigh` is specifically for symmetric matrices. `np.linalg.eigh` is faster and more stable and therefore preferred if your matrix is symmetric. `np.linalg.eig` is more susceptible to numerical errors and therefore less stable.\n",
        "```\n",
        "\n",
        "The eigenvalues and eigenvectors are shown below."
      ],
      "id": "fa26a001"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame({\n",
        "    \"Eigenvalue\": evals\n",
        "}).T.style.background_gradient(cmap='cividis', axis = 1).set_caption(\"Eigenvalues of the normalized adjacency matrix\")"
      ],
      "id": "80fc02e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame({\n",
        "    \"Eigenvector %i\" % i: evecs[:, i]\n",
        "    for i in range(10)\n",
        "}).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Eigenvectors of the normalized adjacency matrix\")"
      ],
      "id": "91b3209e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix.\n",
        "The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.\n",
        "\n",
        "```{note}\n",
        "The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, `np.linalg.eigh` returns the eigenvector whose sign can vary for a different run.\n",
        "```\n",
        "\n",
        "We decompose $\\overline{\\bf A}$ as\n",
        "\n",
        "$$\\overline {\\bf A} = {\\bf Q}{\\bf \\Lambda}{\\bf Q}^\\top$$\n",
        "\n",
        "where ${\\bf Q}$ corresponds to `eigvecs` and ${\\bf \\Lambda}$ corresponds to `np.diag(evals)` (since ${\\bf \\Lambda}$ is a diagonal matrix). Let's see if this is correct:"
      ],
      "id": "1d8df882"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(A_norm.toarray()).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Normalized Adjacency Matrix\")"
      ],
      "id": "3464e1d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "A_norm_reconstructed = evecs @ np.diag(evals) @ evecs.T\n",
        "pd.DataFrame(A_norm_reconstructed).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Reconstruction of the Normalized Adjacency Matrix\")"
      ],
      "id": "1cf67444",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.\n",
        "\n",
        "### Multi-step Transition Probability\n",
        "\n",
        "Let us first conform whether we can compute the transition probability after $t$ steps by using the eigenvalues and eigenvectors."
      ],
      "id": "5b90332e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "t = 5\n",
        "x_0 = np.zeros(g.vcount())\n",
        "x_0[0] = 1\n",
        "\n",
        "# Compute x_t by using the eigenvalues and eigenvectors\n",
        "Q_L = np.diag(1.0/np.sqrt(deg)) @ evecs\n",
        "Q_R = np.diag(np.sqrt(deg)) @ evecs\n",
        "x_t = x_0 @ Q_L @ np.diag(evals**t) @ Q_R.T\n",
        "\n",
        "# Compute x_t by using the power iteration\n",
        "x_t_power = x_0.copy()\n",
        "for i in range(t):\n",
        "    x_t_power = x_t_power @ P\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"Eigenvector\": x_t.flatten(),\n",
        "    \"Power iteration\": x_t_power.flatten()\n",
        "}).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Comparison of Eigenvector and Power Iteration\")"
      ],
      "id": "9752bb26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relaxation Time and Mixing Time\n",
        "\n",
        "Let us measure the relaxation time of the random walk."
      ],
      "id": "1dcca962"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "evals, evecs = np.linalg.eigh(A_norm.toarray())\n",
        "lambda_2 = -np.sort(-evals)[1]\n",
        "tau = 1 / lambda_2\n",
        "print(f\"The relaxation time of the random walk is {tau:.4f}.\")"
      ],
      "id": "8022c69c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "netdatasci",
      "language": "python",
      "display_name": "netdatasci",
      "path": "/Users/skojaku-admin/Library/Jupyter/kernels/netdatasci"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}