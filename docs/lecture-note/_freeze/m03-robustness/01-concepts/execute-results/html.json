{
  "hash": "84d44911591a5bb10a1fe704d54b94d2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Network Robustness: Core Concepts\"\nbibliography: ../references.bib\nfilters:\n  - marimo-team/marimo\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\n## What to learn in this module\n\nIn this module, we will explore network robustness through the lens of infrastructure design. Starting from the historical challenge of building cost-effective power grids, we will learn:\n\n- How minimum spanning trees provide optimal cost-efficiency for network connectivity\n- Why real-world networks have redundancies beyond minimum connectivity requirements\n- How networks respond to random failures versus targeted attacks\n- Quantitative measures of network robustness and percolation theory\n- Design principles for balancing cost efficiency with resilience\n\n**Keywords**: minimum spanning tree, Kruskal's algorithm, Prim's algorithm, network redundancy, random failures, targeted attacks, connectivity loss, R-index, percolation, robustness paradox\n\n## Pen-and-Paper Exercise & Interactive Visualization\n\n- ‚úçÔ∏è [Pen and Paper Exercise](./pen-and-paper/exercise.pdf): Starting with a minimum spanning tree for cost efficiency, design a power grid network that maintains connectivity even when key components fail.\n\n- [üöÄ Interactive Visualization on Network Robustness](../assets/vis/network-robustness.html)\n\n## Power Grid Design Challenge\n\nIn the aftermath of World War I, the newly formed Czechoslovakia faced massive reconstruction challenges. Cities and towns across [Moravia](https://en.wikipedia.org/wiki/Moravia) needed electricity, but the young nation had limited resources. Every resources spent on unnecessary infrastructure was a resource not available for hospitals, schools, or economic recovery. Engineers at the West Moravian Power Company faced a critical question: How do you connect every town and village to the electrical grid while using the minimum length of cable?\n\n::: {.column-margin}\n**Otakar Bor≈Øvka** (1899-1995) was a Czech mathematician who is best known for his work on the minimum spanning tree problem.\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Otakar_Boruvka_1981.jpg/500px-Otakar_Boruvka_1981.jpg)\n:::\n\n\n\nThe problem reached mathematician Otakar Bor≈Øvka through his friend at the power company. Bor≈Øvka's 1926 solution [@boruvka1926jistem] gave us the first systematic approach to what we now call **the minimum spanning tree problem**: finding the cheapest way to connect all locations in a network.\n\n#### Minimum Spanning Tree\n\nA **minimum spanning tree (MST)** of a weighted network is a tree that:\n\n- **Spans** all nodes (connects every location in the network)\n- Is a **tree** (connected with no cycles - no redundant loops)\n- Has **minimum total weight** among all possible spanning trees\n\nOtakar Bor≈Øvka delivered the first algorithm to solve this problem: **Bor≈Øvka's algorithm**. But it is not the only algorithm to find the minimum spanning tree.\nIn fact, there are several algorithms. We will cover two algorithms: **Kruskal's algorithm** and **Prim's algorithm**, which are easier to understand and implement.\n\n### Finding the Minimum Spanning Tree\n\n::: {.column-margin}\n<iframe width=\"250\" height=\"150\" src=\"https://www.youtube.com/embed/8i2XsxU-VL4?si=UGZH5hgy09Jhsjru\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n\n:::\n\n#### Kruskal's Algorithm\n\n**Kruskal's algorithm** embodies a remarkably simple yet powerful intuition: always choose the cheapest available option, but never create wasteful loops. While sounds heuristic, this algorithm in fact leads to the global optimial solution!\n\nThe algorithm works by first sorting every possible connection from cheapest to most expensive like arranging all the cable segments by cost. Then, it examines each connection in order, asking a crucial question: \"If I add this cable, will it create a redundant loop?\" If the answer is no, the cable joins the growing network. If adding it would create a cycle---meaning the two locations are already connected through some other path---the algorithm skips it as wasteful. This process continues until every location is connected, guaranteeing both minimum cost and complete coverage.\n\n#### Prim's Algorithm\n\n**Prim's algorithm** takes a fundamentally different approach, embodying the intuition of organic growth from a single starting point. Picture an engineer beginning at the central power plant and asking: \"What's the cheapest way to connect one more location to our existing grid?\" This local growth strategy builds the network incrementally, always expanding from what's already been constructed.\n\nThe algorithm begins by selecting any location as its starting point, often the power plant in our analogy. From this initial seed, it repeatedly identifies the cheapest connection that would bring a new, unconnected location into the growing network. Unlike Kruskal's global view, Prim's algorithm maintains a clear distinction between locations already in the network and those still waiting to be connected. At each step, it finds the minimum-cost bridge between these two groups, gradually expanding the connected region until it encompasses every location.\n\nThis local expansion strategy mirrors how many real-world infrastructure projects actually develop. Engineers often start from existing facilities and expand outward, always seeking the most cost-effective way to serve additional areas. Prim's algorithm formalizes this natural growth process.\n\n```python {.marimo}\nmo.vstack(\n    [\n        unique_weights,\n        time_step,\n        mo.md(\n            \"**Instructions**: Move the slider to see how each algorithm builds the MST step by step.\"\n        ),\n    ]\n)\n```\n\n```python {.marimo}\nfig\n```\n\n\n\n\n::: {.callout-note}\n\nTwo algorithms find the same minimum spanning tree when all connection costs are different. If there are connections with the same cost, there are multiple minimum spanning trees of the same cost, and which tree to find depends on the algorithm. In particular, Prim's algorithm finds different trees when starting from different locations.\n\n:::\n\n\n\n## Why Minimum Spanning Tree is Not Enough\n\nMinimum spanning tree is an efficient way to connect all locations in a network with the minimum total cost. However, such a network is vulnerable to failures, e.g., the network can be disconnected when a single node fails, in particular those close to the center of the network. This is why our power grid has a lot of redundancies beyond the minimum spanning tree, for the sake of resilience against failures.\n\n\n::: {#fig-us-powe-grid}\n\n![](https://www.geni.org/globalenergy/library/national_energy_grid/united-states-of-america/graphics/UnitedStatesPowerGrid.jpg){width=50%}\n\nThis is the power grid of the United States.\n\n:::\n\n### Measuring Network Damage\n\nNot every failure is equal. Some failures are more damaging than others. For example, removing somee nodes in a grid network can be catastrophic, while removing other nodes can be more tolerable.\n\nWhile there can be many metrics to quantify network damage, we will focus on a purely topological metric: the fraction of nodes remaining in the largest connected component after removal.\n\n$$\n\\text{Connectivity} = \\frac{\\text{Size of largest component after removal}}{\\text{Original network size}}\n$$\n\n![](../figs/single-node-failure.jpg){#fig-single-node-failure fig-alt=\"The impact of removing a single node varies based on which node is removed.\"}\n\nThe **robustness profile** plots connectivity against the fraction of nodes removed, revealing how networks fragment. Crucially, the shape of this profile depends entirely on the **order** in which nodes are removed - random removal creates one pattern, while strategic targeting creates dramatically different patterns.\n\n![](../figs/robustness-profile.jpg){#fig-multiple-node-failure fig-alt=\"Robustness profile of a network for a sequential failure of nodes.\"}\n\nTo compare networks with a single metric, we use the **R-index** - the area under this curve:\n\n$$\nR = \\frac{1}{N} \\sum_{k=1}^{N-1} y_k\n$$\n\nThe robustness index is a measure of how robust a network is to under a sequential failure of nodes. The higher the R-index, the more robust the network is.\n\nNetworks can exhibit different robustness profiles under different attack strategies. One form of an attack is a **random failure**, where nodes are removed randomly. Another form of an attack is a **targeted attack**, where nodes are removed strategically.\n\nRandom failures are like earthquakes or equipment malfunctions; they strike unpredictably. In power grids, generators might fail due to technical problems. In computer networks, servers might crash randomly.\n\nEven if a network survives random failures beautifully, it might crumble under **targeted attacks**. Adversaries strategically choose which nodes to attack for maximum damage. The most intuitive strategy targets **high-degree nodes** (hubs) first, i.e., like targeting the busiest airports to disrupt air travel.\n\n::: {.column-margin}\nThe asymmetry between random failures and targeted attacks is one of the most counterintuitive discoveries in network science. A network that seems robust can have hidden vulnerabilities that smart adversaries can exploit.\n:::\n\n## Theoretical Framework for Network Robustness\n\nTo understand these patterns mathematically, we can view network attacks as the **reverse process of percolation**. **Percolation theory** studies phase transitions in connectivity by asking: as we randomly add nodes to a grid, when does a giant connected component emerge? Network robustness asks the opposite: as we remove nodes, when does the giant component disappear?\n\n::: {.column-margin}\nPercolation theory originated in physics to understand how liquids flow through porous materials. The same mathematics explains how networks fragment under node removal - a beautiful example of how physics concepts illuminate network behavior.\n:::\n\n```python {.marimo}\np_slider\n```\n\n```python {.marimo}\npercolation_visualization()\n```\n\n::: {.column-margin}\n\nWant to see this in action? üåü Check out this interactive simulation.\nPlay around with it and watch how the puddles grow and connect. üåä\n\n[Bernoulli Percolation Simulation üåê](https://visualize-it.github.io/bernoulli_percolation/simulation.html) üîó\n\n:::\n\n::: {.column-margin}\n**Percolation vs. Robustness: Two Sides of the Same Coin**\n\nPercolation theory asks: *\"Starting from isolation, how many nodes must we connect to form a giant component?\"* - increasing connectivity from $p = 0$ to $p = 1$.\n\nRobustness analysis asks: *\"Starting from full connectivity, how many nodes must we remove to fragment the network?\"* - decreasing connectivity from $p = 1$ to $p = 0$.\n\nThese are mathematically equivalent processes, just viewed in opposite directions along the same connectivity parameter.\n:::\n\n### The Phase Transition\n\nImagine a grid where each square randomly becomes a \"puddle\" with probability $p$. As $p$ increases, something dramatic happens - suddenly, a giant puddle spanning the entire grid appears! This **phase transition** occurs at a critical probability $p_c$. Crucially, the exact timing doesn't matter; only the fraction of nodes present or removed determines connectivity.\n\n```python {.marimo}\np_slider\n```\n\n```python {.marimo}\nphase_transition_plot()\n```\n\n\n### The Molloy-Reed Criterion\n\nFor networks with arbitrary degree distributions, the **Molloy-Reed criterion** [@molloy1995critical] determines whether a **giant component** exists - that is, whether the network contains a single large connected component that includes most of the nodes:\n\n$$\n\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} > 2\n$$\n\nwhere $\\langle k \\rangle$ is the average degree and $\\langle k^2 \\rangle$ is the average of squared degrees. The ratio $\\kappa$ measures **degree heterogeneity** - networks with hubs have high $\\kappa$, while degree homogeneous networks have low $\\kappa$. When $\\kappa > 2$, a giant component forms that dominates the network connectivity. See the Appendix for the proof of the Molloy-Reed criterion.\n\n::: {.column-margin}\n\nDegree distributions with different $\\kappa$ values with the same mean $\\langle k \\rangle=5$\n\n::: {#b06808fb .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n![](01-concepts_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\n:::\n\nThe Molloy-Reed criterion is a powerful tool to predict the existence of a giant component in a network and allows us to find the critical fraction of nodes that must be **removed** to break the network. This critial fraction depends on the strategy of the attack, along with the degree distribution. For simplicity, let us restrict ourselves into the random failures. For the random failure case, the critical fraction is given by:\n\n$$\nf_c = 1 - \\frac{1}{\\kappa - 1}\n$$\n\nThe value of $\\kappa$ depends on the degree distribution, and below, we showcase two examples of degree distributions.\n\n#### Degree homogeneous network\n\nIn case of a degree homogeneous network like a random network considered in the exercise above, the critical fraction is given by:\n\n$$\nf_c = 1 - \\frac{1}{\\langle k \\rangle}\n$$\n\ngiven that $\\langle k^2 \\rangle = \\langle k \\rangle^2$ and thus $\\kappa = \\langle k \\rangle$. This suggests that the threshold is determined by the average degree $\\langle k \\rangle$. A large $\\langle k \\rangle$ results in a larger $f_c$, meaning that the network is more robust against random failures.\n\n\n#### Degree heterogeneous network\n\nMost real-world networks are degree heterogeneous, i.e., the degree distribution $P(k) \\sim k^{-\\gamma}$ follows a power law (called *scale-free* network).\nThe power-law degree distribution has infinite second moment, i.e., $\\langle k^2 \\rangle = \\infty$ and thus $f_c = 1.0$, which means that all nodes must be removed to break the network into disconnected components.\nThis is the case where the number of nodes is infinite (i.e., so that a node has a very large degree for the degree distribution to be a valid power law). When restricting the maximum degree to be finite, the critical fraction is given by:\n\n$$\nf_c =\n\\begin{cases}\n1 - \\dfrac{1}{\\frac{\\gamma-2}{3-\\gamma} k_{\\text{min}} ^{\\gamma-2} k_{\\text{max}}^{3-\\gamma} -1} & \\text{if } 2 < \\gamma < 3 \\\\\n1 - \\dfrac{1}{\\frac{\\gamma-2}{\\gamma-3} k_{\\text{min}} - 1} & \\text{if } \\gamma > 3 \\\\\n\\end{cases}\n$$\n\nwhere $k_{\\text{min}}$ and $k_{\\text{max}}$ are the minimum and maximum degree, respectively.\nThe variable $\\gamma$ is the exponent of the power law degree distribution, controlling the degree heterogeneity, where a lower $\\gamma$ results in a more degree heterogeneous network.\n\n::: {.column-margin}\n\n::: {#0281e808 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](01-concepts_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n:::\n\n- For regime $2 < \\gamma < 3$, the critical threshold $f_c$ is determi\nned by the extreme values of the degree distribution, $k_{\\text{min}}$ and $k_{\\text{max}}$.\nAnd $f_c \\rightarrow 1$ when the maximum degree $k_{\\text{max}} \\in [k_{\\text{min}}, N-1]$ increases.\nNotably, in this regime, the maximum degree $k_{\\text{max}}$ increases as the network size $N$ increases, and this makes $f_c \\rightarrow 1$.\n\n- For regime $\\gamma > 3$, the critical threshold $f_c$ is influenced by the minimum degree $k_{\\text{min}}$. In contrast to $k_{\\text{max}}$, $k_{\\text{min}}$ remains constant as the network size $N$ grows. Consequently, the network disintegrates when a finite fraction of its nodes are removed.\n\n\n\n### Robustness Under Attack\n\nWhile scale-free networks show remarkable robustness against random failures [@cohen2000resilience], they exhibit a fundamental vulnerability to targeted attacks that deliberately target high-degree nodes (hubs) [@albert2000error] [@cohen2001breakdown]. This asymmetry reveals the \"Achilles' heel\" property of complex networks, where the same structural features that provide robustness against random failures create critical vulnerabilities to strategic attacks.\n\nRather than removing nodes randomly, an adversary with knowledge of the network structure can systematically remove the highest-degree nodes first, followed by the next highest-degree nodes, and so on. Under this targeted hub removal strategy, scale-free networks fragment rapidly and dramatically. The critical threshold for attacks, $f_c^{\\text{attack}}$, is dramatically lower than for random failures. While random failures require $f_c^{\\text{random}} \\approx 1$ (nearly all nodes must be removed), targeted attacks need only $f_c^{\\text{attack}} \\ll 1$ (a small fraction of hubs) to fragment the network.\n\nTo understand how networks fragment under targeted attacks, we must consider two key effects that occur when the highest-degree nodes are systematically removed. First, the removal of hub nodes changes the maximum degree of the remaining network from $k_{\\max}$ to a new lower value $k'_{\\max}$. Second, since these removed hubs had many connections, their elimination also removes many links from the network, effectively changing the degree distribution of the surviving nodes.\n\nThe mathematical analysis of this process relies on mapping the attack problem back to the random failure framework through careful accounting of these structural changes. When we remove an $f$ fraction of the highest-degree nodes in a scale-free network, the new maximum degree becomes $k'_{\\max} = k_{\\min} f^{1/(1-\\gamma)}$, where the power-law exponent $\\gamma$ determines how rapidly the degree sequence declines.\n\nFor scale-free networks with degree exponent $\\gamma$, the critical attack threshold $f_c$ satisfies:\n\n$$\nf_c^{\\frac{2-\\gamma}{1-\\gamma}} = \\frac{2 + 2^{-\\gamma}}{3-\\gamma} k_{\\min} \\left(f_c^{\\frac{3-\\gamma}{1-\\gamma}} - 1\\right)\n$$\n\nThe fractional exponents $(2-\\gamma)/(1-\\gamma)$ and $(3-\\gamma)/(1-\\gamma)$ arise from the power-law degree distribution and determine how quickly the network fragments as hubs are removed. For networks with $\\gamma < 3$ (highly heterogeneous degree distributions), these exponents are negative, leading to extremely small values of $f_c$, i.e., meaning just a tiny fraction of hub removal can destroy network connectivity.\n\nThis vulnerability has profound real-world implications across multiple domains. Power grids invest heavily in protecting major substations and transmission hubs because their failure could cascade throughout the system. Internet infrastructure includes hub redundancy and protection protocols to maintain connectivity when major routing nodes are compromised. Transportation networks maintain backup routes and alternative pathways when major airports or train stations fail. Even biological systems have evolved protective mechanisms for critical proteins that serve as hubs in cellular networks.\n\nThe robustness paradox demonstrates that no single network structure can be optimal against all types of failures. There's always a fundamental trade-off between efficiency, which naturally favors hub-based architectures for optimal resource distribution, and security, which requires redundancy and distributed connectivity to prevent catastrophic failures from targeted attacks.\n\n## Design Principles for Robust Networks\n\nHow do we design networks that resist both random failures and targeted attacks? Key principles include:\n\n1. **Balanced Degree Distribution**: Avoid both extreme homogeneity and extreme hub concentration\n2. **Multiple Redundant Pathways**: Ensure removing any single node doesn't isolate large portions\n3. **Strategic Hub Protection**: In hub-based networks, invest heavily in protecting critical nodes\n4. **Hierarchical Design**: Combine local clusters with hub connections and redundant backbones\n5. **Adaptive Responses**: Design systems that can reconfigure when attacks are detected\n\nThese strategies reflect lessons learned from our historical power grid challenge: moving beyond the minimum spanning tree to create networks that balance efficiency with resilience.\n\n\n<!--\n========================================\nCode for the interactive visualizations\n========================================\n-->\n\n\n```python {.marimo}\nimport marimo as mo\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom scipy.ndimage import label\nfrom matplotlib.colors import ListedColormap\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import connected_components\n\n```\n\n```python {.marimo}\n# Control for edge weight uniqueness\nunique_weights = mo.ui.checkbox(\n    label=\"Use unique edge weights (when unchecked, some edges have same weight)\",\n    value=True,\n)\n\n# Time step slider - fixed to accommodate MST (max 6 edges for 7 nodes)\ntime_step = mo.ui.slider(\n    start=0,\n    stop=6,  # Fixed max for power grid (7 nodes = 6 MST edges)\n    step=1,\n    value=6,  # Start at final state\n    label=\"Algorithm Time Step (0=start, 6=complete MST)\",\n)\n```\n\n```python {.marimo}\ndef create_power_grid_graph(use_unique_weights=True):\n    \"\"\"Create power grid graph with nodes and edges\"\"\"\n\n    # Define nodes with positions\n    nodes = {\n        \"A\": (0, 1),\n        \"B\": (1, 2),\n        \"C\": (1, 0),\n        \"D\": (2, 2.5),\n        \"E\": (2, 1.5),\n        \"F\": (2, 0.5),\n        \"G\": (2, -0.5),\n    }\n\n    if use_unique_weights:\n        # All weights are unique\n        edges = [\n            (\"A\", \"B\", 8),\n            (\"A\", \"C\", 12),\n            (\"B\", \"D\", 5),\n            (\"B\", \"E\", 7),\n            (\"C\", \"F\", 6),\n            (\"C\", \"G\", 4),\n            (\"D\", \"E\", 3),\n            (\"E\", \"F\", 9),\n            (\"F\", \"G\", 2),\n            (\"D\", \"C\", 11),\n        ]\n    else:\n        # Some weights are the same - multiple MSTs possible\n        edges = [\n            (\"A\", \"B\", 8),\n            (\"A\", \"C\", 12),\n            (\"B\", \"D\", 5),\n            (\"B\", \"E\", 7),\n            (\"C\", \"F\", 6),\n            (\"C\", \"G\", 4),\n            (\"D\", \"E\", 3),\n            (\"E\", \"F\", 11),\n            (\"F\", \"G\", 2),\n            (\"D\", \"C\", 11),\n        ]\n\n    return nodes, edges\n\n\n# Create the graph based on current setting\nnodes, edges = create_power_grid_graph(unique_weights.value)\n\ndef kruskal_algorithm(nodes_dict, edges_list):\n    \"\"\"Kruskal's algorithm implementation without networkx\"\"\"\n\n    # Step 1: Sort edges by weight (global perspective)\n    sorted_edges = sorted(edges_list, key=lambda x: x[2])\n\n    # Initialize Union-Find data structure\n    parent = {}\n    rank = {}\n\n    # Initialize all nodes in Union-Find\n    for node in nodes_dict:\n        parent[node] = node\n        rank[node] = 0\n\n    def find(x):\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n\n    def union(x, y):\n        px, py = find(x), find(y)\n        if px == py:\n            return False\n        if rank[px] < rank[py]:\n            px, py = py, px\n        parent[py] = px\n        if rank[px] == rank[py]:\n            rank[px] += 1\n        return True\n\n    mst_edges = []\n    steps = []\n\n    for u, v, weight in sorted_edges:\n        if union(u, v):\n            mst_edges.append((u, v, weight))\n            steps.append(\n                {\n                    \"edge\": (u, v),\n                    \"weight\": weight,\n                    \"action\": \"added\",\n                    \"reason\": f\"Connects {u} and {v} without creating cycle\",\n                }\n            )\n\n        # Continue until we have a spanning tree OR all edges are processed\n        if len(mst_edges) == len(nodes_dict) - 1:\n            break\n\n    return mst_edges, steps\n\n\ndef prim_algorithm(nodes_dict, edges_list, start_node=\"A\"):\n    \"\"\"Prim's algorithm implementation without networkx\"\"\"\n\n    # Create adjacency list\n    adj = {node: [] for node in nodes_dict}\n    for u, v, weight in edges_list:\n        adj[u].append((v, weight))\n        adj[v].append((u, weight))\n\n    visited = {start_node}\n    mst_edges = []\n    steps = []\n\n    steps.append(\n        {\n            \"node\": start_node,\n            \"action\": \"start\",\n            \"reason\": f\"Starting from {start_node}\",\n        }\n    )\n\n    while len(visited) < len(nodes_dict):\n        min_weight = float(\"inf\")\n        min_edge = None\n\n        # Find cheapest edge from visited to unvisited nodes\n        for node in visited:\n            for neighbor, weight in adj[node]:\n                if neighbor not in visited and weight < min_weight:\n                    min_weight = weight\n                    min_edge = (node, neighbor, weight)\n\n        if min_edge:\n            u, v, weight = min_edge\n            visited.add(v)\n            mst_edges.append((u, v, weight))\n            steps.append(\n                {\n                    \"edge\": (u, v),\n                    \"weight\": weight,\n                    \"action\": \"added\",\n                    \"reason\": f\"Cheapest connection from visited set to {v}\",\n                }\n            )\n\n    return mst_edges, steps\n\n\n# Run both algorithms\nkruskal_mst, kruskal_steps = kruskal_algorithm(nodes, edges)\nprim_mst, prim_steps = prim_algorithm(nodes, edges)\n\n# Calculate total weights\nkruskal_weight = sum(w for _, _, w in kruskal_mst)\nprim_weight = sum(w for _, _, w in prim_mst)\n\n# Display algorithm results with current step information\nweight_match = \"‚úÖ Same\" if kruskal_weight == prim_weight else \"‚ùå Different\"\n\n# Get current step information\ncurrent_step = time_step.value\nmax_steps = len([s for s in kruskal_steps if s[\"action\"] == \"added\"])\n```\n\n```python {.marimo}\ndef visualize_both_algorithms():\n    \"\"\"Create side-by-side visualization of both algorithms with time step control\"\"\"\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n\n    # Get edges to show up to current time step for each algorithm\n    current_step = time_step.value\n\n    # For Kruskal: edges added in order they appear in steps\n    kruskal_edges_to_show = []\n    for i, step in enumerate(kruskal_steps):\n        if i >= current_step:\n            break\n        if step[\"action\"] == \"added\":\n            u, v = step[\"edge\"]\n            weight = step[\"weight\"]\n            kruskal_edges_to_show.append((u, v, weight))\n\n    # For Prim: edges added in order they appear in steps\n    prim_edges_to_show = []\n    for i, step in enumerate(prim_steps[1:], 1):  # Skip the 'start' step\n        if i > current_step:\n            break\n        if step[\"action\"] == \"added\":\n            u, v = step[\"edge\"]\n            weight = step[\"weight\"]\n            prim_edges_to_show.append((u, v, weight))\n\n    algorithms = [\n        (ax1, \"Kruskal's Algorithm\", kruskal_edges_to_show),\n        (ax2, \"Prim's Algorithm\", prim_edges_to_show),\n    ]\n\n    for ax, title, edges_to_show in algorithms:\n        ax.clear()\n        ax.set_facecolor(\"white\")\n\n        # Draw all possible edges - dashed for unconnected\n        mst_edge_set = set((u, v) for u, v, _ in edges_to_show) | set(\n            (v, u) for u, v, _ in edges_to_show\n        )\n\n        for u, v, weight in edges:\n            x1, y1 = nodes[u]\n            x2, y2 = nodes[v]\n\n            if (u, v) in mst_edge_set or (v, u) in mst_edge_set:\n                # MST edge - solid black line\n                ax.plot(\n                    [x1, x2],\n                    [y1, y2],\n                    \"black\",\n                    linewidth=3,\n                    solid_capstyle=\"round\",\n                )\n            else:\n                # Non-MST edge - dashed grey line\n                ax.plot(\n                    [x1, x2],\n                    [y1, y2],\n                    \"grey\",\n                    linewidth=2,\n                    linestyle=\"--\",\n                    alpha=0.7,\n                )\n\n            # Add edge weight labels with larger font\n            mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n            ax.text(\n                mid_x,\n                mid_y,\n                str(weight),\n                fontsize=20,\n                bbox=dict(\n                    boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.9\n                ),\n                ha=\"center\",\n                va=\"center\",\n                fontweight=\"bold\",\n            )\n\n        # Draw nodes - color based on connection status\n        connected_nodes = set()\n        if \"Prim\" in title:\n            # For Prim, start with node A always connected\n            connected_nodes.add(\"A\")\n            for u, v, _ in edges_to_show:\n                connected_nodes.add(u)\n                connected_nodes.add(v)\n        else:\n            # For Kruskal, find all nodes in connected components\n            if edges_to_show:\n                # Build Union-Find to determine connected components\n                parent = {node: node for node in nodes}\n\n                def find_root(x):\n                    if parent[x] != x:\n                        parent[x] = find_root(parent[x])\n                    return parent[x]\n\n                def union_nodes(x, y):\n                    px, py = find_root(x), find_root(y)\n                    if px != py:\n                        parent[py] = px\n\n                # Apply edges to build connected components\n                for u, v, _ in edges_to_show:\n                    union_nodes(u, v)\n\n                # Find all nodes connected to any component that has edges\n                connected_roots = set()\n                for u, v, _ in edges_to_show:\n                    connected_roots.add(find_root(u))\n                    connected_roots.add(find_root(v))\n\n                for node in nodes:\n                    if find_root(node) in connected_roots:\n                        connected_nodes.add(node)\n\n        for node, (x, y) in nodes.items():\n            color = (\n                \"#f5cbcc\" if node in connected_nodes else \"#d0e2f3\"\n            )  # light red : light blue\n            # Draw larger circle with black border\n            circle = plt.Circle((x, y), 0.15, color=color, zorder=5)\n            ax.add_patch(circle)\n            # Add black border\n            border_circle = plt.Circle(\n                (x, y),\n                0.15,\n                fill=False,\n                edgecolor=\"black\",\n                linewidth=2,\n                zorder=6,\n            )\n            ax.add_patch(border_circle)\n            # Larger text\n            ax.text(\n                x,\n                y,\n                node,\n                ha=\"center\",\n                va=\"center\",\n                fontsize=20,\n                fontweight=\"bold\",\n                zorder=7,\n            )\n\n        # Clean title\n        ax.set_title(title, fontsize=22, fontweight=\"bold\", pad=20)\n        ax.set_aspect(\"equal\")\n        ax.axis(\"off\")\n\n        # Set axis limits with padding\n        x_coords = [x for x, y in nodes.values()]\n        y_coords = [y for x, y in nodes.values()]\n        ax.set_xlim(min(x_coords) - 0.3, max(x_coords) + 0.3)\n        ax.set_ylim(min(y_coords) - 0.3, max(y_coords) + 0.3)\n\n    plt.tight_layout()\n    return fig\n\n\n# Create and display the visualization\nfig = visualize_both_algorithms()\n```\n\n\n\n<!---\nCode for the percolation visualization\n--->\n\n\n```python {.marimo}\n# Create a slider to control the puddle probability\np_slider = mo.ui.slider(\n    start=0.0,\n    stop=1.0,\n    step=0.01,\n    value=0.5,\n    label=\"Puddle Probability (p)\"\n)\n```\n\n\n```python {.marimo}\nnp.random.seed(42)  # For reproducible results during demo\n# Grid parameters\ngrid_size = 50\nS = np.random.random((grid_size, grid_size))\n\ndef percolation_visualization():\n\n    # Generate the percolation grid based on slider value\n    np.random.seed(42)  # For reproducible results during demo\n    grid = S < p_slider.value\n\n    # Find connected components (puddles that touch each other)\n    labeled_array, num_features = label(grid)\n\n    # Find the largest connected component\n    if num_features > 0:\n        sizes = [(labeled_array == i).sum() for i in range(1, num_features + 1)]\n        largest_size = max(sizes)\n        largest_fraction = largest_size / (grid_size * grid_size)\n    else:\n        largest_size = 0\n        largest_fraction = 0.0\n\n    # Create visualization\n    plt.figure(figsize=(8, 8))\n\n    # Create a display grid that shows largest component in red\n    display_grid = np.zeros_like(grid, dtype=int)\n\n    # Find the largest component\n    if num_features > 0:\n        # Find which label corresponds to the largest component\n        largest_label = np.argmax(sizes) + 1  # +1 because labels start from 1\n\n        # Set display values: 0=white, 1=blue (small components), 2=red (largest component)\n        display_grid[grid] = 1  # All puddles start as blue\n        display_grid[labeled_array == largest_label] = 2  # Largest component in red\n\n    # Create custom colormap: white for empty, blue for small components, red for largest\n    colors = ['white', '#4472C4', '#E74C3C']  # white, blue, red\n    cmap = ListedColormap(colors)\n\n    # Plot the grid\n    ax = plt.imshow(display_grid, cmap=cmap, interpolation='nearest')\n\n    # Styling\n    plt.title(f'Percolation Grid (p = {p_slider.value:.2f})\\n'\n              f'Largest Component (Red): {largest_size} squares '\n              f'({largest_fraction:.1%} of grid)',\n              fontsize=14, pad=20)\n    plt.xlabel('Grid Position')\n    plt.ylabel('Grid Position')\n\n    # Add grid lines for clarity\n    plt.xticks(np.arange(-0.5, grid_size, 10), minor=True)\n    plt.yticks(np.arange(-0.5, grid_size, 10), minor=True)\n    plt.grid(which='minor', color='gray', linestyle='-', alpha=0.3)\n    plt.tick_params(which='minor', size=0)\n\n    # Add a legend\n    from matplotlib.patches import Patch\n    legend_elements = [\n        Patch(facecolor='white', edgecolor='black', label='Empty'),\n        Patch(facecolor='#4472C4', label='Small Components'),\n        Patch(facecolor='#E74C3C', label='Largest Component')\n    ]\n    plt.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))\n\n    # Return the plot\n    return ax\n```\n\n```python {.marimo}\n# Generate data for the phase transition plot\nprob_values = np.linspace(0, 1, 100)\ncomponent_fractions = []\n\n#grid_size_phase = 50  # Use smaller grid for faster computation\n\n# Calculate largest component size for different probabilities\nnp.random.seed(42)  # Fixed seed for consistent results\n#S = np.random.random((grid_size_phase, grid_size_phase))\nfor prob in prob_values:\n    # Generate random grid\n    phase_grid = S < prob\n\n    # Find connected components\n    labeled_phase, num_phase = label(phase_grid)\n\n    if num_phase > 0:\n        phase_sizes = [(labeled_phase == i).sum() for i in range(1, num_phase + 1)]\n        largest_phase = max(phase_sizes) / (grid_size * grid_size)\n    else:\n        largest_phase = 0.0\n\n    component_fractions.append(largest_phase)\n```\n\n```python {.marimo}\ndef phase_transition_plot():\n    # Create the phase transition plot\n    plt.figure(figsize=(8, 6))\n\n    # Plot the phase transition curve\n    plt.plot(prob_values, component_fractions, 'b-', linewidth=2,\n             label='Largest Component Size')\n\n    # Highlight current probability\n    current_idx = int(p_slider.value * 99)  # Convert to index\n    plt.plot(p_slider.value, component_fractions[current_idx],\n             'ro', markersize=10, label=f'Current p = {p_slider.value:.2f}')\n\n    # Mark approximate critical point (for 2D lattice, pc ‚âà 0.593)\n    critical_p = 0.593\n    plt.axvline(x=critical_p, color='gray', linestyle='--', alpha=0.7,\n                label=f'Critical point (p_c ‚âà {critical_p})')\n\n    # Styling\n    plt.xlabel('Probability (p)', fontsize=12)\n    plt.ylabel('Fraction of Grid in Largest Component', fontsize=12)\n    plt.title('Percolation Phase Transition', fontsize=14)\n    plt.grid(True, alpha=0.3)\n    plt.legend(frameon=False)\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n\n    # Add phase labels\n    plt.text(0.2, 0.2, 'Disconnected\\nPhase', fontsize=15, ha='center',\n             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n    plt.text(0.8, 0.2, 'Connected\\nPhase', fontsize=15, ha='center',\n             bbox=dict(boxstyle='round', facecolor='#f5cbcc', alpha=0.7))\n\n    return plt.gca()\n```\n\n",
    "supporting": [
      "01-concepts_files"
    ],
    "filters": [],
    "includes": {}
  }
}