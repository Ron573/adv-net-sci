{
  "hash": "d9a9a7845ce3f019eb75ff34c52c85e3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Visualizing Degree Distributions in Python\"\njupyter: advnetsci\nexecute:\n    enabled: true\n---\n\n## Computing degree distributions from network data\n\nWe'll start by creating a scale-free network using the Barabási-Albert model and computing its degree distribution. This model generates networks with power-law degree distributions, making it ideal for demonstrating visualization techniques that work well with heavy-tailed distributions.\n\n::: {#001f5003 .cell execution_count=1}\n``` {.python .cell-code}\nimport igraph\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a Barabási-Albert network with 10,000 nodes\ng = igraph.Graph.Barabasi(n=10000, m=1)\nA = g.get_adjacency()\n```\n:::\n\n\nThe first step in analyzing any network is computing the degree sequence. In Python, we can extract degrees directly from the adjacency matrix by summing along rows (for undirected networks, row and column sums are identical). The `flatten()` method ensures we get a 1D array of degree values.\n\n::: {#5fdd6206 .cell execution_count=2}\n``` {.python .cell-code}\n# Compute degree for each node\ndeg = np.sum(A, axis=1).flatten()\n\n# Convert to probability distribution\np_deg = np.bincount(deg) / len(deg)\n```\n:::\n\n\n## Why standard histograms fail for degree distributions\n\nLet's start with the obvious approach—a simple histogram. This immediately reveals why degree distribution visualization is challenging.\n\n::: {#b774a69b .cell execution_count=3}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(8, 5))\nax = sns.lineplot(x=np.arange(len(p_deg)), y=p_deg)\nax.set_xlabel('Degree')\nax.set_ylabel('Probability')\nax.set_title('Linear Scale: Most Information Hidden')\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'Linear Scale: Most Information Hidden')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-4-output-2.png){}\n:::\n:::\n\n\nThis linear-scale plot shows the fundamental problem: most nodes cluster at low degrees, making the interesting high-degree tail invisible. Since power-law networks have heavy tails—a few nodes with very high degrees—we need visualization techniques that can handle this extreme heterogeneity.\n\n## Log-log plots: revealing the power-law structure\n\nSwitching to logarithmic scales on both axes dramatically improves visibility across the entire degree range. This transformation is essential for identifying power-law behavior, which appears as straight lines in log-log space.\n\n::: {#d97789d3 .cell execution_count=4}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(8, 5))\nax = sns.lineplot(x=np.arange(len(p_deg)), y=p_deg)\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_ylim(np.min(p_deg[p_deg>0])*0.01, None)\nax.set_xlabel('Degree')\nax.set_ylabel('Probability')\nax.set_title('Log-Log Scale: Structure Revealed')\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nText(0.5, 1.0, 'Log-Log Scale: Structure Revealed')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-5-output-2.png){}\n:::\n:::\n\n\nThe log-log plot reveals the power-law structure, but notice the noisy fluctuations at high degrees. This noise occurs because only a few nodes have very high degrees, leading to statistical fluctuations. While binning could smooth these fluctuations, it introduces arbitrary choices about bin sizes and loses information.\n\n## The CCDF approach: smooth curves without binning\n\nThe complementary cumulative distribution function (CCDF) provides a superior visualization method. Instead of plotting the fraction of nodes with exactly degree k, CCDF plots the fraction with degree greater than k. This approach naturally smooths the data without requiring binning decisions.\n\n::: {#cb186236 .cell execution_count=5}\n``` {.python .cell-code}\n# Compute CCDF: fraction of nodes with degree > k\nccdf_deg = 1 - np.cumsum(p_deg)[:-1]  # Exclude last element (always 0)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg)\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlabel('Degree')\nax.set_ylabel('CCDF')\nax.set_title('CCDF: Smooth Power-Law Visualization')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nText(0.5, 1.0, 'CCDF: Smooth Power-Law Visualization')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-6-output-2.png){}\n:::\n:::\n\n\nThe CCDF produces clean, interpretable curves even for noisy data. The slope directly relates to the power-law exponent: steeper slopes indicate more homogeneous degree distributions (fewer hubs), while flatter slopes suggest more heterogeneous distributions (more extreme hubs). This visualization technique has become the standard in network science for analyzing heavy-tailed distributions.\n\n## Implementing the friendship paradox\n\nNow let's demonstrate the friendship paradox computationally. As covered in the concepts module, this phenomenon arises because high-degree nodes are more likely to be someone's friend than low-degree nodes. We'll implement this by sampling friends through edge-based sampling.\n\nTo capture this bias, we need to sample edges rather than nodes. When we sample an edge uniformly at random, we're effectively sampling one endpoint of that edge—this gives us a \"friend\" from someone's perspective. The key insight is that nodes with higher degrees appear as endpoints more frequently, creating the degree bias that drives the friendship paradox.\n\n::: {#4150a28e .cell execution_count=6}\n``` {.python .cell-code}\nfrom scipy import sparse\n\n# Extract all edges from the adjacency matrix\nsrc, trg, _ = sparse.find(A)\nprint(f\"Total number of edges: {len(src)}\")\nprint(f\"First few source nodes: {src[:10]}\")\nprint(f\"First few target nodes: {trg[:10]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal number of edges: 19998\nFirst few source nodes: [0 0 0 0 0 0 0 0 0 0]\nFirst few target nodes: [   1   24  242  555 1385 1885 2254 2521 2654 5133]\n```\n:::\n:::\n\n\nThe `sparse.find()` function returns three arrays: source nodes, target nodes, and edge weights. Since we're working with an unweighted network, we ignore the weights. Each edge appears twice in an undirected network (once as src→trg and once as trg→src), which is exactly what we want for sampling friends.\n\nNow we can compute the degree distribution of friends by taking the degrees of the source nodes from our edge list. This automatically implements the degree-biased sampling because high-degree nodes appear more frequently in the source node list.\n\n::: {#f1c98234 .cell execution_count=7}\n``` {.python .cell-code}\n# Get degrees of \"friends\" (source nodes from edge sampling)\ndeg_friend = deg[src]\n\n# Compute degree distribution of friends\np_deg_friend = np.bincount(deg_friend) / len(deg_friend)\n\nprint(f\"Average degree in network: {np.mean(deg):.2f}\")\nprint(f\"Average degree of friends: {np.mean(deg_friend):.2f}\")\nprint(f\"Friendship paradox ratio: {np.mean(deg_friend) / np.mean(deg):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage degree in network: 2.00\nAverage degree of friends: 4.87\nFriendship paradox ratio: 2.44\n```\n:::\n:::\n\n\n## Visualizing the degree bias\n\nLet's create a side-by-side comparison of both distributions using CCDF plots. This clearly shows how the friendship paradox manifests as a shift toward higher degrees in the friend distribution.\n\n::: {#d526db0f .cell execution_count=8}\n``` {.python .cell-code}\n# Compute CCDFs for both distributions\nccdf_deg = 1 - np.cumsum(p_deg)[:-1]\nccdf_deg_friend = 1 - np.cumsum(p_deg_friend)[:-1]\n\n# Create comparison plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg,\n                  label='Regular nodes', linewidth=2, color='blue')\nax = sns.lineplot(x=np.arange(len(ccdf_deg_friend)), y=ccdf_deg_friend,\n                  label='Friends (degree-biased)', linewidth=2, color='red', ax=ax)\n\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlabel('Degree')\nax.set_ylabel('CCDF')\nax.set_title('Friendship Paradox: Friends Have Higher Degrees')\nax.legend(frameon=False)\nax.grid(True, alpha=0.3)\n```\n\n::: {.cell-output .cell-output-display}\n![](02-coding_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nThe plot clearly demonstrates the friendship paradox: the friend distribution (red line) lies below the node distribution (blue line), indicating that friends have systematically higher degrees. The flatter slope of the friend CCDF shows that the probability of encountering high-degree friends is much higher than encountering high-degree nodes when sampling uniformly.\n\nThis computational demonstration confirms the theoretical prediction that your friends will, on average, have more friends than you do. The magnitude of this effect depends on the heterogeneity of the degree distribution—the more heterogeneous the network, the stronger the friendship paradox becomes.\n\n",
    "supporting": [
      "02-coding_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}