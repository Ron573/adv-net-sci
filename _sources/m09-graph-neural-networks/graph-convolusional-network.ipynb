{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bbde81",
   "metadata": {},
   "source": [
    "# Graph Convolutional Networks\n",
    "\n",
    "## Analogy between image and graph data\n",
    "\n",
    "We can think of a convolution of an image from the perspective of networks.\n",
    "In the convolution of an image, a pixel is convolved with its *neighbors*. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.\n",
    "\n",
    "![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp)\n",
    "\n",
    "Building on this analogy, we can extend the idea of convolution to general graph data.\n",
    "Each node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph.\n",
    "This is the key idea of graph convolutional networks.\n",
    "\n",
    "But, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the \"kernel\" for graph convolution.\n",
    "\n",
    "## Spectral filter on graphs\n",
    "\n",
    "Just like we can define a convolution on images in the frequency domain, we can also define a convolution on graphs in another domain.\n",
    "\n",
    "Suppose we somehow define a spatial convolution on a graph represented as a matrix ${\\mathbf W} = \\{W_{ij}\\}$. Then, the spatial convolution of a node $i$ is given by\n",
    "\n",
    "$$\n",
    "{\\mathbf x}_i' = \\sum_{j=1}^N W_{ij} {\\mathbf x}_j,\n",
    "$$\n",
    "\n",
    "where ${\\mathbf x}_i$ is a row vector of features of node $i$. For all nodes, we can write this as\n",
    "\n",
    "$$\n",
    "X' = {\\mathbf W} {\\mathbf X}, \\quad\n",
    "X = \\begin{bmatrix} {\\mathbf x}_1 \\\\ {\\mathbf x}_2 \\\\ \\vdots \\\\ {\\mathbf x}_N \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "where ${\\mathbf X}$ is the feature matrix of all nodes.\n",
    "\n",
    "Let us assume that ${\\mathbf W}$ is a real-valued, symmetric matrix, which includes the adjacency matrix and the Laplacian matrix.\n",
    "Just like the Fourier transform, we can decompose ${\\mathbf W}$ into *basis* matrices as\n",
    "\n",
    "$$\n",
    "{\\mathbf W} = \\sum_{i=1}^n \\lambda_i {\\mathbf B}_i,\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ are the eigenvalues, and ${\\mathbf B}_i$ are the eigenvectors given by\n",
    "\n",
    "$$\n",
    "{\\mathbf B}_i = {\\mathbf u}_i {\\mathbf u}_i^\\top,\n",
    "$$\n",
    "\n",
    "where ${\\mathbf u}_i$ is the eigenvector corresponding to the eigenvalue $\\lambda_i$.\n",
    "\n",
    "Thus, the convolution of ${\\mathbf X}$ can be written as\n",
    "\n",
    "$$\n",
    "X' = \\sum_{i=1}^n \\lambda_i {\\mathbf u}_i {\\mathbf u}_i^\\top {\\mathbf X}.\n",
    "$$\n",
    "\n",
    "Now, we can see a parallel between the Fourier transform and the eigendecomposition of the Laplacian matrix.\n",
    "\n",
    "| Fourier Transform | Eigendecomposition |\n",
    "|-------------------|-----------------|\n",
    "| Basis waves | Eigenvectors |\n",
    "| Frequency | Eigenvalues |\n",
    "| Convolution in spatial domain | Multiplication in spectral domain |\n",
    "\n",
    "To address the problem, we need to understand the properties of graph data in a mathematical way."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}