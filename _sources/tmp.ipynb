{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ba7fc3",
   "metadata": {},
   "source": [
    "# Ladder Lottery\n",
    "\n",
    "```{admonition} Ladder Lottery\n",
    ":class: tip\n",
    "\n",
    "Ladder Lottery is a fun East Asian game, also known as \"鬼腳圖\" (Guijiaotu) in Chinese, \"阿弥陀籤\" (Amida-kuzi) in Japanese, \"사다리타기\" (Sadaritagi) in Korean, and \"Ladder Lottery\" in English. The game is played as follows:\n",
    "1. A player is given a board with a set of vertical lines.\n",
    "2. The player chooses a line and starts to move along the line\n",
    "3. When hitting a horizontal line, the player must move along the horizontal line and then continue to move along the next vertical line.\n",
    "4. The player wins if the player can hit a marked line at the bottom of the board.\n",
    "5. You cannot see the horizontal lines in advance!\n",
    "\n",
    "Play the {{ '[Ladder Lottery Game! 🎮✨]( BASE_URL/vis/amida-kuji.html?)'.replace('BASE_URL', base_url) }} and try to answer the following questions:\n",
    "\n",
    "1. Is tehre a strategy to maximize the probability of winning?\n",
    "2. How does the probability of winning change as the number of horizontal lines increases?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/6/64/Amidakuji_2022-05-10.gif)\n",
    "\n",
    "```# Pen and paper exercises\n",
    "\n",
    "- [✍️ Pen and paper exercises](pen-and-paper/exercise.pdf)\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Random Walks in Python\n",
    "\n",
    "## Simulating Random Walks\n",
    "\n",
    "We will simulate random walks on a simple graph of five nodes as follows.\n",
    "\n",
    "```{code-cell} ipython3\n",
    "import numpy as np\n",
    "import igraph\n",
    "\n",
    "g = igraph.Graph()\n",
    "\n",
    "g.add_vertices([0, 1, 2, 3, 4])\n",
    "g.add_edges([(0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (2, 4), (3, 4)])\n",
    "igraph.plot(g, vertex_size=20, vertex_label=g.vs[\"name\"])\n",
    "```\n",
    "\n",
    "A random walk is characterized by the transition probabilities between nodes.\n",
    "\n",
    "$$\n",
    "P_{ij} = \\frac{A_{ij}}{k_i}\n",
    "$$\n",
    "\n",
    "Let us first compute the transition probabilities and store them in a matrix, $\\mathbf{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dafc631",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m A \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mget_adjacency_sparse()\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m      2\u001b[0m k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(g\u001b[38;5;241m.\u001b[39mdegree())\n\u001b[1;32m      3\u001b[0m n_nodes \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mvcount()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g' is not defined"
     ]
    }
   ],
   "source": [
    "A = g.get_adjacency_sparse().toarray()\n",
    "k = np.array(g.degree())\n",
    "n_nodes = g.vcount()\n",
    "\n",
    "# A simple but inefficient way to compute P\n",
    "P = np.zeros((n_nodes, n_nodes))\n",
    "for i in range(n_nodes):\n",
    "    for j in range(n_nodes):\n",
    "        if k[i] > 0:\n",
    "            P[i, j] = A[i, j] / k[i]\n",
    "        else:\n",
    "            P[i, j] = 0\n",
    "\n",
    "# Alternative, more efficient way to compute P\n",
    "P = A / k[:, np.newaxis]\n",
    "\n",
    "# or even more efficiently\n",
    "P = np.einsum(\"ij,i->ij\", A, 1 / k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca97e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transition probability matrix:\\n\", P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(P, annot=True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf3b72",
   "metadata": {},
   "source": [
    "Each row and column of $\\mathbf{P}$ corresponds to a node, with entries representing the transition probabilities from the row node to the column node.\n",
    "\n",
    "Now, let us simulate a random walk on this graph. We represent a position of the walker by a vector, $\\mathbf{x}$, with five elements, each of which represents a node. We mark the node that the walker is currently at by `1` and others as `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 0, 0, 0, 0])\n",
    "x[0] = 1\n",
    "print(\"Initial position of the walker:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2fadc",
   "metadata": {},
   "source": [
    "This vector representation is convenient to get the probabilities of transitions to other nodes from the current node:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\mathbf{P}\n",
    "$$\n",
    "\n",
    "which is translated into the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0175c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = x @ P\n",
    "print(\"Position of the walker after one step:\\n\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda9fbb8",
   "metadata": {},
   "source": [
    "We can then draw the next node based on the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549109bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_node = np.random.choice(n_nodes, p=probs)\n",
    "x[:] = 0 # zero out the vector\n",
    "x[next_node] = 1 # set the next node to 1\n",
    "print(\"Position of the walker after one step:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68da9e",
   "metadata": {},
   "source": [
    "By repeating this process, we can simulate the random walk.\n",
    "\n",
    "### Exercise 01\n",
    "\n",
    "Write the following function to simulate the random walk for a given number of steps and return the $x$ for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(A, n_steps):\n",
    "    \"\"\"\n",
    "    Simulate the random walk on a graph with adjacency matrix A.\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): The adjacency matrix of the graph.\n",
    "        x (np.ndarray): The initial position of the walker.\n",
    "        n_steps (int): The number of steps to simulate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The position of the walker after each step.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f5c15f",
   "metadata": {},
   "source": [
    "## Expected behavior of random walks\n",
    "\n",
    "What is the expected position of the walker after multiple steps? It is easy to compute the expected position of the walker after one step from initial position $x(0)$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x(1)] = x(0) P\n",
    "$$\n",
    "\n",
    "where $x(t)$ is the probability distribution of the walker at time $t$. In Python, the expected position of the walker at time $t=1$ is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([1, 0, 0, 0, 0])\n",
    "x_1 = x_0 @ P\n",
    "print(\"Expected position of the walker after one step:\\n\", x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea344d",
   "metadata": {},
   "source": [
    "For the second step, the expected position of the walker is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x(2)] = \\mathbb{E}[x(1) P] = \\mathbb{E}[x(0) P] P = x(0) P^2\n",
    "$$\n",
    "\n",
    "In other words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = x_1 @ P\n",
    "print(\"Expected position of the walker after two steps:\\n\", x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0b00b",
   "metadata": {},
   "source": [
    "Following the same argument, the expected position of the walker at time $t$ is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x(t)] = x(0) P^t\n",
    "$$\n",
    "\n",
    "### Exercise 02\n",
    "\n",
    "Write a function to compute the expected position of the walker at time $t$ using the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_position(A, x_0, t):\n",
    "    \"\"\"\n",
    "    Compute the expected position of the walker at time t.\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): The adjacency matrix of the graph.\n",
    "        x_0 (np.ndarray): The initial position of the walker.\n",
    "        t (int): The number of steps to simulate.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d2bb5",
   "metadata": {},
   "source": [
    "### Exercise 03\n",
    "\n",
    "Plot each element of $x(t)$ as a function of $t$ for $t=0,1,2,\\ldots, 1000$. Try different initial positions and compare the results!\n",
    "\n",
    "Steps:\n",
    "1. Define the initial position of the walker.\n",
    "2. Compute the expected position of the walker at time $t$ using the function you wrote above.\n",
    "3. Draw a line for each element of $x(t)$, totalling 5 lines.\n",
    "4. Create multiple such plots for different initial positions and compare them.\n",
    "\n",
    "## Community structure\n",
    "\n",
    "Random walks can capture community structure of a network.\n",
    "To see this, let us consider a network of a ring of cliques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ea792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import igraph\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_cliques = 3\n",
    "n_nodes_per_clique = 5\n",
    "\n",
    "G = nx.ring_of_cliques(n_cliques, n_nodes_per_clique)\n",
    "g = igraph.Graph().Adjacency(nx.to_numpy_array(G).tolist()).as_undirected()\n",
    "membership = np.repeat(np.arange(n_cliques), n_nodes_per_clique)\n",
    "\n",
    "color_map = [sns.color_palette()[i] for i in membership]\n",
    "igraph.plot(g, vertex_size=20, vertex_color=color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b628aa",
   "metadata": {},
   "source": [
    "Let us compute the expected position of the walker after 1 to 10 steps.\n",
    "\n",
    "**Compute the transition matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f3f28",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# Get the adjacency matrix and degree\n",
    "A = g.get_adjacency_sparse()\n",
    "k = np.array(g.degree())\n",
    "\n",
    "# This is an efficient way to compute the transition matrix\n",
    "# using scipy.sparse\n",
    "P = sparse.diags(1 / k) @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87f30a",
   "metadata": {},
   "source": [
    "**Compute the expected position of the walker after 1 to 300 steps**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cc9aa",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "x_t = np.zeros(g.vcount())\n",
    "x_t[2] = 1\n",
    "x_list = [x_t]\n",
    "for t in range(300):\n",
    "    x_t = x_t @ P\n",
    "    x_list.append(x_t)\n",
    "x_list = np.array(x_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d362d076",
   "metadata": {},
   "source": [
    "**Plot the expected position of the walker at each step**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b1c4e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15,10), ncols = 3, nrows = 2)\n",
    "\n",
    "t_list = [0, 1, 3, 5, 10, 299]\n",
    "for i, t in enumerate(t_list):\n",
    "    igraph.plot(g, vertex_size=20, vertex_color=[cmap(x_list[t][j] / np.max(x_list[t])) for j in range(g.vcount())], target = axes[i//3][i%3])\n",
    "    axes[i//3][i%3].set_title(f\"$t$ = {t}\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2058df1",
   "metadata": {},
   "source": [
    "where the color of each node represents the probability of the walker being at that node.\n",
    "\n",
    "An important observation is that the walker spends more time in the clique that it started from and then diffuse to others. Thus, the position of the walker before reaching the steady state tells us the community structure of the network.\n",
    "\n",
    "## Exercise 04\n",
    "\n",
    "1. Generate a network of 100 nodes with 4 communities using a stochastic block model, with inter-community edge probability $0.05$ and intra-community edge probability $0.2$. Then, compute the expected position of the walker starting from node zero after $x$ steps. Plot the results for $x = 0, 5, 10, 1000$.\n",
    "\n",
    "2. Increase the inter-community edge probability to $0.15$ and repeat the simulation. Compare the results with the previous simulation.\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Characteristics of Random Walks\n",
    "\n",
    "## Stationary State\n",
    "\n",
    "Let's dive into the math behind random walks in a way that's easy to understand.\n",
    "\n",
    "Imagine you're at node $i$ at time $t$. You randomly move to a neighboring node $j$. The probability of this move, called the transition probability $p_{ij}$, is:\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{A_{ij}}{k_i},\n",
    "$$\n",
    "\n",
    "Here, $A_{ij}$ is an element of the adjacency matrix, and $k_i$ is the degree of node $i$. For a network with $N$ nodes, we can represent all transition probabilities in a transition probability matrix $P$:\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\begin{pmatrix}\n",
    "p_{11} & p_{12} & \\cdots & p_{1N} \\\\\n",
    "p_{21} & p_{22} & \\cdots & p_{2N} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{N1} & p_{N2} & \\cdots & p_{NN}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This matrix $P$ encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:\n",
    "\n",
    "- After one step: $P_{ij} = p_{ij}$\n",
    "- After two steps: $\\left(\\mathbf{P}^{2}\\right)_{ij} = \\sum_{k} P_{ik} P_{kj}$\n",
    "- After $T$ steps: $\\left(\\mathbf{P}^{T}\\right)_{ij}$\n",
    "\n",
    "```{note}\n",
    "Let's explore why $\\mathbf{P}^2$ represents the transition probabilities after two steps.\n",
    "\n",
    "First, recall that $\\mathbf{P}_{ij}$ is the probability of moving from node $i$ to node $j$ in one step. Now, consider a two-step walk from $i$ to $j$. We can express this as:\n",
    "\n",
    "$$(\\mathbf{P}^2)_{ij} = \\sum_k \\mathbf{P}_{ik} \\mathbf{P}_{kj}$$\n",
    "\n",
    "This equation encapsulates a key idea: to go from $i$ to $j$ in two steps, we must pass through some intermediate node $k$. Let's break this down step by step:\n",
    "\n",
    "1. The probability of the first step ($i$ to $k$) is $\\mathbf{P}_{ik}$.\n",
    "2. The probability of the second step ($k$ to $j$) is $\\mathbf{P}_{kj}$.\n",
    "3. The probability of this specific path ($i$ → $k$ → $j$) is the product $\\mathbf{P}_{ik} \\mathbf{P}_{kj}$.\n",
    "4. We sum over all possible intermediate nodes $k$ to get the total probability.\n",
    "\n",
    "Likewise, for three steps, we have:\n",
    "\n",
    "$$(\\mathbf{P}^3)_{ij} = \\sum_k \\left( \\mathbf{P}\\right)^2_{ik} \\mathbf{P}_{kj}$$\n",
    "\n",
    "where:\n",
    "1. The probability of going from $i$ to $k$ in two steps is $\\left( \\mathbf{P}\\right)^2_{ik}$.\n",
    "2. The probability of going from $k$ to $j$ in one step is $\\mathbf{P}_{kj}$.\n",
    "3. The probability of this specific path ($i$ →...→$k$ → $j$) is the product $\\left( \\mathbf{P}\\right)^2_{ik} \\mathbf{P}_{kj}$.\n",
    "4. We sum over all possible intermediate nodes $k$ to get the total probability.\n",
    "\n",
    "And we can extend this reasoning for any number of steps $t$.\n",
    "\n",
    "In summary, for any number of steps $t$, $\\left( \\mathbf{P}^t \\right)_{ij}$ gives the probability of being at node $j$ after $t$ steps, starting from node $i$.\n",
    "\n",
    "```\n",
    "\n",
    "As $T$ becomes very large, the probability distribution of being at each node, $\\mathbf{x}(t)$, approaches a constant value:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t+1) =\\mathbf{x}(t) \\mathbf{P}\n",
    "$$\n",
    "\n",
    "This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(\\infty) = \\mathbb{\\pi}, \\; \\mathbf{\\pi} = [\\pi_1, \\ldots, \\pi_N]\n",
    "$$\n",
    "\n",
    "For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:\n",
    "\n",
    "$$\n",
    "\\pi_j = \\frac{k_j}{\\sum_{\\ell} k_\\ell} \\propto k_j\n",
    "$$\n",
    "\n",
    "This means the probability of being at node $j$ in the long run is proportional to the degree of node $j$. The normalization ensures that the sum of all probabilities is 1, i.e., $\\sum_{j=1}^N \\pi_j = 1$.\n",
    "\n",
    "\n",
    "## Experiment\n",
    "\n",
    "Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "edge_list = []\n",
    "for i in range(5):\n",
    "    for j in range(i+1, 5):\n",
    "        edge_list.append((i, j))\n",
    "        edge_list.append((i+5, j+5))\n",
    "edge_list.append((0, 6))\n",
    "\n",
    "g = ig.Graph(edge_list)\n",
    "ig.plot(g, vertex_size=20, vertex_label=np.arange(g.vcount()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b117cb",
   "metadata": {},
   "source": [
    "The transition probability matrix $P$ is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "A = g.get_adjacency_sparse()\n",
    "deg = np.array(A.sum(axis=1)).flatten()\n",
    "Dinv = sparse.diags(1/deg)\n",
    "P = Dinv @ A\n",
    "P.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c070bb30",
   "metadata": {},
   "source": [
    "Let us compute the stationary distribution by using the power method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64721cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x = np.zeros(g.vcount())\n",
    "x[1] = 1 # Start from node 1\n",
    "T = 100\n",
    "xt = []\n",
    "for t in range(T):\n",
    "    x = x.reshape(1, -1) @ P\n",
    "    xt.append(x)\n",
    "\n",
    "xt = np.vstack(xt) # Stack the results vertically\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "palette = sns.color_palette().as_hex()\n",
    "for i in range(g.vcount()):\n",
    "    sns.lineplot(x=range(T), y=xt[:, i], label=f\"Node {i}\", ax=ax, color=palette[i])\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_title(\"Stationary distribution of a random walk\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f79ab2",
   "metadata": {},
   "source": [
    "We see that the distributions of the walker converges, and there are three characteristic features in the convergence:\n",
    "1. The distribution of the walker occilates with a decying amplitude and eventually converges.\n",
    "2. Nodes of the same degree converge to the same stationary probability.\n",
    "3. Nodes with higher degree converge to the higher stationary probability.\n",
    "\n",
    "To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d506f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "n_edges = np.sum(deg) / 2\n",
    "expected_stationary_dist = deg / (2 * n_edges)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Expected stationary distribution\": expected_stationary_dist,\n",
    "    \"Stationary distribution of a random walk\": xt[-1].flatten()\n",
    "}).style.format(\"{:.4f}\").set_caption(\"Comparison of Expected and Observed Stationary Distributions\").background_gradient(cmap='cividis', axis = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab03cc",
   "metadata": {},
   "source": [
    "## Time to reach the stationary state\n",
    "\n",
    "Let's explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.\n",
    "\n",
    "The mixing time, denoted as $t_{\\text{mix}}$, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than $\\epsilon$:\n",
    "\n",
    "$$t_{\\text{mix}} = \\min\\{t : \\max_{{\\bf x}(0)} \\|{\\bf x}(t) - {\\bf \\pi}\\|_{1} \\leq \\epsilon\\}$$\n",
    "\n",
    "where $\\|{\\bf x}(t) - {\\bf \\pi}\\|_{1} = 2\\max_{i} |x_i(t) - \\pi_i|$ represents the L1 distance between two probability distributions. The choice of $\\epsilon$ is arbitrary.\n",
    "\n",
    "We know that the distribution of a walker after $t$ steps is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) =  \\mathbf{x}(0) \\mathbf{P} ^{t}\n",
    "$$\n",
    "\n",
    "To find this distribution, we need to compute $\\mathbf{P}^t$. However, we face a challenge: $\\mathbf{P}$ is not diagonalizable.\n",
    "\n",
    "A diagonalizable matrix $\\mathbf{S}$ can be written as $\\mathbf{S} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}$, where $\\mathbf{\\Lambda}$ is a diagonal matrix and $\\mathbf{Q}$ is an orthogonal matrix. Visually, it looks like this:\n",
    "\n",
    "![](../figs/diagonalizable.jpg)\n",
    "\n",
    "It is useful because we can then compute the power of the matrix as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{S}^t = \\mathbf{Q} \\mathbf{\\Lambda}^t \\mathbf{Q}^{-1}\n",
    "$$\n",
    "\n",
    "And it is easy to find ${\\bf Q}$ and ${\\bf \\Lambda}$ by using eigenvalue decomposition if ${\\bf S}$ is symmetric and consists only of real values. Namely, the eigenvectors form ${\\cal Q}$ and the eigenvalues form the diagonal matrix ${\\cal \\Lambda}$.\n",
    "\n",
    "```{note}\n",
    "Let us demonstrate the above relation by calculating $\\mathbf{S}^2$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{S}^2 &= \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1} \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1} \\\\\n",
    "&= \\mathbf{Q} \\mathbf{\\Lambda}^2 \\mathbf{Q}^{-1}.\n",
    "\\end{align}\n",
    "$$\n",
    "(Note that $\\mathbf{Q} \\mathbf{Q}^{-1} = {\\bf I}$.)\n",
    "\n",
    "![](../figs/diagonalizable-squared.jpg)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "$\\mathbf{P}$ is also diagonalizable but not symmetric like $\\mathbf{\\overline A}$ so that we cannot use the above relation directly. So we do a trick by rewriteing $\\mathbf{P}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\mathbf{D}^{-1} \\mathbf{A} = \\mathbf{D}^{-\\frac{1}{2}} \\overline {\\bf A} \\mathbf{D}^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "where $\\overline{\\bf A} = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}$ is the normalized adjacency matrix.\n",
    "\n",
    "The advantage is that $\\overline{\\bf A}$ is diagonalizable: $\\overline{\\bf A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top$. Using this, we can compute $\\mathbf{P}^t$:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}^t = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{Q} \\mathbf{\\Lambda}^t \\mathbf{Q}^\\top \\mathbf{D}^{\\frac{1}{2}} = \\mathbf{Q}_L \\mathbf{\\Lambda}^t \\mathbf{Q}_R ^\\top\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Q}_L = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{Q}$ and $\\mathbf{Q}_R = \\mathbf{D}^{\\frac{1}{2}} \\mathbf{Q}$.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Let us demonstrate the above relation by calculating $\\mathbf{P}^2$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}^2 &= \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} \\mathbf{D}^{\\frac{1}{2}} \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} \\mathbf{D}^{\\frac{1}{2}}\\\\\n",
    "&=  \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} ^2 \\mathbf{D}^{\\frac{1}{2}}\\\\\n",
    "&= \\mathbf{Q}_L \\mathbf{\\Lambda}^2 \\mathbf{Q}_R ^\\top\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "The probability distribution after $t$ steps is then:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = \\mathbf{x}(0) \\mathbf{Q}_L \\mathbf{\\Lambda}^t \\mathbf{Q}_R ^\\top\n",
    "$$\n",
    "\n",
    "We can rewrite this in a more intuitive form:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x_1(t) \\\\\n",
    "x_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "x_N(t)\n",
    "\\end{pmatrix}\n",
    " =\n",
    " \\sum_{\\ell=1}^N\n",
    " \\left[\n",
    " \\lambda_\\ell^t\n",
    " \\begin{pmatrix}\n",
    " q^{(L)}_{\\ell 1} \\\\\n",
    " q^{(L)}_{\\ell 2} \\\\\n",
    " \\vdots \\\\\n",
    " q^{(L)}_{\\ell N}\n",
    " \\end{pmatrix}\n",
    " \\langle\\mathbf{q}^{(R)}_{\\ell},  \\mathbf{x}(0) \\rangle\n",
    " \\right]\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "Visualize the above equation by using the following figure.\n",
    "\n",
    "![](../figs/diagonalizable-sum.jpg)\n",
    "\n",
    "```\n",
    "\n",
    "The term $\\lambda_\\ell^t$ represents the contribution of each eigenvalue to the stationary distribution over time. As $t$ increases, all terms decay exponentially except for the largest eigenvalue ($\\lambda_1 = 1$). This explains how the random walk converges to the stationary distribution:\n",
    "\n",
    "$$\n",
    "\\pi_i = \\lim_{t\\to\\infty} x_i(t) = \\begin{pmatrix} q^{(L)}_{1 1} \\\\ q^{(L)}_{1 2} \\\\ \\vdots \\\\ q^{(L)}_{1 N} \\end{pmatrix} \\langle\\mathbf{q}^{(R)}_{1},  \\mathbf{x}(0) \\rangle\n",
    "$$\n",
    "\n",
    "The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.\n",
    "\n",
    "Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as\n",
    "\n",
    "$$\n",
    "t_{\\text{mix}} < \\tau \\log \\left( \\frac{1}{\\epsilon \\min_{i} \\pi_i} \\right), \\quad \\tau = \\frac{1}{\\lambda_2}\n",
    "$$\n",
    "\n",
    "where $\\lambda_2$ is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as\n",
    "\n",
    "More commonly, it is expressed using the second smallest eigenvalue $\\mu$ of the normalized laplacian matrix as\n",
    "\n",
    "$$\n",
    "t_{\\text{mix}} \\leq \\frac{1}{1-\\mu}\n",
    "$$\n",
    "\n",
    "where $\\mu = 1-\\lambda_2$.\n",
    "\n",
    "\n",
    "### Compute the mixing time\n",
    "\n",
    "Let us demonstrate the above math by using the network of two cliques.\n",
    "\n",
    "#### Normalized Adjacency Matrix\n",
    "\n",
    "First, let us construct the normalized adjacency matrix $\\overline{\\bf A}$ of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dinv_sqrt = sparse.diags(1.0/np.sqrt(deg))\n",
    "A_norm = Dinv_sqrt @ A @ Dinv_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e3ff21",
   "metadata": {},
   "source": [
    "Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eigh(A_norm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae628ac",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`evals` and `evecs` are sorted in descending order of the eigenvalues. `evecs[:, 0]` is the eigenvector corresponding to the largest eigenvalue, which is always 1.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "There is a similar function called `np.linalg.eig` which returns the eigenvalues and eigenvectors. It can be used for any matrices, while `np.linalg.eigh` is specifically for symmetric matrices. `np.linalg.eigh` is faster and more stable and therefore preferred if your matrix is symmetric. `np.linalg.eig` is more susceptible to numerical errors and therefore less stable.\n",
    "```\n",
    "\n",
    "The eigenvalues and eigenvectors are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55838b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"Eigenvalue\": evals\n",
    "}).T.style.background_gradient(cmap='cividis', axis = 1).set_caption(\"Eigenvalues of the normalized adjacency matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d73c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"Eigenvector %i\" % i: evecs[:, i]\n",
    "    for i in range(10)\n",
    "}).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Eigenvectors of the normalized adjacency matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a2812",
   "metadata": {},
   "source": [
    "Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix.\n",
    "The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.\n",
    "\n",
    "```{note}\n",
    "The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, `np.linalg.eigh` returns the eigenvector whose sign can vary for a different run.\n",
    "```\n",
    "\n",
    "We decompose $\\overline{\\bf A}$ as\n",
    "\n",
    "$$\\overline {\\bf A} = {\\bf Q}{\\bf \\Lambda}{\\bf Q}^\\top$$\n",
    "\n",
    "where ${\\bf Q}$ corresponds to `eigvecs` and ${\\bf \\Lambda}$ corresponds to `np.diag(evals)` (since ${\\bf \\Lambda}$ is a diagonal matrix). Let's see if this is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a50493",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(A_norm.toarray()).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Normalized Adjacency Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7710715",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_norm_reconstructed = evecs @ np.diag(evals) @ evecs.T\n",
    "pd.DataFrame(A_norm_reconstructed).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Reconstruction of the Normalized Adjacency Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215499e",
   "metadata": {},
   "source": [
    "Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.\n",
    "\n",
    "#### Multi-step Transition Probability\n",
    "\n",
    "Let us first conform whether we can compute the transition probability after $t$ steps by using the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5\n",
    "x_0 = np.zeros(g.vcount())\n",
    "x_0[0] = 1\n",
    "\n",
    "# Compute x_t by using the eigenvalues and eigenvectors\n",
    "Q_L = np.diag(1.0/np.sqrt(deg)) @ evecs\n",
    "Q_R = np.diag(np.sqrt(deg)) @ evecs\n",
    "x_t = x_0 @ Q_L @ np.diag(evals**t) @ Q_R.T\n",
    "\n",
    "# Compute x_t by using the power iteration\n",
    "x_t_power = x_0.copy()\n",
    "for i in range(t):\n",
    "    x_t_power = x_t_power @ P\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Eigenvector\": x_t.flatten(),\n",
    "    \"Power iteration\": x_t_power.flatten()\n",
    "}).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Comparison of Eigenvector and Power Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02d027",
   "metadata": {},
   "source": [
    "#### Relaxation Time and Mixing Time\n",
    "\n",
    "Let us measure the relaxation time of the random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65805aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eigh(A_norm.toarray())\n",
    "lambda_2 = -np.sort(-evals)[1]\n",
    "tau = 1 / lambda_2\n",
    "print(f\"The relaxation time of the random walk is {tau:.4f}.\")\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Random Walks\n",
    "\n",
    "Suppose you walk in a city. You are drunk and your feet have no idea where to go. You just take a step wherever your feet take you. At every intersection, you make a random decision and take a step. This is the core idea of a random walk.\n",
    "\n",
    "While your feet are taking you to a random street, after making many steps and looking back, you will realize that you have been to certain places more frequently than others. If you were to map the frequency of your visits to each street, you will end up with a distribution that tells you about salient structure of the street network. It is surprising that this seemingly random, brainless behavior can tell us something deep about the structure of the city.\n",
    "\n",
    "\n",
    "<img src=\"../figs/random-walk.png\" alt=\"Random walk on a network\" width=\"50%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "## Random walks in a network\n",
    "\n",
    "A random walk in undirected networks is the following process:\n",
    "1. Start at a node $i$\n",
    "2. Randomly choose an edge to traverse to a neighbor node $j$\n",
    "3. Repeat step 2 until you have taken $T$ steps.\n",
    "\n",
    "\n",
    "```{figure-md} random-walk-example\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/a56ca795f324f75baab70bb3b49e0544c89e05f7/2-Figure1-1.png\" alt=\"Random walk example\" width=\"100%\">\n",
    "\n",
    "Random walk on a small network. The figure is taken from Li, Xing et al. “Representation Learning of Reconstructed Graphs Using Random Walk Graph Convolutional Network.” ArXiv abs/2101.00417 (2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3ffc8",
   "metadata": {},
   "source": [
    "```{note}\n",
    "In case of directed networks, a random walker can only move along the edge direction, and it can be that the random walker is stuck in a so-called ``dead end'' that does not have any outgoing edges.\n",
    "```\n",
    "\n",
    "How does this simple process tell us something about the network structure? To get some insights, let us play with a simple interactive visualization.\n",
    "\n",
    "```{admonition} Random Walk Simulation\n",
    ":class: tip\n",
    "\n",
    "Play with the {{ '[Random Walk Simulator! 🎮✨]( BASE_URL/vis/random-walks/index.html?)'.replace('BASE_URL', base_url) }} and try to answer the following questions:\n",
    "\n",
    "1. When the random walker makes many steps, where does it tend to visit most frequently?\n",
    "2. When the walker makes only a few steps, where does it tend to visit?\n",
    "3. Does the behavior of the walker inform us about centrality of the nodes?\n",
    "3. Does the behavior of the walker inform us about communities in the network?\n",
    "\n",
    "```\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Random walks unify centrality and communities\n",
    "\n",
    "## Modularity: Interpretation from random walk perspective\n",
    "\n",
    "Modularity can be intepreted as a random walk perspective. Modularity is given by\n",
    "\n",
    "$$\n",
    "Q = \\frac{1}{2m} \\sum_{ij} \\left( A_{ij} - \\frac{d_i d_j}{2m} \\right) \\delta(c_i, c_j)\n",
    "$$\n",
    "\n",
    "where $m$ is the number of edges in the network, $A_{ij}$ is the adjacency matrix, $d_i$ is the degree of node $i$, $c_i$ is the community of node $i$, and $\\delta(c_i, c_j)$ is the Kronecker delta function (which is 1 if $c_i = c_j$ and 0 otherwise).\n",
    "\n",
    "We can rewrite the modularity using the language of random walks as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q &= \\sum_{ij} \\left(\\frac{A_{ij}}{2m}  - \\frac{d_i}{2m} \\frac{d_j}{2m} \\right) \\delta(c_i, c_j) \\\\\n",
    "&= \\sum_{ij} \\left(\\pi_i P_{ij}  - \\pi_i \\pi_j \\right) \\delta(c_i, c_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\pi_i$ is the stationary distribution of the random walk given by\n",
    "\n",
    "$$\n",
    "\\pi_i = \\frac{d_i}{2m}\n",
    "$$\n",
    "\n",
    "and $P_{ij}$ is the transition probability between nodes $i$ and $j$.\n",
    "\n",
    "```{note}\n",
    "Let's break down this derivation step by step:\n",
    "\n",
    "1. We start with the original modularity formula:\n",
    "\n",
    "   $$Q = \\frac{1}{2m} \\sum_{ij} \\left( A_{ij} - \\frac{d_i d_j}{2m} \\right) \\delta(c_i, c_j)$$\n",
    "\n",
    "2. First, we move the constant $1/(2m)$ to the inside of the parentheses:\n",
    "\n",
    "   $$Q = \\sum_{ij} \\left(\\frac{A_{ij}}{2m} - \\frac{d_i d_j}{2m^2} \\right) \\delta(c_i, c_j)$$\n",
    "\n",
    "3. Now, we recognize that $\\frac{A_{ij}}{2m}$ can be rewritten as:\n",
    "\n",
    "   $$\\frac{A_{ij}}{2m} = \\frac{d_i}{2m} \\cdot \\frac{A_{ij}}{d_i} = \\pi_i P_{ij}$$\n",
    "\n",
    "4. We also recognize that $\\frac{d_i}{2m}$ is the stationary distribution of the random walk, which we denote as $\\pi_i$:\n",
    "\n",
    "   $$\\frac{d_i}{2m} = \\pi_i$$\n",
    "\n",
    "5. Substituting these into our equation:\n",
    "\n",
    "   $$Q = \\sum_{ij} \\left(\\pi_i P_{ij} - \\pi_i \\pi_j \\right) \\delta(c_i, c_j)$$\n",
    "\n",
    "```\n",
    "\n",
    "The expression suggests that:\n",
    "\n",
    "1. The first term, $\\pi_i P_{ij} \\delta(c_i, c_j)$, represents the probability that a walker is at node $i$ and moves to node $j$ within the same community **by one step**.\n",
    "2. The second term, $\\pi_i \\pi_j$, represents the probability that a walker is at node $i$ and moves to another node $j$ within the same community **after long steps**.\n",
    "\n",
    "In summary, modularity compares short-term and long-term random walk probabilities. High modularity indicates that a random walker is more likely to stay within the same community after one step than after many steps.\n",
    "\n",
    "```{note}\n",
    "Building on this perspective from random walks, Delvenne et al. {footcite}`delvenne2010stability` extends the modularity by comparing multi-step and long-step transition probabilities of a random walk. This approach, known as \"Markov stability\", shows that the number of steps acts as a \"resolution parameter\" that determines the scale of detectable communities.\n",
    "```\n",
    "\n",
    "\n",
    "## PageRank: Interpretation from random walk perspective\n",
    "\n",
    "PageRank can be interpreted from a random walk perspective:\n",
    "\n",
    "$$\n",
    "c_i = (1-\\beta) \\sum_j P_{ji} c_j + \\beta \\cdot \\frac{1}{N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $c_i$ is the PageRank of node $i$\n",
    "- $P_{ji}$ is the transition probability from node $j$ to node $i$\n",
    "- $\\beta$ is the teleportation probability\n",
    "- $N$ is the total number of nodes\n",
    "\n",
    "This equation represents a random walk where:\n",
    "1. With probability $(1-\\beta)$, the walker follows a link to the next node.\n",
    "2. With probability $\\beta$, the walker *teleports* to a random node in the network.\n",
    "\n",
    "The PageRank $c_i$ is the stationary distribution of this random walk, representing the long-term probability of finding the walker at node $i$.\n",
    "\n",
    "```{note}\n",
    "This sounds odd at first glance. But it makes sense when you think about what PageRank was invented for, i.e., Web search. It characterizes a web surfer as a random walker that chooses the next page by randomly jumping to a random page with probability $\\beta$ or by following a link to a page with probability $1-\\beta$. The web page with the largest PageRank means that the page is most likely to be visited by this random web surfer.\n",
    "```\n",
    "\n",
    "```{footbibliography}\n",
    "\n",
    "```# Module 7: Random Walks\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn random walks, one of the most fundamental techniques in network analysis. We will learn:\n",
    "- What is a random walk?\n",
    "- How to simulate a random walk on a network?\n",
    "- What is the behavior of a random walk on a network?\n",
    "- Implicit connections to community detection and network centralities\n",
    "- **Keywords**: random walk, community detection, network centralities"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}