{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054b8985",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m01-euler-tour.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "#  Exercise\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 01\n",
    "\n",
    "1. Create a network of landmasses and bridges of Binghamton, NY.\n",
    "2. Find an Euler path that crosses all the bridges of Binghamton, NY exactly once.\n",
    "\n",
    "![Binghamton Map](https://github.com/skojaku/adv-net-sci/raw/main/docs/lecture-note/figs/binghamton-map.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7ca9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using colab, uncomment the following line\n",
    "# !sudo apt install libcairo2-dev pkg-config python3-dev\n",
    "# !pip install pycairo cairocffi\n",
    "# !pip install igraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77874695",
   "metadata": {},
   "source": [
    "Define the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbfe4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for your code for the exercise\n",
    "edges = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a325d24",
   "metadata": {},
   "source": [
    "Define the adjacnecy matrix (without for loops!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81619036",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beeebdb",
   "metadata": {},
   "source": [
    "Visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bba383e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m       g\u001b[38;5;241m.\u001b[39madd_edge(s, t)\n\u001b[1;32m     13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m igraph\u001b[38;5;241m.\u001b[39mplot(g, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 15\u001b[0m visualize_graph(A)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mvisualize_graph\u001b[0;34m(A, **params)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_graph\u001b[39m(A, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m      6\u001b[0m   A \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(A)\n\u001b[0;32m----> 7\u001b[0m   src, trg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(A)\n\u001b[1;32m      8\u001b[0m   g \u001b[38;5;241m=\u001b[39m igraph\u001b[38;5;241m.\u001b[39mGraph(directed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m   g\u001b[38;5;241m.\u001b[39madd_vertices(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import igraph\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_graph(A, **params):\n",
    "  A = np.array(A)\n",
    "  src, trg = np.where(A)\n",
    "  g = igraph.Graph(directed=False)\n",
    "  g.add_vertices(A.shape[0])\n",
    "  for s, t in zip(src, trg):\n",
    "    for _ in range(A[s, t]):\n",
    "      g.add_edge(s, t)\n",
    "  return igraph.plot(g, **params)\n",
    "\n",
    "visualize_graph(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5c333",
   "metadata": {},
   "source": [
    "Check if the graph has an Euler path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac4f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dbdac50",
   "metadata": {},
   "source": [
    "##  Exercise 02\n",
    "\n",
    "Let's create a network from pre-existing data and check if it has an Euler path.\n",
    "\n",
    "1. Select a network of your choice from [Netzschleuder](https://networks.skewed.de/). For convenience, choose a network of nodes less than 5000.\n",
    "2. Download the csv version of the data by clicking something like \"3KiB\" under `csv` column.\n",
    "3. Unzip the file and find \"edges.csv\", open it with a text editor to familiarize yourself with the format.\n",
    "4. Load the data using `pandas`.\n",
    "5. Get the source and target nodes from the data to create an edge list.\n",
    "6. Construct the adjacency matrix from the edge list.\n",
    "7. Draw the graph using `igraph`.\n",
    "8. Check if the graph has an Euler path.\n",
    "\n",
    "\n",
    "Load the data by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22aeeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('edges.csv') # load the data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe04ff",
   "metadata": {},
   "source": [
    "Then, get the srce and target nodes to compose an edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a42fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ...\n",
    "trg = ...\n",
    "edges = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9783776",
   "metadata": {},
   "source": [
    "Create the adjacency matrix from the edge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea34ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d4f02e9",
   "metadata": {},
   "source": [
    "Get the degree of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa84f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb9684",
   "metadata": {},
   "source": [
    "Visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dfc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_graph(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1b41a",
   "metadata": {},
   "source": [
    "Check if the graph has an Euler path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "launch_buttons:\n",
    "  notebook_interface: \"classic\"\n",
    "  colab_url: \"https://colab.research.google.com\"\n",
    "  thebe: true\n",
    "  binderhub_url: \"https://mybinder.org\"  # The URL for your BinderHub (e.g., https://mybinder.org)\n",
    "---\n",
    "\n",
    "\n",
    "# Euler's solution\n",
    "\n",
    "Euler consider two cases:\n",
    "- a node has an even number of edges, or\n",
    "- a node has an odd number of edges.\n",
    "\n",
    "When a node has an even number $2k$ of edges, one can enter and leave the node $k$ times by crossing different edges.\n",
    "\n",
    "When a node has an odd number $2k+1$ of edges, one can enter and leave the node $k$ times by crossing different edges but leave one last edge to cross. The only way to cross this last edge is that one starts or ends at the node.\n",
    "\n",
    "Based up on the above reasoning, Euler leads to the following necessary (and later shown as sufficient) conditions:\n",
    "\n",
    ":::{admonition} Euler's path\n",
    "\n",
    "There exists a walk that crosses all edges exactly once if and only if all nodes have even number of edges, or exactly two nodes have an odd number of edges.\n",
    ":::\n",
    "\n",
    "![alt text](https://lh3.googleusercontent.com/-CYxppcJBwe4/W2ndkci9bVI/AAAAAAABX-U/K6SNM8gAhg0oNsnWNgQbH3uKNd5Ba10wwCHMYCw/euler-graph-bridges2?imgmax=1600)\n",
    "\n",
    "Back to the Konigsberg bridge problem, every node has an odd number of edges, meaning that there is no way to cross all edges exactly once. What a sad story for the citizens of Konigsberg. But the problem was solved during World War II, where Koingberg was bombarded by Soviet Union, losing two of the seven bridges 🫠.\n",
    "\n",
    ":::{figure-md} markdown-fig\n",
    "<img src=\"../figs/seven-bridge-bombared.png\" alt=\"fishy\" width=\"50%\">\n",
    "\n",
    "Two bridges were bombed by Soviet Union, which allows the Euler path to exist.\n",
    ":::\n",
    "\n",
    "---\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/docs/lecture-note/m01-euler_tour/how-to-code-network.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Compute with networks\n",
    "\n",
    "So far we worked out the network of bridges of Konigsberg by illustrating the network with points and lines.\n",
    "From now, we will work with a representation of the network that can be easily computed with code.\n",
    "\n",
    "## Network representation\n",
    "\n",
    "An atomic element of a network is a node, i.e., a network is a collection of edges which are pairs of nodes.\n",
    "We *label* a unique integer as an identifier for each node. For instance, the bridges of Konigsberg has 4 nodes, and we assign the number 0 to 3 to the nodes. An edge can be represented by a pair of nodes. For instance, the edge between node 0 and node 1 can be represented by the pair `(0, 1)`.\n",
    "\n",
    "\n",
    "```{figure-md} numbered-koningsberg-graph\n",
    "\n",
    "![file](https://github.com/skojaku/adv-net-sci/blob/gh-pages/_images/labeled-koningsberg.jpg?raw=true)\n",
    "\n",
    "Labeled Knigsberg graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba6b3a",
   "metadata": {},
   "source": [
    "```{note}\n",
    ":name: node-labeling\n",
    "We label nodes starting from 0 with consecutive numbers, which is convenient for Python. However, this is *not the only way* to label nodes.\n",
    "```\n",
    "\n",
    "The Konigsberg graph can be represented by a list of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [(0,1), (0, 1), (0, 3), (1, 2), (1, 2), (1, 3), (2, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f169e7",
   "metadata": {},
   "source": [
    "Another, more convenient format is the *adjacency matrix*.\n",
    "In this form, one regard the node index as a coordinate in the matrix. For instance, edge $(1,3)$ is represented by the entry in the second row and fourth column. The entry of the matrix represents the number of edges between two nodes. Thus, the zeros in the matrix represent the absence of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7148a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[0, 2, 0, 1],\n",
    "     [2, 0, 2, 1],\n",
    "     [0, 2, 0, 1],\n",
    "     [1, 1, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc528d68",
   "metadata": {},
   "source": [
    "or equivalently, using for loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e669cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.zeros((4, 4))\n",
    "for i, j in edges:\n",
    "    A[i][j] += 1\n",
    "    A[j][i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d16c2f",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "In the Konigsberg graph, the edges are *undirected*, meaning edge (i,j) is the same as edge (j,i), which is why we increment both entries $(i,j)$ and $(j,i)$ in the for loop. If the edges are *directed*, we treat (i,j) and (j,i) as two different edges, and increment only (i,j).\n",
    ":::\n",
    "\n",
    "## Edge counting\n",
    "\n",
    "Let us showcase the convenience of the adjacency matrix by counting the number of edges in the network.\n",
    "\n",
    "The total number of edges in the network is the sum of the entities in the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(A) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c178a3",
   "metadata": {},
   "source": [
    "We divide by 2 because an edge corresponds to two entries in the matrix. Now, let us consider\n",
    "\n",
    "It is also easy to compute the number of edges pertained to individual nodes by taking the row or column sum of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2467cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(A, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7bbe9",
   "metadata": {},
   "source": [
    "The result is an array of length 4, where the i-th entry is the number of edges connected to node i.\n",
    "\n",
    ":::{important}\n",
    "The number of edges connected to a node is called the ***degree*** of the node.\n",
    ":::\n",
    ":::{tip}\n",
    "The `np.sum(A, axis = 1)` is the column sum of `A`. Alternatively, `np.sum(A, axis = 0)` is the row sum of `A`.\n",
    "Check out the numpy [documentation](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) for more details.\n",
    ":::\n",
    ":::{tip}\n",
    "If the adjacency matrix is `scipy` CSR format (or CSC format), you can instead use `A_csr.sum(axis=1)`, `A_csr.sum(axis=0)`, and `A_csr.sum()`.\n",
    "Check out the [scipy documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) for more details.\n",
    ":::\n",
    "\n",
    "We can check the number of nodes with odd degree by taking the modulus of the degree by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32430f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = np.sum(A, axis = 1)\n",
    "is_odd = deg % 2 == 1\n",
    "is_odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.sum(is_odd) == 2 or np.sum(is_odd) == 0:\n",
    "    print(\"The graph has a Euler path.\")\n",
    "else:\n",
    "    print(\"The graph does not have a Euler path.\")\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# A puzzle\n",
    "\n",
    "Back in 18th century, there was a city called *Königsberg* situated on the Pregel River in a historical region of Germany. The city had two large islands connected to each other and the mainland by seven bridges.\n",
    "The citizens of Königsberg pondered a puzzle during their Sunday walks:\n",
    "\n",
    "```{admonition} Problem\n",
    "How could one walk through the city and cross each bridge exactly once?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28686804",
   "metadata": {},
   "source": [
    ":::{figure-md} seven-bridges\n",
    "![alt text](https://99percentinvisible.org/wp-content/uploads/2022/02/bridges-with-water-600x418.png)\n",
    "\n",
    "The seven bridges of Königsberg\n",
    ":::\n",
    "\n",
    "Leonard Euler worked out the solution to this puzzle in 1736. He first simplified the city into *a network of landmasses connected by bridges*, by noting that the landareas, the positions of the islands and the bridges are nothing to do with the puzzle, and that the only thing that matters is the connections between the landmasses.\n",
    "\n",
    ":::{figure-md} euler-graph\n",
    "<img src=\"https://lh3.googleusercontent.com/-CYxppcJBwe4/W2ndkci9bVI/AAAAAAABX-U/K6SNM8gAhg0oNsnWNgQbH3uKNd5Ba10wwCHMYCw/euler-graph-bridges2?imgmax=1600\">\n",
    "\n",
    "Euler's graph of the bridges of Knigsberg\n",
    ":::\n",
    "\n",
    "\n",
    "## Pen-and-paper worksheet\n",
    "\n",
    "Let's follow the worksheet to solve the puzzle step by step.\n",
    "\n",
    "- [Worksheet](http://estebanmoro.org/pdf/netsci_for_kids/the_konisberg_bridges.pdf) {cite:p}`esteban-moro-worksheet`\n",
    "\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    ":filter: docname in docnames\n",
    "```# Module 1: Euler Tour\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "\n",
    "\n",
    "In this module, we will learn a historical example that leads to the genesis of graph theory in mathematics and modern network science. Through this example, we will learn:\n",
    "- How to describe a network using mathematical language\n",
    "- How to code a network in Python\n",
    "- Keywords: **network**, **degree**, **Euler walk**---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Appendix\n",
    "\n",
    "\n",
    "## Compressed Sparse Row (CSR) format\n",
    "\n",
    "CSR format is implemented in [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html). This consists of three arrays called `indptr`, `indices`, and `data`. For example,\n",
    "\n",
    "```{code-cell} python\n",
    "import networkx as nx\n",
    "from scipy import sparse\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "A = sparse.csr_matrix(nx.adjacency_matrix(G))\n",
    "\n",
    "print(\"A.indices:\", A.indices[:5])\n",
    "print(\"A.indptr:\", A.indptr[:5])\n",
    "print(\"A.data:\", A.data[:5])\n",
    "```\n",
    "\n",
    "We will walk you through what these arrays mean, how they are generated, and how we can leverage them for efficient computations.\n",
    "\n",
    "### How to generate CSR format from an adjacency matrix\n",
    "\n",
    "Let's walk you through how to store an example adjacency matrix in Compressed Sparse Row (CSR) format. Our example adjacency matrix is as follows.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "|      |    0 |    1 |    2 |    3 |    4 |    5 |    6 |    7 |    8 |    9 |   10 |\n",
    "| ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: | ---: |\n",
    "|    0 |      |      |      |      |      |      |      |      |      |      |    1 |\n",
    "|    1 |      |      |    1 |      |      |      |      |      |      |      |    1 |\n",
    "|    2 |      |    1 |      |    1 |      |      |      |      |      |      |    1 |\n",
    "|    3 |      |      |    1 |      |    1 |    1 |    1 |      |      |      |      |\n",
    "|    4 |      |      |      |    1 |      |      |      |    1 |      |      |      |\n",
    "|    5 |      |      |      |    1 |      |      |      |      |      |      |      |\n",
    "|    6 |      |      |      |    1 |      |      |      |      |    1 |    1 |      |\n",
    "|    7 |      |      |      |      |    1 |      |      |      |      |      |      |\n",
    "|    8 |      |      |      |      |      |      |    1 |      |      |      |    1 |\n",
    "|    9 |      |      |      |      |      |      |    1 |      |      |      |    1 |\n",
    "|   10 |    1 |    1 |    1 |      |      |      |      |      |    1 |    1 |      |\n",
    "</div>\n",
    "\n",
    "We will first create **adjacency list**, which is a dictionary consisting of the row IDs and column IDs for the non-zero entries in the adjacency matrix.\n",
    "\n",
    "<div class=\"container\" align=\"center\">\n",
    "<div class=\"col\" style=\"margin-top:0%\">\n",
    "\n",
    "$\\{\\text{Row ID}: (\\text{Column ID}, \\text{Value})\\}$\n",
    "\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Concretely, in Python,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c933024",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list = {\n",
    "  0:[(10,1)],\n",
    "  1:[(2,1), (10, 1)],\n",
    "  2:[(1,1), (3,1), (10, 1)],\n",
    "  3:[(2,1), (4,1), (5,1), (6,1)],\n",
    "  #...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a4b58",
   "metadata": {},
   "source": [
    "CSR format is a *concatenation* of the keys and values of the adjacency list, respectively. The CSR format has a concatenated array of the values, one for column IDs and one for the values, called `indices` and `data`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "indices = np.array([vv[0] for k, v in adj_list.items() for vv in v])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([vv[1] for k, v in adj_list.items() for vv in v])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5590ef9",
   "metadata": {},
   "source": [
    "Additionally, the CSR format has another array called `indptr`, which stores the Row IDs of the non-zero entries in the adjacency matrix. This `indptr` array has a value such that `indptr[i]` is the first index of `indices` that corresponds to the `i`-th row of the adjacency matrix. This can be generated by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0788d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "indptr = np.cumsum([0] + [len(adj_list[i]) for i in range(len(adj_list))])\n",
    "indptr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ead571",
   "metadata": {},
   "source": [
    "where we added `0` at the beginning of the array to represent the first non-zero entry in the first row.\n",
    "The first row ends at index `len(adj_list[0])-1`, and the second row starts at index `len(adj_list[0])` and ends at index `len(adj_list[0])+len(adj_list[1])-1`, and so on.\n",
    "\n",
    "Now we have three compressed vectors `indptr`, `indices`, and `data`, that together form the CSR format for the adjacency matrix.\n",
    "\n",
    "\n",
    "### How to use CSR format for efficient computations\n",
    "\n",
    "The key advantage of the CSR representation is the memory efficiency. But you can leverage the CSR format for more efficient computations, if you know the semantics of `indptr`, `indices`, and `data` arrays.\n",
    "\n",
    "For instance, one can compute the degree of a node by using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = 1\n",
    "degree = indptr[node+1] - indptr[node]\n",
    "degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727bf33",
   "metadata": {},
   "source": [
    "Let us break down the above code.\n",
    "- `indptr[node]` is the first index of the `indices` array that corresponds to the `node`-th row of the adjacency matrix.\n",
    "- `indptr[node+1]` is the first index of the `indices` array that corresponds to the `(node+1)`-th row of the adjacency matrix.\n",
    "- Thus, `indptr[node+1] - indptr[node]` is the number of non-zero entries in the `node`-th row of the adjacency matrix, which is the degree of the `node`-th node.\n",
    "\n",
    "Using `indices`, it is easy to identify the neighbors of a given node by using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406e31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = indices[indptr[node]:indptr[node+1]]\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30fdeef",
   "metadata": {},
   "source": [
    "where `indices[indptr[node]:indptr[node+1]]` is the corresponding column IDs of the non-zero entries in the `node`-th row of the adjacency matrix, which corresponds to the node IDs connected to the `node`-th node.\n",
    "\n",
    "The edge weights to the neighbors can be obtained by using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccd18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weights = data[indptr[node]:indptr[node+1]]\n",
    "edge_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714845f8",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "We will compute the average path length of a network of scientists. The network is constructed from {footcite:p}`sinatra2016quantifying`, where each node represents a scientist and two scientists are connected if they have co-authored a paper in Physical Review Journals from American Physical Society.\n",
    "\n",
    "- **For students enrolled in SSIE 641**\n",
    "  - You will receive a dedicated link to the assignment repository from the instructor.\n",
    "- *For those who are not enrolled in SSIE 641*\n",
    "  - You can access the assignment repository at [Github](https://github.com/sk-classroom/adv-net-sci-small-world).\n",
    "  - This repository does not offer auto-grading. But you can grade the assignment by yourself by\n",
    "    - `bash grading-toolkit/grade_notebook.sh tests/test_01.py assignment/assignment.ipynb`\n",
    "    - `bash grading-toolkit/grade_notebook.sh tests/test_02.py assignment/assignment.ipynb`\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .Rmd\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.16.3\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Efficient representation for large sparse networks\n",
    "\n",
    "An adjacency matrix is a convenient way to represent a network.\n",
    "A challenge of handling large networks is that the adjacency matrix can be too large to fit in memrory.\n",
    "For example, a network with $10^5$ nodes requires a $10^5 \\times 10^5$ matrix, totaling $10$ billion entries!\n",
    "A good news is that we do not need to hold all these entries in memory, if we know the network is *sparse*.\n",
    "\n",
    "Many networks in real-world are sparse, meaning most nodes connect to only a few others.\n",
    "The result is that the adjacency matrix often contains many zeros.\n",
    "This is where we can save significant memory by storing only the non-zero entries.\n",
    "\n",
    "**Compressed Sparse Row (CSR)** is an efficient way to store sparse networks by treating the adjacency matrix like a scatter plot. Instead of storing all entries, CSR only keeps track of the \"coordinates\" (row and column indices) of non-zero entries, along with their values.\n",
    "\n",
    ":::{admonition} Optional Exercise\n",
    ":class: tip\n",
    "For those who are interested in the details of CSR format, please do the following:\n",
    "- 📝 Pen and paper exercise [here](./pen-paper-csr/exercise.pdf)\n",
    "- 💻 (Advanced) Coding exercise in the [Appendix](./appendix.md).\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{figure-md} csr_matirx\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*zWhtdW4nYTSO3nya.gif\" width=\"100%\">\n",
    "\n",
    "Compressed Sparse Row (CSR) matrix. Source: [Medium: Sparse GEMM and Tensor Core’s Structured Sparsity](https://medium.com/@hxu296/exploring-spgemm-and-nvidias-leap-in-deep-neural-network-efficiency-d367adc68791)\n",
    ":::\n",
    "\n",
    "The CSR format is implemented in the [scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) library. It is straightforward to convert the CSR matrix from the *dense* adjacency matrix.\n",
    "```{code-cell} ipython3\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "A = [[0, 2, 0, 1],\n",
    "     [2, 0, 2, 1],\n",
    "     [0, 2, 0, 1],\n",
    "     [1, 1, 1, 0]]\n",
    "\n",
    "A_csr = csr_matrix(A)\n",
    "A_csr\n",
    "```\n",
    "\n",
    "If you have an *edge list*, you can directly generate the CSR matrix without creating the dense matrix first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e65d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "edges = [(0,1), (0, 1), (0, 3), (1, 2), (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "src = [edge[0] for edge in edges]\n",
    "trg = [edge[1] for edge in edges]\n",
    "values = [1 for _ in edges]\n",
    "A_csr = csr_matrix((values, (src, trg)), shape=(4, 4))\n",
    "A_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4bb97",
   "metadata": {},
   "source": [
    "where `src`, `trg`, and `values` are lists of the source nodes, target nodes, and edge weights, respectively.\n",
    "---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .Rmd\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.16.3\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m02-small-world.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Computing the Shortest Paths and Connected Components\n",
    "\n",
    "Let's use `igraph` to compute the shortest paths and connected components. We will then use `scipy` to compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Google Colab, uncomment the following line to install igraph\n",
    "# !sudo apt install libcairo2-dev pkg-config python3-dev\n",
    "# !pip install pycairo cairocffi\n",
    "# !pip install igraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b7d10",
   "metadata": {},
   "source": [
    "## igraph\n",
    "\n",
    "### Create a graph\n",
    "\n",
    "Let us create a graph of 4 nodes and 4 edges. Our edge list is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5889c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = [(0, 1), (1, 2), (0, 2), (0, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4b1f1",
   "metadata": {},
   "source": [
    "`igraph` has an object `Graph` that stores a graph and provides methods to manipulate and analyze the graph. To create a graph from an edge list, we can use the `add_edges` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "\n",
    "g = igraph.Graph() # Create an empty graph\n",
    "g.add_vertices(4) # Add 4 vertices\n",
    "g.add_edges(edge_list) # Add edges to the graph\n",
    "\n",
    "# Plot the graph\n",
    "igraph.plot(g, bbox=(150, 150), vertex_label=list(range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da25fa4",
   "metadata": {},
   "source": [
    "### Shortest Paths\n",
    "\n",
    "Let's compute the paths between nodes 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_all_simple_paths(2, to=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03431e6b",
   "metadata": {},
   "source": [
    "This method enumerates all possible simple paths between two nodes. This is OK for small networks but quickly becomes impractical for larger networks, as the number of paths increases exponentially with the size of the network.\n",
    "\n",
    "Often, we are interested in the shortest path, which is the path with the smallest number of edges. The shortest path can be computed by using the `get_shortest_paths` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753bccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_shortest_paths(2, to=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93caab2d",
   "metadata": {},
   "source": [
    "Note that there can be multiple shortest paths between two nodes. If we are interested in the \"length\" instead of the path itself, there is a more efficient function `distances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc489f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.distances(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71c9ae",
   "metadata": {},
   "source": [
    "### Connected Components\n",
    "\n",
    "In the simple network above, we can see that for every pair of nodes, we can find a path connecting them. This is the definition of a connected graph. We can check this property for a given graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c563767",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = g.connected_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5d509",
   "metadata": {},
   "source": [
    "The `components` is a special object called [VertexClustering](https://python.igraph.org/en/0.11.6/api/igraph.VertexClustering.html) in `igraph`.\n",
    "It has the following useful functions and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"membership: \", components.membership)  # the IDs of the component each node belongs to.\n",
    "print(\"sizes: \", list(components.sizes()))  # the number of nodes in each component.\n",
    "print(\"giant: \", components.giant())  # a subgraph of the largest connected component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a73a3d",
   "metadata": {},
   "source": [
    "#### Exercise 01 🏋️‍♀️💪🧠\n",
    "\n",
    "1. Now, let us add two nodes that are not connected to the existing graph, and call `connected_components` again. 🔗➕\n",
    "\n",
    "2. Call `get_shortest_paths` between the two new nodes in different connected components. 🛣️🔍\n",
    "\n",
    "3. Get the largest connected component. 🌐🏆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a0879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88838f2c",
   "metadata": {},
   "source": [
    "### Directed networks\n",
    "Let's extend these ideas about paths and connected components to directed graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808469f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list =[(0, 1), (1, 2), (2, 1), (2, 3), (2, 5), (3, 1), (3, 4), (3, 5), (4, 5), (5, 3)]\n",
    "g = igraph.Graph(directed=True)\n",
    "g.add_vertices(6)\n",
    "g.add_edges(edge_list)\n",
    "igraph.plot(g, bbox=(250, 250), vertex_label=list(range(6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddb3c8",
   "metadata": {},
   "source": [
    "In directed graphs, edges and paths can be one-way. For instance, in our graph, you can go from node 0 to node 3, but not from 3 to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From 0 to 3\", g.get_all_simple_paths(0, to=3))\n",
    "print(\"From 3 to 0\", g.get_all_simple_paths(3, to=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c76cea",
   "metadata": {},
   "source": [
    "The shortest path from 4 to 1 must take a longer route due to edge directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de16cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_shortest_paths(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a38ae93",
   "metadata": {},
   "source": [
    "Directed networks have two kinds of connected components.\n",
    "\n",
    "- **Strongly connected components:** Strongly connected means that there exists a direct path between every pair of nodes, i.e., that from any node to any other nodes while respecting the edge directionality.\n",
    "- **Weakly connected components:** Weakly connected means that there exists a path between every pair of nodes when ignoring the edge directionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce474fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(g.connected_components(mode=\"strong\")))\n",
    "print(list(g.connected_components(mode=\"weak\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89be2a3",
   "metadata": {},
   "source": [
    "## Scipy\n",
    "\n",
    "We can create a graph, compute the shortest paths, and connected components using `scipy`.\n",
    "\n",
    "### Create a graph\n",
    "\n",
    "\n",
    "With scipy, we represent a network by an adjacency matrix using something called a *Compressed Sparse Row (CSR) matrix*. CSR matrices are efficient format for storing and manipulating *sparse* matrices. Why *sparse* is highlighed here? Because in many networks, the adjacency matrix is sparse, i.e., most of the entries are zero. For example, here is the adjacency matrix of a real-world network:\n",
    "\n",
    "![](https://www.researchgate.net/publication/263506932/figure/fig1/AS:392539896074252@1470600212952/Scale-free-Network-left-its-adjacency-matrix-upper-right-and-degree-distribution.png)\n",
    "\n",
    "Most of the entries in this adjacency matrix are white, and white means that the value of the entry is zero. And the adjacency matrix looks very white! This is pretty common in real-world networks. We call these matrices \"sparse\" because they are mostly empty. And CSR matrices are a way to store these sparse matrices efficiently. Don't worry too much about the technical details for now. If you're curious to learn more, you can check out the [Appendix](appendix.md).\n",
    "\n",
    "The great thing is, `scipy` (especially the `scipy.sparse` module) provides efficient tools for working with these sparse matrices. This comes in really handy when we're working with large networks.\n",
    "\n",
    "Let create a graph using scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67769ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse # We will use sparse module in scipy\n",
    "\n",
    "edge_list = [(0, 1), (1, 2), (0, 2), (0, 3)]\n",
    "\n",
    "src = [src for src, dst in edge_list]\n",
    "trg = [dst for src, dst in edge_list]\n",
    "weight = [1 for src, dst in edge_list]\n",
    "\n",
    "A = sparse.csr_matrix((weight, (src, trg)), shape=(4, 4))\n",
    "A = A + A.T # Make the adjacency matrix symmetric\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72db004",
   "metadata": {},
   "source": [
    "Let's break down the code.\n",
    "- `src` and `trg` are the source and target nodes of the edges.\n",
    "- `weight` is the weight of the edges.\n",
    "- `sparse.csr_matrix((weight, (src, trg)))` creates a sparse matrix, filling `weight` into the positions specified by `(src, trg)`.\n",
    "- `A.T` is the transpose of `A` and `A + A.T` makes the adjacency matrix symmetric.\n",
    "\n",
    "The CSR matrix does not print nicely. But you can see it by converting to a numpy array and printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5917e4",
   "metadata": {},
   "source": [
    "## Shortest Paths\n",
    "\n",
    "The `sparse` module has a submodule `csgraph` that provides APIs for network analysis.\n",
    "\n",
    "For example, `csgraph.shortest_path` computes the shortest path length from a specific node to all other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csgraph\n",
    "\n",
    "# `indices` is the node to compute the shortest path from.\n",
    "D = csgraph.shortest_path(A, indices=2, directed=False)\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f16568",
   "metadata": {},
   "source": [
    "**Advanced:** If you want to get the actual paths (i.e., list of nodes in the path), you can pass `return_predecessors=True` to `csgraph.shortest_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23119a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, predecessors = csgraph.shortest_path(A, indices=2, directed=False, return_predecessors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cad5c9",
   "metadata": {},
   "source": [
    "## Connected Components\n",
    "\n",
    "Connected components can be computed by `csgraph.connected_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3482d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components, labels = csgraph.connected_components(A, directed=False, return_labels=True)\n",
    "\n",
    "print(\"Number of connected components:\", n_components)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41219bf",
   "metadata": {},
   "source": [
    "- `n_components` is the number of connected components.\n",
    "- `labels` is an array of length `n_nodes` where each element is the ID of the connected component the node belongs to.\n",
    "\n",
    "\n",
    "## Exercise 02 🏋️‍♀️💪🧠\n",
    "\n",
    "Let's compute the average path length of a network from pre-existing data and check if how long on average it takes to go from any node to any other node.\n",
    "\n",
    "1. Select a network of your choice from [Netzschleuder](https://networks.skewed.de/). For convenience, choose a network of nodes less than 5000.\n",
    "2. Download the csv version of the data by clicking something like \"3KiB\" under `csv` column.\n",
    "3. Unzip the file and find \"edges.csv\", open it with a text editor to familiarize yourself with the format.\n",
    "4. Load the data using `pandas`.\n",
    "5. Get the source and target nodes from the data to create an edge list.\n",
    "6. Construct a graph from the edge list, either using `igraph` or `scipy`.\n",
    "7. Compute the average path length\n",
    "\n",
    "**Hint:** Finding all shortest paths is a qubic time operation with respect to the number of nodes, or simply put, it takes a long time to compute. So compute the \"estimate\" by sampling many pairs of nodes uniformly at random and computing the average path length.---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .Rmd\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.16.3\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Walks, Trails, Paths, and Connectedness\n",
    "\n",
    "## Walks, Trails, Paths\n",
    "\n",
    "While we have already used the term **path**, let us make clear its definition, together with other related terms.\n",
    "\n",
    "- A **walk** is a sequence of nodes that are connected to form a continous route in a network. For instance, walk (0, 1, 2, 3) is a walk in the graph of the bridges of Konigsberg. But the sequence (0,2,3,1) is not a walk, because the node 0 is not directly connected to node 2.\n",
    "\n",
    "- A **trail** is a walk with no repeated edge. For instance, walk (0, 1, 2, 3) is also a trail as it does not cross the same edge twice. But walk (0,2,3,1,3) is not a trail due to the repeated edge (1,3).\n",
    "\n",
    "- A **path** is a walk without repeated node. For instance, walk (0,1,2,3) is a path. But walk (0, 1, 2, 1, 2, 3) is not a path due to the repeated node 1 and 2.\n",
    "\n",
    "- When a walk starts and ends at the same node, it is called a **loop*. If the loop is a trail, it is called a **circuit**. If the loop is a path, it is called a **cycle**.\n",
    "\n",
    "***Question***: Is a path always a trail, and is a trail always a path?\n",
    "\n",
    ":::{figure-md} numbered-koningsberg-graph2\n",
    "\n",
    "<img src= \"../figs/labeled-koningsberg.jpg\" width=\"30%\">\n",
    "\n",
    "Labeled Knigsberg graph\n",
    "\n",
    ":::\n",
    "\n",
    "- **Shortest Path** is the path with the smallest number of edges (or nodes) between two nodes.\n",
    "A shortest path from node 0 to 2 is (0, 1, 2). Two nodes can have multiple shortest paths e.g., (0, 3, 2).\n",
    "- **The shortest path length** is the number of edges in the shortest path, *not the number of nodes!* 👈👈\n",
    "\n",
    ":::{note} Are there **shortest trails** and **shortest walks**?\n",
    "Shortest trails and shortest walks are fundamentally equivalent to shortest paths. A shortest trail must visit each node only once (otherwise it would not be the shortest), and similarly, a shortest walk does not repeat nodes (otherwise it would not be the shortest), both forming a shortest path.\n",
    ":::\n",
    "\n",
    "\n",
    "## Connectedness\n",
    "\n",
    "- A network is **connected** if there is a path between every pair of nodes.\n",
    "- A network is **disconnected** if there is no path between some pairs of nodes.\n",
    "- **A connected component** of a network is a set of nodes that are connected to each other.\n",
    "- **The giant component** of a network is the largest connected component that contains a significant fraction of nodes in the network (in order of the number of nodes).\n",
    "\n",
    ":::{figure-md} connected-components\n",
    "\n",
    "<img src= \"../figs/connected-component.jpg\" width=\"50%\">\n",
    "\n",
    "connected components of a network. the nodes with the same color form a connected component.\n",
    "\n",
    ":::\n",
    "\n",
    "## Connectedness in directed networks\n",
    "\n",
    "We call a network is *directed* if the edges have a direction. Example directed networks include the network of Web pages, the network of friendships on X, the network of citations on academic papers.\n",
    "\n",
    "In a directed network, a walk must follow the edge directions. Paths, trails, and loops extend similarly to directed networks. But one thing to keep in mind: a walk may not be reversible, meaning there can be a walk from one node to another but not vice versa.\n",
    "\n",
    "This leads to two different types of `connectedness` as follows:\n",
    "\n",
    "- **Strong connectedness**: A directed network is said to be strongly connected if there is a path from every node to every other node.\n",
    "- **Weak connectedness**: A directed network is said to be weakly connected if there is a path from every node to every other node on its *undirected* counterpart.\n",
    "\n",
    "\n",
    ":::{figure-md} connected-components-directed\n",
    "\n",
    "<img src= \"../figs/connected-component-directed.jpg\" width=\"50%\">\n",
    "\n",
    "connected components of a network. the nodes with the same color form a connected component.\n",
    "\n",
    ":::\n",
    "\n",
    "**Question**: Is a strongly-connected component always a weakly-connected component?\n",
    "\n",
    "In the next section, we will learn how to compute the shortest paths and connected components of a network using a library [igraph](https://python.igraph.org/en/stable/).\n",
    "\n",
    "# Why is our social network small world?\n",
    "\n",
    "\n",
    "- ✍️ [It’s a small world!! 6 degrees of separation](http://estebanmoro.org/pdf/netsci_for_kids/6_degrees_of_separation.pdf) {footcite}`esteban-moro-worksheet`\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```\n",
    "---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .Rmd\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.16.3\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Small-world experiment\n",
    "\n",
    "How far are two people in a social network? Milgram and his colleagues conducted a series of expriment to find out in the 1960s.\n",
    "\n",
    ":::{figure-md} milgram-small-world-experiment\n",
    "\n",
    "<img src=\"../figs/milgram-small-world-experiment.png\" width=\"70%\">\n",
    "\n",
    "Milgram's small world experiment.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "The experiment went as follows:\n",
    "1. Milgram first sent out packets to randomly selected people in Omaha, Nebraska, and Wichita, Kansas.\n",
    "2. The recipient was asked to send the packet to the target person in Boston if they knew them. If not, they were to forward it to someone they knew on a first-name basis who might know the target.\n",
    "3. The recipient continued to forward the packet to their acquaintances until it reached the target.\n",
    "\n",
    "The results were surprising: out of the 160 letters sent, 64 successfully reached the target person by the chain of nearly six people, which was later called **six degrees of separation**.\n",
    "The results imply that, despite the fact that there were hundreds of millions of people in the United States, their social network was significantly compact, with two random people being connected to each other in only a few steps.\n",
    "\n",
    ":::{tip}\n",
    "The term \"Six degrees of separation\" is commonly associated with Milgram's experiment, but Milgram never used it. John Guare coined the term for his 1991 play and movie [\"Six Degrees of Separation.\"](https://en.wikipedia.org/wiki/Six_Degrees_of_Separation_(film))\n",
    ":::\n",
    "\n",
    "The results were later confirmed independently.\n",
    "\n",
    "-  Yahoo research replicate the Milgram's experiment by using emails. Started from more than 24,000 people, only 384 people reached the one of the 18 target person in 13 countries. Among the successful ones, the average length of the chain was about 4. When taken into account the broken chain, the average length was estimated between 5 and 7.{footcite}`goel2009social`\n",
    "\n",
    "- Researchers in Facebook and University of Milan analyzed the social network n Facebook, which consisted of 721 million active users and 69 billion friendships. The average length of the shortest chain was found to be 4.74. {footcite}`backstrom2012four`\n",
    "\n",
    "```{footbibliography}\n",
    "```# Module 2: Small-world\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn small-world experiments and conduct a small small-world experiment . We will learn:\n",
    "- Small-world experiment by Milgram\n",
    "- Different concepts of *distance*: path, walks, circuits, cycles, connectedness\n",
    "- How to load a large sparse network efficiently into memory\n",
    "- How to measure a *distance* between two nodes using `igraph`\n",
    "- **Keywords**: small-world experiment, six degrees of separation, path, walks, circuits, cycles, connectedness, connected component, weakly connected component, strongly connected component, compressed sparse row format.\n",
    "---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .Rmd\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.16.3\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Toolbox for network analysis\n",
    "\n",
    "Here are some useful tools for network analysis in Python:\n",
    "\n",
    "- [Python built-in data structures](https://docs.python.org/3/tutorial/datastructures.html) (list, tuple, dict)\n",
    "- [networkx](https://networkx.org/) - a beginner-friendly library for network analysis\n",
    "- [igraph](https://igraph.org/python/) - a mature library with a wide range of algorithms\n",
    "- [graph-tool](https://graph-tool.skewed.de/) - for stochastic block models\n",
    "- [scipy](https://scipy.org/) - for analyzing large networks\n",
    "- [pytorch-geometric](https://pytorch-geometric.readthedocs.io/en/latest/) - for graph neural networks\n",
    "\n",
    "While `networkx` is a popular and beginner-friendly library for network analysis in Python, we'll be using `igraph` in this course. `igraph` is a mature library with a wide range of algorithms, originally developed for R.\n",
    "Why `igraph` instead of `networkx`?\n",
    "Because networkx has some persistent bugs in the implementations of some algorithms (e.g., LFR benchmark and weighted degree assortativity), which can skew the analysis.\n",
    "`igraph` offers more reliable implementations.\n",
    "\n",
    "We'll also use `scipy` for scientific computing. `scipy` is one of the most popular Python libraries and also a powerful network analysis tool, especially for large networks. While it requires a bit more effort to learn, once you get the hang of it, you'll find it's a powerful tool for your network analysis projects.\n",
    "\n",
    "![](../figs/scipy.jpg)\n",
    "\n",
    "\n",
    "---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .Rmd\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.16.3\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Wikirace\n",
    "\n",
    "Let us feel how small a large network can be by playing the [Wikirace](https://wiki-race.com) game.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://cdn.sparkfun.com/assets/home_page_posts/3/8/8/0/Wikirace.jpeg\" alt=\"Wikirace\" width=\"70%\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### What's next\n",
    "\n",
    "At the end of the module, we will measure the average path length in a social network.\n",
    "Before jumping on, let us arm with some coding techniques to handle the network in the next two sections.\n",
    "\n",
    "```{bibliography}\n",
    ":style: unsrt\n",
    ":filter: docname in docnames\n",
    "```# Appendix\n",
    "\n",
    "## Derivation of the Molloy-Reed criterion\n",
    "\n",
    "Molloy and Reed derived the following criterion for an existence of a giant component in a network with an arbitrary degree distribution {footcite}`molloy1995critical`.\n",
    "It is based on a simple heuristic argument: the network has a giant component when a random node $i$ with neighbor $j$ has, on average, is connected to at least one other node. We write the condition as\n",
    "\n",
    "$$\n",
    "\\langle k_i \\vert i \\leftrightarrow j \\rangle = \\sum_{k} k P(k \\vert i \\leftrightarrow j) > 2\n",
    "$$\n",
    "\n",
    "where $\\langle k_i \\vert i \\leftrightarrow j \\rangle$ is the conditional average degree of node $i$ given that it is connected to node $j$. From Bayes' theorem, we have\n",
    "\n",
    "$$\n",
    "P(k_i \\vert i \\leftrightarrow j) = \\frac{P(i \\leftrightarrow j \\vert k_i) P(k_i)}{P(i \\leftrightarrow j)}\n",
    "$$\n",
    "\n",
    "Assuming that the network is uncorrelated and sparse (meaning, we neglect loops), then $P(i \\leftrightarrow j \\vert k_i) = k_i / (N-1)$ and $P(i \\leftrightarrow j) = \\langle k \\rangle / (N-1)$. Substituting these into the above equation, we get\n",
    "\n",
    "$$\n",
    "P(k_i \\vert i \\leftrightarrow j) = \\frac{k_i P(k_i)}{\\langle k \\rangle}\n",
    "$$\n",
    "\n",
    "Thus, the condition for the existence of a giant component is\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\langle k \\rangle} \\sum_{k_i} k_i^2 P(k_i) = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} > 2\n",
    "$$\n",
    "\n",
    "\n",
    "## Derivation of the percolation threshold for a random attack.\n",
    "\n",
    "Assume a fraction $p$ of nodes are removed independently from the network. The removal of nodes reduces the connectivity of the network and the degree of the remaining nodes.\n",
    "The probability that a node with initial degree $k_0$ reduces its degree to $k$ follows\n",
    "a binomial distribution,\n",
    "\n",
    "$$\n",
    "P(k \\vert k_0, p) = \\binom{k_0}{k} (1-p)^k p^{k_0-k}\n",
    "$$\n",
    "\n",
    "Considering all nodes, the new degree distribution is given by\n",
    "\n",
    "$$\n",
    "P'(k) = \\sum_{k_0 = k}^{\\infty} P(k_0) \\binom{k_0}{k} (1-p)^k p^{k_0-k}\n",
    "$$\n",
    "\n",
    "To connect with the Molloy-Reed criterion, we need to compute the first and second moments, denoted by $\\langle k \\rangle'$ and $\\langle k^2 \\rangle'$, of the new degree distribution.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle k \\rangle' &= \\sum_{k} k P'(k_0) \\\\\n",
    "&= \\sum_{k} \\sum_{k_0=k}^{\\infty} k \\binom{k_0}{k} (1-p)^k p^{k_0-k} P(k_0) \\\\\n",
    "&= \\sum_{k_0=0}^\\infty P(k_0) \\underbrace{\\sum_{k} k \\binom{k_0}{k} (1-p)^k p^{k_0-k}}_{\\text{Expected value of $k$ for a binomial distribution}} \\\\\n",
    "&= \\sum_{k_0=0}^\\infty P(k_0) k_0 (1-p) \\\\\n",
    "&= \\langle k_0 \\rangle (1-p)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Similarly, we can compute the second moment, which is given by\n",
    "\n",
    "$$\n",
    "\\langle k^2 \\rangle' = \\langle k_0^2 \\rangle (1-p)^2 + \\langle k_0 \\rangle p (1-p)\n",
    "$$\n",
    "\n",
    "By substituting these into the Molloy-Reed criterion, we get\n",
    "\n",
    "$$\n",
    "\\frac{\\langle k^2 \\rangle'}{\\langle k \\rangle'}  = \\frac{\\langle k_0^2 \\rangle (1-p)^2 + \\langle k_0 \\rangle p (1-p)}{\\langle k_0 \\rangle (1-p)} = \\frac{\\langle k_0^2 \\rangle (1-p) + \\langle k_0 \\rangle p}{\\langle k_0 \\rangle} > 2\n",
    "$$\n",
    "\n",
    "By solving the inequality for $p$, we get the percolation threshold for a random attack,\n",
    "\n",
    "$$\n",
    "1-p < \\frac{1}{\\langle k_0 ^2 \\rangle / \\langle k_0 \\rangle - 1}\n",
    "$$\n",
    "\n",
    "which is the condition for the existence of a giant component.\n",
    "# Building a cost-effective power grid network\n",
    "\n",
    "- ✍️ [Pen and Paper](./pen-and-paper/exercise.pdf)---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Minimum spanning tree\n",
    "\n",
    "Many networks are surprisingly sparse, with most nodes having only a few connections, in part because connections are costly.\n",
    "Like cables in power grids and human-to-human communications, networks are often built with cost constraints, and it is often of a great interest to find the most cost-effective structure.\n",
    "\n",
    "**Minimum spanning tree** is a tree that connects all the nodes in a network with the minimum total weight of edges.\n",
    "The term involves the following two concepts:\n",
    "\n",
    "- **Tree**: A network is a tree if it is connected and has no cycles.\n",
    "- **Spanning tree**: A spanning tree is a tree that spans all the nodes in a network.\n",
    "\n",
    "The minimum spanning tree may not be unique, meaning there can be multiple spanning trees with the same minimum total weight of edges for a network.\n",
    "\n",
    ":::{figure-md} minimum-spanning-tree\n",
    "<img src=\"../figs/minimum-spanning-tree.jpg\" alt=\"Minimum spanning tree\" width=\"80%\">\n",
    "\n",
    "Minimum spanning tree of a network.\n",
    ":::\n",
    "\n",
    "## How to find the minimum spanning tree\n",
    "\n",
    "**Kruskal's algorithm** and **Prim's algorithms** are two common methods to find a minimum spanning tree.\n",
    "Both start with an empty edge set and add the smallest weight edge iteratively, while ensuring that the edges form a tree, until all nodes are included.\n",
    "The difference between the two algorithms lies in the order of edge addition.\n",
    "\n",
    "**Kruskal's algorithm** operates as follows:\n",
    "1. Sort the edges by *the increasing order of the edge weights*.\n",
    "2. Select the edge with the smallest weight that does not form a cycle with the edges already in the tree.\n",
    "3. Repeat step 2 until all the nodes are connected.\n",
    "\n",
    "**Prim's algorithm**:\n",
    "1. Start with a singleton network $G$ consisting of a randomly chosen node.\n",
    "2. Add the smallest weight edge connecting $G$ to a node not in $G$.\n",
    "3. Repeat step 2 until all nodes are connected.\n",
    "\n",
    "{{ '[🚀 Check out the Demo for Kruskal\\'s and Prim\\'s algorithm 🌐]( BASE_URL/vis/kruskal-vs-prime.html)'.replace('BASE_URL', base_url) }}\n",
    "\n",
    "Kruskal's algorithm sorts the edges globally at the beginning, while Prim's algorithm sorts the edges locally at each step.\n",
    "Both algorithms find the same minimum spanning tree, provided that all edge weights are distinct.\n",
    "Otherwise, they may yield different trees.\n",
    "\n",
    "## Code\n",
    "\n",
    "`igraph` provides a function `igraph.Graph.spanning_tree` to find a minimum spanning tree in a given network.\n",
    "\n",
    "Let's first create a network with random edge weights.\n",
    "```{code-cell} ipython3\n",
    "import igraph\n",
    "import random\n",
    "\n",
    "g = igraph.Graph.Famous('Zachary')\n",
    "g.es[\"weight\"] = [random.randint(1, 10) for _ in g.es]\n",
    "igraph.plot(g, edge_width = g.es[\"weight\"])\n",
    "```\n",
    "\n",
    ":::{note}\n",
    "[Zachary's karate club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club) is a famous network of 34 members of a karate club and documents of their links between friends.\n",
    "The network is undirected and unweighted.\n",
    ":::\n",
    "\n",
    "The minimum spanning tree of the network can be found by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmst = g.spanning_tree(weights=g.es[\"weight\"]) # If not `weights` are not specified, the edges are assumed to be unweighted\n",
    "igraph.plot(gmst, edge_width = gmst.es[\"weight\"])\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Percolation\n",
    "\n",
    "Network robustness can be viewed as a inverse process of **percolation** on a network. What is percolation?\n",
    "Imagine a grid where each square has a chance to become a little puddle. Two puddles are connected if they are next to each other. As more puddles appear, they start connecting with their neighbors to form bigger puddles. This is basically what percolation is all about!\n",
    "Random failure can be viewed as an inverse process of percolation, where a puddle is dried up (i.e., removed from the network),\n",
    "\n",
    "```{figure-md} percolation\n",
    "\n",
    "![](https://jamesmccaffrey.wordpress.com/wp-content/uploads/2021/07/percolation.jpg?w=584&h=389)\n",
    "\n",
    "Image taken from https://jamesmccaffrey.wordpress.com/2021/07/12/whatever-happened-to-percolation-theory/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8ef78",
   "metadata": {},
   "source": [
    "Now, the big question is: When the probability of a node being puddle is $p$, how big can our largest puddle get? 🌊\n",
    "As we increase the chance of puddles appearing (that's our $p$), the biggest puddle does not grow slowly but explodes in size when $p$ reaches a critical value $p_c$. This sudden change is what we call a *phase transition*! From the percolation perspective, we approach to the critical point from disconnected phase, whereas from the network robustness perspective, we approach to the critical point from connected phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e64e54",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def percolate(g, p):\n",
    "    return g.subgraph(np.where(np.random.rand(g.vcount()) < p)[0])\n",
    "\n",
    "\n",
    "def largest_cluster_size(g):\n",
    "    return g.connected_components().giant().vcount()\n",
    "\n",
    "\n",
    "n, nei = 500, 1\n",
    "g = ig.Graph.Lattice([n, n], nei=nei, directed=False, mutual=False, circular=False)\n",
    "\n",
    "p_values = np.linspace(0, 1, 20)\n",
    "largest_sizes = [largest_cluster_size(percolate(g, p)) / n**2 for p in p_values]\n",
    "\n",
    "sns.set(style=\"ticks\", font_scale=1.2)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.lineplot(x=p_values, y=largest_sizes, ax=ax, marker=\"o\")\n",
    "ax.set(\n",
    "    xlabel=\"Probability (p)\",\n",
    "    ylabel=\"Fractional Largest Cluster Size\",\n",
    "    title=\"Percolation on a 500x500 Lattice\",\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "critical_p = 0.592746  # Critical probability for 2D square lattice\n",
    "colors = sns.color_palette(\"colorblind\", 3)\n",
    "ax.axvline(x=critical_p, color=\"k\", linestyle=\"--\", alpha=0.7)\n",
    "ax.fill_betweenx(\n",
    "    y=[0, 1], x1=0, x2=critical_p, alpha=0.2, color=colors[0], label=\"Disconnected\"\n",
    ")\n",
    "ax.fill_betweenx(\n",
    "    y=[0, 1], x1=critical_p, x2=1, alpha=0.2, color=colors[1], label=\"Connected\"\n",
    ")\n",
    "ax.legend().remove()\n",
    "\n",
    "ax.annotate(\n",
    "    \"Disconnected\",\n",
    "    xy=(0.3, 0.5),\n",
    "    xytext=(0.4, 0.1),\n",
    "    textcoords=\"data\",\n",
    "    horizontalalignment=\"right\",\n",
    "    verticalalignment=\"center\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.annotate(\n",
    "    \"Connected\",\n",
    "    xy=(0.9, 0.5),\n",
    "    xytext=(0.7, 0.1),\n",
    "    textcoords=\"data\",\n",
    "    horizontalalignment=\"left\",\n",
    "    verticalalignment=\"center\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlim(0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e862dbc",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "Want to see this in action? 🌟 Check out this interactive simulation.\n",
    "Play around with it and watch how the puddles grow and connect. 🌊\n",
    "\n",
    "[Bernoulli Percolation Simulation 🌐](https://visualize-it.github.io/bernoulli_percolation/simulation.html) 🔗\n",
    "\n",
    "```\n",
    "\n",
    "```{note}\n",
    "\n",
    "The transition at $p_c$ is discontinuous in the limit of large $N$, called *first-order phase transition*.\n",
    "In practice, it is often a continuous transition because of the finite size of the network.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## A criterion for the giant component\n",
    "\n",
    "Percolation theory focuses on lattice, a regular structure that is rare in real-world networks. What happens if the network has a complex structure?\n",
    "The Molloy-Reed criterion {footcite}`molloy1995critical` provides a simple condition for the existence of a giant component in a rewired network. It states that a giant component is likely to exist if:\n",
    "\n",
    "$$\n",
    "\\kappa_0 := \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} > 2\n",
    "$$\n",
    "\n",
    "where $k$ is the degree of a node, and $\\langle k \\rangle$ and $\\langle k^2 \\rangle$ are the average degree and the average of the square of the degree, respectively. The variable $\\kappa_0$ is a shorthand for the ratio. See the [Appendix](./appendix.md) for the derivation of this criterion.\n",
    "\n",
    "What does $\\kappa_0$ represent? It represents the heterogeneity of the degree distribution. For example, a high $\\kappa_0$ indicates that there are a few nodes with very high degrees and many nodes with low degrees. When $\\kappa_0$ is small, the nodes have similar degree. And Molloy-Reed criterion tells us an important fact about the role of degree distributions on the robustness of networks:\n",
    "the more heterogeneous the degree distribution is, the more likely the network is to have a giant component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ddd82",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import uniform, poisson, lognorm\n",
    "# Set Seaborn style\n",
    "sns.set_style('white')\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Set up the plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Degree distributions with different $\\\\kappa_0$ values with the same mean $\\\\langle k \\\\rangle=5$')\n",
    "\n",
    "# Set the average degree\n",
    "avg_k = 5\n",
    "samples = 3000\n",
    "\n",
    "# Uniform distribution\n",
    "k_uniform = np.array([avg_k] * samples)\n",
    "\n",
    "# Poisson distribution\n",
    "k_poisson = np.random.poisson(avg_k, size=samples)\n",
    "\n",
    "# Log-normal distribution\n",
    "k_lognorm = np.random.lognormal(mean=0, sigma=1, size=samples)\n",
    "k_lognorm = k_lognorm * avg_k / np.mean(k_lognorm)\n",
    "\n",
    "\n",
    "# Plot histograms\n",
    "sns.histplot(k_uniform, ax=axs[0], kde=True, stat=\"density\", discrete=True)\n",
    "axs[0].set_title(f'Uniform\\n$\\\\kappa_0 = {np.mean(k_uniform**2) / np.mean(k_uniform):.2f}$')\n",
    "axs[0].set_xlim(0, 10)\n",
    "\n",
    "sns.histplot(k_poisson, ax=axs[1], kde=True, stat=\"density\", discrete=True)\n",
    "axs[1].set_title(f'Poisson\\n$\\\\kappa_0 = {np.mean(k_poisson**2) / np.mean(k_poisson):.2f}$')\n",
    "\n",
    "sns.histplot(k_lognorm, ax=axs[2], kde=True, stat=\"density\", bins=20)\n",
    "axs[2].set_title(f'Log-normal\\n$\\\\kappa_0 = {np.mean(k_lognorm**2) / np.mean(k_lognorm):.2f}$')\n",
    "\n",
    "# Set labels and adjust layout\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Degree (k)')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71783a32",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The rewired network considered here is **the configuration model**, where the edges are rewired randomly while keeping the degree distribution fixed. We will discuss more about the configuration model later.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "Consider a random network of $N$ nodes, where every pair of nodes are connected by an edge with a certain probability.\n",
    "Then, the degree $k$ of a node is a binomial random variable, which we approximate by a Poisson random variable with mean $\\langle k \\rangle$. The variance of the Poisson random variable is also $\\langle k \\rangle$.\n",
    "\n",
    "1. Derive $\\langle k^2 \\rangle$ using $\\langle k \\rangle$.\n",
    "  - Hint: Variance is defined as $\\text{Var}(k) = \\langle (k-\\langle k \\rangle)^2 \\rangle$.\n",
    "2. Compute the ratio $\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\n",
    "3. Check when the network satisfies the Molloy-Reed criterion.\n",
    "\n",
    "```{dropdown} Solution\n",
    "\n",
    "**Solution for Q1**:\n",
    "To derive $\\langle k^2 \\rangle$, we start with the definition of variance\n",
    "\n",
    "$$\\text{Var}(k) = \\langle (k - \\langle k \\rangle)^2 \\rangle$$\n",
    "\n",
    "Expanding the square, we get\n",
    "\n",
    "$$\\text{Var}(k) = \\langle k^2 \\rangle - 2\\langle k \\rangle \\langle k \\rangle + \\langle k \\rangle^2$$\n",
    "\n",
    "Since $\\text{Var}(k) = \\langle k \\rangle$ for a Poisson distribution, we can substitute and rearrange\n",
    "\n",
    "$$\\langle k \\rangle = \\langle k^2 \\rangle - \\langle k \\rangle^2$$\n",
    "\n",
    "Solving for $\\langle k^2 \\rangle$, we obtain\n",
    "\n",
    "$$\\langle k^2 \\rangle = \\langle k \\rangle + \\langle k \\rangle^2$$\n",
    "\n",
    "**Solution for Q2**:\n",
    "$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} = 1 + \\langle k \\rangle$\n",
    "\n",
    "**Solution for Q3**:\n",
    "$\\langle k \\rangle >1$. In other words, if a node has on average more than one neighbor, the random network is likely to have a giant component.\n",
    "```\n",
    "\n",
    "## How many nodes are needed to break a network?\n",
    "\n",
    "When does a network become disconnected? Based on the Molloy-Reed criterion, we can identify the critical fraction of nodes $f_c$ that need to be removed for the giant component to disappear in a network with an arbitrary degree distribution. This critical point is given by {footcite}`cohen2000resilience`:\n",
    "\n",
    "$$\n",
    "f_c = 1 - \\frac{1}{\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} - 1}\n",
    "$$\n",
    "\n",
    "See the [Appendix](./appendix.md) for the derivation of this criterion.\n",
    "\n",
    "Let us illustrate this by considering two kinds of networks:\n",
    "\n",
    "**Degree homogeneous network**:\n",
    "\n",
    "In case of a degree homogeneous network like a random network considered in the exercise above,\n",
    "\n",
    "$$\n",
    "f_c = 1 - \\frac{1}{\\langle k \\rangle}\n",
    "$$\n",
    "\n",
    "This suggests that the threshold is determined by the average degree $\\langle k \\rangle$. A large $\\langle k \\rangle$ results in a larger $f_c$, meaning that the network is more robust against random failures.\n",
    "\n",
    "**Degree heterogeneous network**:\n",
    "\n",
    "Most real-world networks are degree heterogeneous, i.e., the degree distribution $P(k) \\sim k^{-\\gamma}$ follows a power law (called *scale-free* network).\n",
    "In this case, $f_c$ is given by\n",
    "\n",
    "$$\n",
    "f_c =\n",
    "\\begin{cases}\n",
    "1 - \\dfrac{1}{\\frac{\\gamma-2}{3-\\gamma} k_{\\text{min}} ^{\\gamma-2} k_{\\text{max}}^{3-\\gamma} -1} & \\text{if } 2 < \\gamma < 3 \\\\\n",
    "1 - \\dfrac{1}{\\frac{\\gamma-2}{3-\\gamma} k_{\\text{min}} - 1} & \\text{if } \\gamma > 3 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where $k_{\\text{min}}$ and $k_{\\text{max}}$ are the minimum and maximum degree, respectively.\n",
    "The variable $\\gamma$ is the exponent of the power law degree distribution, controlling the degree heterogeneity, where a lower $\\gamma$ results in a more degree heterogeneous network.\n",
    "\n",
    "- For regime $2 < \\gamma < 3$, the critical threshold $f_c$ is determined by the extreme values of the degree distribution, $k_{\\text{min}}$ and $k_{\\text{max}}$.\n",
    "And $f_c \\rightarrow 1$ when the maximum degree $k_{\\text{max}} \\in [k_{\\text{min}}, N-1]$ increases.\n",
    "Notably, in this regime, the maximum degree $k_{\\text{max}}$ increases as the network size $N$ increases, and this makes $f_c \\rightarrow 1$.\n",
    "\n",
    "- For regime $\\gamma > 3$, the critical threshold $f_c$ is influenced by the minimum degree $k_{\\text{min}}$. In contrast to $k_{\\text{max}}$, $k_{\\text{min}}$ remains constant as the network size $N$ grows. Consequently, the network disintegrates when a finite fraction of its nodes are removed.\n",
    "\n",
    "## Case study: Airport network\n",
    "\n",
    "Let's consider an empirical network of international airports, where nodes are airports and edges denote a regular commercial flight between two airports.\n",
    "\n",
    "\n",
    "Data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716ad9f",
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the airport network data from a CSV file\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/skojaku/core-periphery-detection/master/data/edge-table-airport.csv\")\n",
    "\n",
    "# Process the edge data\n",
    "edges = df[[\"source\", \"target\"]].to_numpy()\n",
    "edges = np.unique(edges.reshape(-1), return_inverse=True)[1]\n",
    "edges = edges.reshape(-1, 2)\n",
    "\n",
    "# Create the original graph\n",
    "g = ig.Graph()\n",
    "g.add_vertices(np.unique(edges) + 1)\n",
    "g.add_edges([tuple(edge) for edge in edges])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8282a77",
   "metadata": {},
   "source": [
    "Based on the argument above, we can predict the critical point $f_c$ for the airport network as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "g # igraph object of the airport network\n",
    "\n",
    "# Compute the degree distribution\n",
    "deg = np.array(g.degree())\n",
    "\n",
    "k_ave = np.mean(deg)\n",
    "k_2 = np.mean(deg **2)\n",
    "\n",
    "# Compute the critical fraction of nodes that need to be removed (prediction)\n",
    "f_c = 1 - 1 / (k_2 / k_ave - 1)\n",
    "print(f\"The critical fraction of nodes that need to be removed is predicted to be {f_c:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366aff9c",
   "metadata": {},
   "source": [
    "The $f_c$ is very close to 1, meaning that the network is highly robust to random failures that it keeps the giant component until when almost all nodes are removed.\n",
    "Let us confirm this by simulating the random failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44085a1",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original graph for manipulation\n",
    "g_damaged = g.copy()\n",
    "n_nodes = g.vcount()  # Number of nodes in the graph\n",
    "\n",
    "# Initialize list to store results\n",
    "results = []\n",
    "\n",
    "# Simulate random node removal and measure network connectivity\n",
    "for i in range(n_nodes - 1):  # Loop until only one node remains\n",
    "\n",
    "    # Randomly select and remove a node\n",
    "    node_idx = np.random.choice(g_damaged.vs.indices)\n",
    "    g_damaged.delete_vertices(node_idx)\n",
    "\n",
    "    # Evaluate the connectivity of the remaining network\n",
    "    components = g_damaged.connected_components()\n",
    "    connectivity = np.max(components.sizes()) / g.vcount()\n",
    "\n",
    "    # Save the results\n",
    "    results.append(\n",
    "        {\n",
    "            \"connectivity\": connectivity,\n",
    "            \"frac_nodes_removed\": (i + 1) / n_nodes,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_robustness_profile = pd.DataFrame(results)\n",
    "\n",
    "# Set up the plot style\n",
    "sns.set(style='white', font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Create the plot\n",
    "ax = df_robustness_profile.plot(\n",
    "    x=\"frac_nodes_removed\",\n",
    "    y=\"connectivity\",\n",
    "    kind=\"line\",\n",
    "    figsize=(5, 5),\n",
    "    label=\"Random attack\",\n",
    "    linewidth=2,\n",
    "    color = sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "# Set labels for x and y axes\n",
    "plt.xlabel(\"Proportion of nodes removed\")\n",
    "plt.ylabel(\"Fractional size of largest component\")\n",
    "\n",
    "# Remove the legend\n",
    "plt.legend().remove()\n",
    "\n",
    "# Add a diagonal line from top left to bottom right\n",
    "ax.plot([0, 1], [1, 0], color='gray', linestyle='--')\n",
    "\n",
    "# Adjust the plot limits to ensure the diagonal line is visible\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add a vertical line at the critical fraction\n",
    "ax.axvline(x=f_c, color='red', linestyle='--', alpha=0.7, label=\"Critical fraction\")\n",
    "\n",
    "# Remove top and right spines of the plot\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164bd4b",
   "metadata": {},
   "source": [
    "The robustness profile of the airport network shows a very robust nature of the network, i.e., the airport network keeps the giant component until almost all nodes are removed.\n",
    "\n",
    "\n",
    "## Targeted attacks\n",
    "\n",
    "A key implication of the random failures is that a hub plays a critical role in holding the network together. This also implies a vulnerability of the network to targeted attacks. Namely, if we remove the hub preferentially, the network can be quickly disconnected into small components.\n",
    "\n",
    "One can consider a targeted attack as a process of reducing the degree of nodes in a network. The degree-based attack, for example, reduces the maximum degree of the network, together with the degrees of neighboring nodes.\n",
    "An effective attack is one that quickly breaks the Molloy-Reed criterion, and from this perspective, the degree-based attack is not effective because it reduces the maximum degree of the network, a major contributor to the degree heterogeneity, $\\kappa_0$.\n",
    "\n",
    "\n",
    "## How to design a robust network?\n",
    "\n",
    "Based on the percolation theory, how we do we design a network that is robust against random failures and targeted attacks? Two key ingredients are:\n",
    "\n",
    "1. **Degree heterogeneity**: As we have seen in the percolation theory, the more heterogeneous the degree distribution is, the more likely the network is to have a giant component.\n",
    "\n",
    "2. **Resilience to hub removal**: A network is vulnerable to targeted attacks if the removal of a single node significantly decreases the heterogeneity of the degree distribution. The most susceptible structure is a star graph, where a central node connects to all other nodes, as removing this central node will disconnect the network.\n",
    "\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "What is the design strategy to make a network robust against targeted attacks?\n",
    "Design a network that is robust against both random failures and targeted attacks.\n",
    "\n",
    "{{ '[🚀 Interactive Demo]( BASE_URL/vis/network-robustness.html)'.replace('BASE_URL', base_url) }}\n",
    "\n",
    "```{dropdown} An answer\n",
    "\n",
    "A bimodal degree distribution can enhance network robustness against both random failures and targeted attacks.\n",
    "In this setup, $(1-r)$ portion of nodes have a degree of 1, while $r$ portion of nodes have a high degree, $k_{\\text{max}}$.\n",
    "This structure ensures that the network remains connected even if a hub is removed, as other hubs maintain the connectivity. It also withstands random failures due to its heterogeneous degree distribution.\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m03-robustness.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "```{code-cell} ipython3\n",
    "# If you are using Google Colab, uncomment the following line to install igraph\n",
    "# !sudo apt install libcairo2-dev pkg-config python3-dev\n",
    "# !pip install pycairo cairocffi\n",
    "# !pip install igraph\n",
    "```\n",
    "\n",
    "# Hands-on: Robustness (Random attack)\n",
    "\n",
    "We consider a small social network of 34 members in a university karate club, called Zachary's karate club network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "g = igraph.Graph.Famous(\"Zachary\")\n",
    "igraph.plot(g, vertex_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c261264",
   "metadata": {},
   "source": [
    "Let's break the network 😈!\n",
    "We will remove nodes one by one and see how the connectivity of the network changes at each step.\n",
    "It is useful to create a copy of the network to keep the original network unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_original = g.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c28db6",
   "metadata": {},
   "source": [
    "## Robustness against random failures\n",
    "\n",
    "Let us remove a single node from the network. To this end, we need to first identify which nodes are in the network. With `igraph`, the IDs of the nodes in a graph are accessible through `Graph.vs.indices` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.vs.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939f641",
   "metadata": {},
   "source": [
    "We randomly choose a node and remove it from the network by using `Graph.delete_vertices`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3898cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "node_idx = np.random.choice(g.vs.indices)\n",
    "g.delete_vertices(node_idx)\n",
    "print(\"Node removed:\", node_idx)\n",
    "print(\"Nodes remaining:\", g.vs.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e05522",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "`np.random.choice(array)` takes an array `array` and returns a single element from the array.\n",
    "For example, `np.random.choice(np.array([1, 2, 3]))` returns either 1, 2, or 3 with equal probability.\n",
    "See [the documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) for more details.\n",
    ":::\n",
    "\n",
    "The connectivity of the network is the fraction of nodes in the largest connected component of the network after node removal.\n",
    "We can get the connected components of the network by using `Graph.connected_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = g.connected_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c875c0",
   "metadata": {},
   "source": [
    "The sizes of the connected components are accessible via `Graph.connected_components.sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06345b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "components.sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a029d66",
   "metadata": {},
   "source": [
    "Thus, the connectivity of the network can be computed by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = g.connected_components()\n",
    "connectivity = np.max(components.sizes()) / g_original.vcount()\n",
    "connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7d184",
   "metadata": {},
   "source": [
    "Putting together the above code, let us compute the robustness profile of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcf8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "g = g_original.copy() # restore the network\n",
    "n_nodes = g.vcount()  # Number of nodes\n",
    "\n",
    "results = []\n",
    "for i in range(n_nodes -1):  # Loop if the network has at least one node\n",
    "\n",
    "    # Remove a randomly selected node\n",
    "    node_idx = np.random.choice(g.vs.indices)\n",
    "    g.delete_vertices(node_idx)\n",
    "\n",
    "    # Evaluate the connectivity\n",
    "    components = g.connected_components()\n",
    "    connectivity = np.max(components.sizes()) / g_original.vcount()\n",
    "\n",
    "    # Save the results\n",
    "    results.append(\n",
    "        {\n",
    "            \"connectivity\": connectivity,\n",
    "            \"frac_nodes_removed\": (i + 1) / n_nodes,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_robustness_profile = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7550ebd",
   "metadata": {},
   "source": [
    "Let us plot the robustness profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='white', font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "ax = df_robustness_profile.plot(\n",
    "    x=\"frac_nodes_removed\",\n",
    "    y=\"connectivity\",\n",
    "    kind=\"line\",\n",
    "    figsize=(5, 5),\n",
    "    label=\"Random attack\",\n",
    ")\n",
    "plt.xlabel(\"Proportion of nodes removed\")\n",
    "plt.ylabel(\"Connectivity\")\n",
    "plt.legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe4454",
   "metadata": {},
   "source": [
    "## Targeted attack\n",
    "\n",
    "In a targeted attack, nodes are removed based on specific criteria rather than randomly.\n",
    "One common strategy is to remove nodes from the largest node degree to the smallest, based on the idea that removing nodes with many edges is more likely to disrupt the network connectivity.\n",
    "\n",
    "The degree of the nodes is accessible via `Graph.degree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_original.degree())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbb767",
   "metadata": {},
   "source": [
    "We compute the robustness profile by removing nodes with the largest degree and measuring the connectivity of the network after each removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g_original.copy() # restore the network\n",
    "n_nodes = g.vcount()  # Number of nodes\n",
    "\n",
    "results = []\n",
    "for i in range(n_nodes -1):  # Loop if the network has at least one node\n",
    "\n",
    "    # Remove the nodes with thelargest degree\n",
    "    node_idx = g.vs.indices[np.argmax(g.degree())]\n",
    "    g.delete_vertices(node_idx)\n",
    "\n",
    "    # Evaluate the connectivity\n",
    "    components = g.connected_components()\n",
    "    connectivity = np.max(components.sizes()) / g_original.vcount()\n",
    "\n",
    "    # Save the results\n",
    "    results.append(\n",
    "        {\n",
    "            \"connectivity\": connectivity,\n",
    "            \"frac_nodes_removed\": (i + 1) / n_nodes,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_robustness_profile_targeted = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa78795",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='white', font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "sns.set(style=\"white\", font_scale=1.2)\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "ax = df_robustness_profile.plot(\n",
    "    x=\"frac_nodes_removed\",\n",
    "    y=\"connectivity\",\n",
    "    kind=\"line\",\n",
    "    figsize=(5, 5),\n",
    "    label=\"Random attack\",\n",
    ")\n",
    "ax = df_robustness_profile_targeted.plot(\n",
    "    x=\"frac_nodes_removed\",\n",
    "    y=\"connectivity\",\n",
    "    kind=\"line\",\n",
    "    label=\"Targeted attack\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"Proportion of nodes removed\")\n",
    "ax.set_ylabel(\"Connectivity\")\n",
    "ax.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009910b",
   "metadata": {},
   "source": [
    "While the network is robust against the random attacks, it is vulnerable to the degree-based targeted attack.---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Network Robustness\n",
    "\n",
    "Nodes and edges can fail or be attacked, which disrupt the connectivity of a network.\n",
    "Roughly speaking, we say a network is robust if it maintain most of its connectivity after failures or attacks.\n",
    "There are different types of attacks, together with how we quantify the damage they cause. So let us first showcase a case study with code.\n",
    "\n",
    "## Random node failures\n",
    "\n",
    "Nodes can fail and disconnect from networks, such as power station closures in power grids. This is modeled as a **random failure**, where randomly chosen nodes are removed from the network. When a node fails, it and its edges are removed.\n",
    "\n",
    "The damage varies depending on the node to be removed. The damage to the network can be measued in many different ways, but an accepted measure is the loss of **connectivity**, defined as the fraction of nodes left in the largest connected part of the network after the failure.\n",
    "\n",
    ":::{figure-md} single-node-failure\n",
    "<img src=\"../figs/single-node-failure.jpg\" alt=\"Single node failure\" width=\"70%\">\n",
    "\n",
    "The impact of removing a single node varies based on which node is removed.\n",
    ":::\n",
    "\n",
    "Multiple nodes can fail simultaneously, e.g., due to natural disasters like earthquakes or tsunamis.\n",
    "Thus it is often useful to assess the robustness of the network against such failures.\n",
    "**Robustness profile** is a plot of the connectivity drop as a function of the number of nodes removed. It provides a visual summary of the robustness of the network against *a given sequential failure of nodes*.\n",
    "In random failure, the order of nodes removed is random.\n",
    "\n",
    ":::{figure-md} multiple-node-failure\n",
    "<img src=\"../figs/robustness-profile.jpg\" alt=\"Multiple node failure\" width=\"70%\">\n",
    "\n",
    "Robustness profile of a network for a sequential failure of nodes.\n",
    ":::\n",
    "\n",
    "Beyond the qualitative observation, it is useful to quantify the robustness of the network.\n",
    "The **$R$-index** is a single number that summarizes the robustness of the network.\n",
    "It is defined as the area under the connectivity curve with integral approximation.\n",
    "\n",
    "$$\n",
    "R = \\frac{1}{N} \\sum_{k=1}^{N-1} y_k\n",
    "$$\n",
    "\n",
    "where $y_k$ is the connectivity at fraction $k/N$ of nodes removed, where $N$ is the total number of nodes in the network. A higher value indicates that the network is robust against the attack. The $R$-index has a maximum value of 1/2 (i.e., which corresponds to a diagonal line in the plot above).\n",
    "\n",
    "\n",
    "## Targeted attack\n",
    "\n",
    "A network robust against random failures can still be fragmented by **targeted attacks**.\n",
    "In targeted attacks, nodes are removed based on specific criteria rather than randomly.\n",
    "For example, nodes can be removed in order of their degree, starting with the largest degree to the smallest degree. The rationale for this attack strategy is that large-degree nodes have many connections, so removing them disrupts the network more significantly.\n",
    "\n",
    "Degree-based attack is not the only form of targeted attacks. Other forms of targeted attacks include removing nodes based on their centrality (closeness centrality, betweenness centrality) and those based on proximity.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "In the next section, we will code up a simple example to compute the robustness profile of a network using Python.\n",
    "# Module 3: Robustness\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn about network robustness. We will learn:\n",
    "- Minimum spanning tree\n",
    "- Network robustness against random and targeted attacks\n",
    "- **Keywords**: minimum spanning tree, Kruskal’s algorithm, Prim's algorithm, random attacks, targeted attacks, network robustness, robustness index---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m04-friendship-paradox.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Degree distribution\n",
    "\n",
    "![](https://barabasi.com/img/6/159.png)\n",
    "\n",
    "Understanding degree distribution is the first key step to understand networks! And often, we want to see how the degree distribution looks like by plotting it like using histogram. But, it is not as easy as it may seem...\n",
    "\n",
    "## Visualization basics\n",
    "\n",
    "To learn the basics of data visualization, please take a [pen and paper exercise](./pen-and-paper/exercise.pdf).\n",
    "\n",
    "## Coding exercise\n",
    "\n",
    "[Exercise: Plotting degree distribution](https://github.com/skojaku/adv-net-sci/blob/main/notebooks/exercise-m04-friendship-paradox.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "### Plotting degree distribution\n",
    "\n",
    "(The following content includes the answer to the exercise. So please do the exercise first before reading the following content.)\n",
    "\n",
    "We will first introduce a formal definition of the degree distribution. Then, we will learn how to plot the degree distribution of a network.\n",
    "\n",
    "The degree of a node $i$, denoted by $d_i$, is the number of edges connected to it. With the adjacency matrix $A$, the degree of node $i$ is given by:\n",
    "\n",
    "$$\n",
    "k_i = \\sum_{j=1}^N A_{ij}.\n",
    "$$\n",
    "\n",
    "Let us compute the degree distribution of a network. We will create a Barabási-Albert network with $N=10,000$ nodes and $m=1$ edge per node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984dbc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "g = igraph.Graph.Barabasi(n = 10000, m = 1) # Create a Barabási-Albert network\n",
    "A = g.get_adjacency() # Get the adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8482152",
   "metadata": {},
   "source": [
    "Compute the degree of each node by summing the elements of the adjacency matrix along the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "deg = np.sum(A, axis=1)\n",
    "deg = deg.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc31d5e",
   "metadata": {},
   "source": [
    "The degree distribution $p(k)$ can be computed by counting the number of nodes with each degree and dividing by the total number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26053b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_deg = np.bincount(deg) / len(deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9e5d51",
   "metadata": {},
   "source": [
    "Let us plot the degree distribution. This is not as trivial as you might think... 🤔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bda659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.lineplot(x=np.arange(len(p_deg)), y=p_deg)\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba4120",
   "metadata": {},
   "source": [
    "While it clearly shows that most nodes have small degree, it does not show the tail of the distribution clearly, and often it is this tail that is of great interest (e.g., hub nodes). To show the tail of the distribution more clearly, we can use a log-log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5768bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(x=np.arange(len(p_deg)), y=p_deg)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(np.min(p_deg[p_deg>0])*0.01, None)\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c57e04f",
   "metadata": {},
   "source": [
    "We see fluctuations for large degree nodes because of the small number of nodes with large degree.\n",
    "One can use \"binning\" to smooth the plot. Binning involves grouping the data into bins and calculating the fraction of data within each bin. However, selecting an appropriate bin size can be challenging, and even with a well-chosen bin size, some information may be lost.\n",
    "\n",
    "A more convenient way is to use the complementary cumulative distribution function (CCDF).\n",
    "The CCDF at degree $k$ is the probability that a randomly chosen node has degree $k'$ greater than $k$ ($k' > k$).  For a visual comparison of CCDF and PDF, see Figure 3 in {footcite}`newman2005power` or [the arxiv version](https://arxiv.org/pdf/cond-mat/0412004)\n",
    "\n",
    "$$\n",
    "\\text{CCDF}(k) = P(k' > k) = \\sum_{k'=k+1}^\\infty p(k')\n",
    "$$\n",
    "\n",
    "- CCDF is a monotonically decreasing function of $k$.\n",
    "- CCDF encompasses the full information of $p(k)$, i.e., taking the derivative of CCDF gives $p(k)$.\n",
    "- CCDF can be plotted as a smooth curve on a log-log scale without binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdf_deg = 1 - np.cumsum(p_deg)[:-1] # 1 - CDF (cumulative distribution function).\n",
    "# The last element is excluded because it is always 1, resulting in CCDF=0, which cannot be plotted on a log-log scale.\n",
    "\n",
    "ax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('CCDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6a0f30",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "\n",
    "cdf_deg = np.cumsum(p_deg)\n",
    "fig, ax = plt.subplots(figsize=(3,3))\n",
    "ax = sns.lineplot(x=np.arange(len(cdf_deg)), y=cdf_deg, ax = ax)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('CDF')\n",
    "glue(\"cdf_fig\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49674b6",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "CCDF (complementary cumulative distribution function) is used instead of CDF (cumulative distribution function) because it highlights the tail of the distribution better in a log-log plot. A log scale expands small values and compresses large values. In a CDF, large degree nodes have values close to 1, compressing the tail. In a CCDF, large degree nodes have small values, making the tail more visible.\n",
    "```{glue} cdf_fig\n",
    ":align: center\n",
    "```\n",
    ":::\n",
    "\n",
    "The slope of the CCDF tells us the heterogeneity of the degree distribution.\n",
    "- Steep slope: more **homogeneous** degree distribution (similar degrees)\n",
    "- Flat slope: more **heterogeneous** degree distribution (wide range of degrees)\n",
    "\n",
    "\n",
    "The slope of the CCDF is related to the power-law exponent of the degree distribution.\n",
    "A power-law degree distribution is described by *a continuous distribution* with the *density function* (not the probability mass) $p(d)$ given by {footcite}`clauset2009power`:\n",
    "\n",
    "$$\n",
    "p(k) = \\frac{\\gamma-1}{k_{\\min}} \\left( \\frac{k}{k_{\\min}} \\right)^{-\\gamma}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $p(k)$ is the probability *density* of a node having degree $k$\n",
    "- $\\gamma$ is the power-law exponent\n",
    "- $k_{\\min}$ is the minimum degree\n",
    "\n",
    "\n",
    ":::{note}\n",
    "The degree distribution is *discrete* but often approximated by a *continuous* distribution for mathematical convenience. While generally accurate, caution is needed as the reliability varies depending on the range of the degrees. See {footcite}`clauset2009power` for more details.\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "The power-law distribution is ill-defined for $d=0$, which is why there must be a minimum degree $d_{\\min}$ to avoid this issue.\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "There has been a long-standing debate in network science as to whether the power-law well represents the real-world networks. Power-law is just one of many possible distributions with a heavy tail (i.e., a long tail on the right side of the distribution), and other distributions may also fit the data well such as log-normal distribution.\n",
    "This discussion is critical as many theories in network science are built on the assumption of the form of the degree distribution. See {footcite}`artico2020rare,holme2019rare,voitalov2019scale,barabasi2003scale` for the debate.\n",
    ":::\n",
    "\n",
    "The CCDF for the power-law distribution is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{CCDF}(k) &= 1 - \\int_{k_{\\min}}^k p(x) {\\rm d}x \\\\\n",
    "  &= 1 - \\frac{\\gamma -1}{k_{\\min}}\\cdot \\frac{1}{1 - \\gamma} \\left[\n",
    "\\left(\\frac{k^{-\\gamma + 1}}{k_{\\min}^{-\\gamma}}\\right) - \\left(\\frac{k_{\\min} ^{-\\gamma + 1}}{k_{\\min} ^{-\n",
    "\\gamma}}\\right)\\right] \\\\\n",
    "&= \\left( \\frac{k}{k_{\\min}}\\right)^{-\\gamma + 1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Taking the logarithm:\n",
    "\n",
    "$$\n",
    "\\log \\left[ \\text{CCDF}(k) \\right] = (-\\gamma + 1) \\cdot \\log k + \\text{const.}\n",
    "$$\n",
    "\n",
    "Thus, the slope of the CCDF in a log-log plot is related to the power-law exponent $\\gamma$.\n",
    "Specifically, a steeper negative slope (i.e., a more negative value of $-\\gamma + 1$) corresponds to a larger $\\gamma$.\n",
    "A larger $\\gamma$ indicates a more homogeneous degree distribution, where the probability of finding nodes with very high degrees decreases more rapidly.\n",
    "Conversely, a flatter slope (i.e., a value of $-\\gamma + 1$ being closer to zero) corresponds to a smaller $\\gamma$.\n",
    "A smaller $\\gamma$ indicates a more heterogeneous degree distribution, where there's a high probability of finding nodes with high degrees compared to that with a large $\\gamma$ value.\n",
    "\n",
    "For students interested in real-world examples of the CCDF plot, refer to Figure 4 in {footcite}`newman2005power`, or [the arxiv version](https://arxiv.org/pdf/cond-mat/0412004)\n",
    "\n",
    "In sum, the CCDF in a log-log plot provides a convenient visual summary of the degree distribution, with the slope of the CCDF providing a measure of the heterogeneity of the degree distribution.\n",
    "\n",
    "\n",
    "## Degree distribution of a friend\n",
    "\n",
    "Continuing from the previous page, we will now consider the degree distribution of a friend of a node.\n",
    "\n",
    "There are two ways to sample a friend of a node.\n",
    "1. Sample a node uniformly at random and then sample a friend of the node.\n",
    "2. Sample a *friendship* (i.e., edge) uniformly at random and then sample an end node of the edge.\n",
    "\n",
    "Let us focus on the second case and leave the first case for interested students as an exercise.\n",
    "In the second case, we sample an edge from the network.\n",
    "This sampling is biased towards nodes with many edges, i.e., a person with $d$ edges is $d$ times more likely to be sampled than someone with 1 edge.\n",
    "Thus, the degree distribution $p'(k)$ of a friend is given by\n",
    "\n",
    "$$\n",
    "p' (k) = C \\cdot k \\cdot p(k)\n",
    "$$\n",
    "The additional term $k$ reflects the fact that a person with $k$ friends is $k$ times more likely to be sampled than someone with 1 friend.\n",
    "Term $C$ is the normalization constant that ensures the sum of probabilities $p'(k)$ over all $k$ is 1, which can be easily computed as follows:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{\\sum_{k} k \\cdot p(k)} = \\frac{1}{\\langle k \\rangle}\n",
    "$$\n",
    "\n",
    "where $\\langle k \\rangle$ is the average degree of the network. Substituting $C$ into $p'(k)$, we get:\n",
    "\n",
    "$$\n",
    "p' (k) = \\frac{k}{\\langle k \\rangle} p(k)\n",
    "$$\n",
    "\n",
    "This is the degree distribution of a friend, and it is easy to verify that the average degree of a friend is given by\n",
    "\n",
    "$$\n",
    "\\langle k' \\rangle = \\sum_{k} k \\cdot p'(k) = \\sum_{k} k \\cdot \\frac{k}{\\langle k \\rangle} p(k) = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}\n",
    "$$\n",
    "\n",
    "which is always larger than $\\langle k \\rangle$:\n",
    "\n",
    "$$\n",
    "\\langle k' \\rangle = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} \\geq \\langle k \\rangle\n",
    "$$\n",
    "\n",
    "with equality only if every node has the same degree. This is a proof of the friendship paradox 😉!\n",
    "\n",
    "\n",
    ":::{note}\n",
    "The distribution $p'(k)$ is related to *the excess degree distribution* given by\n",
    "\n",
    "$$\n",
    "q(k) = \\frac{k + 1}{\\langle k \\rangle} p(k+1)\n",
    "$$\n",
    "\n",
    "The term *excess* comes from the fact that the distribution represents the number of additional connections a randomly chosen friend has, beyond the connection that led to their selection. It excludes the link to the focal node and focuses on the remaining connections of the selected friend.\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "\n",
    "The friend's degree, $\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$, concides with a term in Molloy-Reed condition:\n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} >2\n",
    "\n",
    "$$\n",
    "\n",
    "which is a condition for the existence of a giant component in a network. The Molloy-Reed condition states that the average degree of a node's friends must be at least 2 (the inequality is strict because the transition from a small component to a giant component is discontinuous). If a friend has only one edge, you and your friend form an isolated component. If a friend has two edges on average, your friend is a friend of someone else, and that someone else is also friend of another someone else and so on, forming a giant component.\n",
    "\n",
    ":::\n",
    "\n",
    "## Plotting degree distribution of a friend\n",
    "\n",
    "Let us compare the degree distribution of a node and its friend.\n",
    "We first get the edges in the network, from which we sample a friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "src, trg, _ = sparse.find(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d8a01",
   "metadata": {},
   "source": [
    "- `sparse.find(A)` returns the source node, target node, and edge weight of the edge.\n",
    "- `src` is the source node of the edge\n",
    "- `trg` is the target node of the edge\n",
    "- `_` is used to ignore the edge weight values, as we only need the source and target nodes for this analysis.\n",
    "\n",
    "Now, let us get the degree of each friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed20c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_friend = deg[src]\n",
    "p_deg_friend = np.bincount(deg_friend) / len(deg_friend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369ff06",
   "metadata": {},
   "source": [
    "The CCDF of the degree distributions of a node and a friend can be computed by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c18c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccdf_deg = 1 - np.cumsum(p_deg)[:-1]\n",
    "ccdf_deg_friend = 1 - np.cumsum(p_deg_friend)[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f2bf3",
   "metadata": {},
   "source": [
    "and plotted by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg, label='Node')\n",
    "ax = sns.lineplot(x=np.arange(len(ccdf_deg)), y=ccdf_deg_friend, label='Friend', ax = ax)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('CCDF')\n",
    "ax.legend(frameon = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732512e",
   "metadata": {},
   "source": [
    "The slope of the CCDF of a friend is flatter than that of a node, indicating that the degree distribution of a friend is biased towards higher degrees.\n",
    "\n",
    "```{footbibliography}\n",
    "```# In-class experiment\n",
    "\n",
    "\"Your friends have more friends than you\" is a well-known phenomenon in social networks. It appears everywhere from physical social networks to online social networks, and even random networks!\n",
    "OK. Let's do not \"think\" but \"feeeeel\" this paradox through the following in-class experiment.\n",
    "\n",
    "## Materials\n",
    "- [📇 Friendship card](./friendship-cards.pdf)\n",
    "- 🖊️ Pen\n",
    "\n",
    "## Friendship Network Experiment\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "\n",
    "   1. [📇] Receive Your Card\n",
    "      Get a card with a unique letter\n",
    "\n",
    "   2. [🤝] Meet and Greet (5 mins)\n",
    "      Move around, exchange cards with at least one friend\n",
    "\n",
    "   3. [🧮] Count Connections (2 mins)\n",
    "      Count received cards, write number, return cards\n",
    "\n",
    "   4. [📈] Calculate Average (2 mins)\n",
    "      Calculate average 'friend count' of your friends\n",
    "\n",
    "   5. [📝] Fill Form\n",
    "      Write your average and your own friend count\n",
    "      in a separate sheet\n",
    "\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "❗ Important Notes:\n",
    "  • This is a fun experiment, not a popularity contest\n",
    "  • Be respectful and inclusive during the meet and greet\n",
    "  • If you finish early, wait patiently for further instructions\n",
    "```# Friendship Paradox\n",
    "\n",
    "\n",
    "## The origin of the friendship paradox\n",
    "\n",
    "The paradox arises not because of the way we form friendships. It's about measurement! For example a person with 100 friends generates 100 cards, while a person with 1 friend generates only 1 card. If we average friend counts over the cards, popular people are counted more. This is where the friendship paradox comes from.\n",
    "\n",
    "In network terms, cards represent edges and people represent nodes. The friendship paradox arises because we measure at different levels: nodes or edges. The average friend count at the node level is lower than at the edge level because popular people are counted more often at the edge level.\n",
    "\n",
    "- **🎉 Fun Challenge**: Can you create a network where your friends have the most friends? 🤔💡 Give it a try in this {{ '[Friendship Paradox Game! 🎮✨]( BASE_URL/vis/friendship-paradox-game.html)'.replace('BASE_URL', base_url) }}\n",
    "\n",
    "- **Question**: Can you create a network where the friendship paradox is absent? In other words, can you create a graph, where your friends have the same number of friends as you?\n",
    "# Vaccination Game\n",
    "\n",
    "Beyond an interesting trivia, the friendship paradox has many practical utilities.\n",
    "\n",
    "- **🎉 Fun Challenge**: Can you control the spread of a virus by strategically vaccinating individuals? 🤔💡 Give it a try in this {{ '[Vaccination Game! 🎮✨]( BASE_URL/vis/vaccination-game.html)'.replace('BASE_URL', base_url) }}\n",
    "# Module 4: Friendship Paradox\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn about the friendship paradox. Specifically,\n",
    "- Friendship paradox: what is it, why it's important, and what are the consequences?\n",
    "- **Keywords**: friendship paradox, degree bias---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .Rmd\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.16.3\n",
    "kernelspec:\n",
    "  display_name: Python 3 (ipykernel)\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m05-clustering.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Hands-on: Clustering\n",
    "\n",
    "```{code-cell} ipython3\n",
    "# If you are using Google Colab, uncomment the following line to install igraph\n",
    "# !sudo apt install libcairo2-dev pkg-config python3-dev\n",
    "# !pip install pycairo cairocffi\n",
    "# !pip install igraph\n",
    "```\n",
    "\n",
    "## Modularity maximization\n",
    "\n",
    "Let us showcase how to use `igraph` to detect communities with modularity. We will use the Karate Club Network as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "g = igraph.Graph.Famous(\"Zachary\")\n",
    "igraph.plot(g, vertex_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15dd373",
   "metadata": {},
   "source": [
    "When it comes to maximizing modularity, there are a variety of algorithms to choose from.\n",
    "Two of the most popular ones are the `Louvain` and `Leiden` algorithms, both of which are implemented in `igraph`. The Louvain algorithm has been around for quite some time and is a classic choice, while the Leiden algorithm is a newer bee that often yields better accuracy. For our example, we'll be using the `Leiden` algorithm, and I think you'll find it really effective!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8155419",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = g.community_leiden(resolution=1, objective_function= \"modularity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f0908",
   "metadata": {},
   "source": [
    "What is `resolution`? It is a parameter that helps us tackle the resolution limit of the modularity maximization algorithm {footcite}`fortunato2007resolution`!\n",
    "In simple terms, when we use the resolution parameter $\\rho$, the modularity formula can be rewritten as\n",
    " follow:\n",
    "\n",
    "$$\n",
    "Q(M) = \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\left(A_{ij} - \\rho \\frac{k_i k_j}{2m}\\right) \\delta(c_i, c_j)\n",
    "$$\n",
    "\n",
    "Here, the parameter $\\rho$ plays a crucial role in balancing the positive and negative parts of the equation.\n",
    "The resolution limit comes into play because of the diminishing effect of the negative term as the number of edges $m$ increases.\n",
    "The parameter $\\rho$ can adjust this balance and allow us to circumvent the resolution limit.\n",
    "\n",
    "What is `communities`? This is a list of communities, where each community is represented by a list of nodes by their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(communities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7761b70",
   "metadata": {},
   "source": [
    "Let us visualize the communities by coloring the nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "community_membership = communities.membership\n",
    "palette = sns.color_palette().as_hex()\n",
    "igraph.plot(g, vertex_color=[palette[i] for i in community_membership])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bb21b",
   "metadata": {},
   "source": [
    "- `community_membership`: This is a list of community membership for each node.\n",
    "- `palette`: This is a list of colors to use for the communities.\n",
    "- `igraph.plot(g, vertex_color=[palette[i] for i in community_membership])`: This plots the graph 'g' with nodes colored by their community.\n",
    "\n",
    "### Exercise 01 🏋️‍♀️💪🧠\n",
    "\n",
    "1. Select a network of your choice from [Netzschleuder](https://networks.skewed.de/). For convenience, choose a network of nodes less than 5000.\n",
    "2. Download the csv version of the data by clicking something like \"3KiB\" under `csv` column.\n",
    "3. Unzip the file and find \"edges.csv\", open it with a text editor to familiarize yourself with the format.\n",
    "4. Load the data using `pandas`.\n",
    "5. Get the source and target nodes from the data to create an edge list.\n",
    "6. Construct a graph from the edge list, either using `igraph` or `scipy`.\n",
    "7. Find communities by maximizing the modularity and visualize them.\n",
    "8. Try at least three different values of the resolution parameter and observe how the community structure changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecd2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2a563",
   "metadata": {},
   "source": [
    "## Stochstic Block Model\n",
    "\n",
    "Let us turn the SBM as our community detection tool using [graph-tool](https://graph-tool.skewed.de/). This is a powerful library for network analysis, with a focus on the stochastic block model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Uncomment the following code if you are using Google Colab\n",
    "#\n",
    "#!wget https://downloads.skewed.de/skewed-keyring/skewed-keyring_1.0_all_$(lsb_release -s -c).deb\n",
    "#!dpkg -i skewed-keyring_1.0_all_$(lsb_release -s -c).deb\n",
    "#!echo \"deb [signed-by=/usr/share/keyrings/skewed-keyring.gpg] https://downloads.skewed.de/apt $(lsb_release -s -c) main\" > /etc/apt/sources.list.d/skewed.list\n",
    "#!apt-get update\n",
    "#!apt-get install python3-graph-tool python3-matplotlib python3-cairo\n",
    "#!apt purge python3-cairo\n",
    "#!apt install libcairo2-dev pkg-config python3-dev\n",
    "#!pip install --force-reinstall pycairo\n",
    "#!pip install zstandard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34438f98",
   "metadata": {},
   "source": [
    "We will identify the communities using the stochastic block model as follows.\n",
    "First, we will convert the graph object in igraph to that in graph-tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool.all  as gt\n",
    "import numpy as np\n",
    "import igraph\n",
    "\n",
    "# igraph object\n",
    "g = igraph.Graph.Famous(\"Zachary\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convert the graph object in igraph to that in graph-tool\n",
    "edges = g.get_edgelist()\n",
    "r, c = zip(*edges)\n",
    "g_gt = gt.Graph(directed=False)\n",
    "g_gt.add_edge_list(np.vstack([r, c]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bf4b7",
   "metadata": {},
   "source": [
    "Then, we will fit the stochastic block model to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the stochastic block model\n",
    "state = gt.minimize_blockmodel_dl(\n",
    "     g_gt,\n",
    "     state_args={\"deg_corr\": False, \"B_min\":2, \"B_max\":10},\n",
    ")\n",
    "b = state.get_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a05892",
   "metadata": {},
   "source": [
    "- `B_min` and `B_max` are the minimum and maximum number of communities to consider.\n",
    "- `deg_corr` is a boolean flag to switch to the degree-corrected SBM {footcite}`karrer2011stochastic`.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Here's a fun fact: the likelihood maximization on its own can't figure out how many communities there should be. But `graph-tool` has a clever trick to circumvent this limitation.\n",
    "`graph-tool` actually fits multiple SBMs, each with a different number of communities. Then, it picks the most plausible one based on a model selection criterion.\n",
    "```\n",
    "\n",
    "Let's visualize the communities to see what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35050904",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert the block assignments to a list\n",
    "community_membership = b.get_array()\n",
    "\n",
    "# The community labels may consist of non-consecutive integers, e.g., 10, 8, 1, 4, ...\n",
    "# So we reassign the community labels to be 0, 1, 2, ...\n",
    "community_membership = np.unique(community_membership, return_inverse=True)[1]\n",
    "community_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a0271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color palette\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette().as_hex()\n",
    "# Plot the graph with nodes colored by their community\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "igraph.plot(\n",
    "    g,\n",
    "    target=ax,\n",
    "    vertex_color=[palette[i] for i in community_membership],\n",
    ")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da5bad4",
   "metadata": {},
   "source": [
    "What we're seeing here isn't a failure at all. In fact, it's the best partition according to our stochastic block model. The model has discovered something called a **core-periphery structure** {footcite}`borgatti2000models`. Let me break that down:\n",
    "\n",
    "- Think of a major international airport (the core) and smaller regional airports (the periphery).\n",
    "- Major international airports have many flights connecting to each other (densely connected).\n",
    "- Smaller regional airports have fewer connections among themselves (sparsely connected).\n",
    "- Many regional airports have flights to major hubs (periphery connected to the core).\n",
    "\n",
    "That's exactly what our model found in this network.\n",
    "\n",
    "If we look at the adjacency matrix, we would see something that looks like an upside-down \"L\". This shape is like a signature for core-periphery structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5867e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert igraph Graph to adjacency matrix\n",
    "A = np.array(g.get_adjacency().data)\n",
    "\n",
    "# Sort nodes based on their community (core first, then periphery)\n",
    "sorted_indices = np.argsort(community_membership)\n",
    "A_sorted = A[sorted_indices][:, sorted_indices]\n",
    "\n",
    "# Plot the sorted adjacency matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(A_sorted, cmap='binary')\n",
    "plt.title(\"Sorted Adjacency Matrix: Core-Periphery Structure\")\n",
    "plt.xlabel(\"Node Index (sorted)\")\n",
    "plt.ylabel(\"Node Index (sorted)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d7d27",
   "metadata": {},
   "source": [
    "### Exercise 02 🏋️‍♀️💪🧠\n",
    "\n",
    "1. Select a network of your choice from [Netzschleuder](https://networks.skewed.de/). For convenience, choose a network of nodes less than 5000.\n",
    "2. Download the csv version of the data by clicking something like \"3KiB\" under `csv` column.\n",
    "3. Unzip the file and find \"edges.csv\", open it with a text editor to familiarize yourself with the format.\n",
    "4. Load the data using `pandas`.\n",
    "5. Get the source and target nodes from the data to create an edge list.\n",
    "6. Construct a graph from the edge list, either using `igraph` or `scipy`.\n",
    "7. Find communities by fitting the stochastic block model and visualize them.\n",
    "8. Try `deg_corr=True` and compare the results with those from `deg_corr=False`.\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Graph cut\n",
    "\n",
    "Another approach from computer science is to treat a community detection problem as an *optimization* problem.\n",
    "An early example is the **graph cut** problem, which asks to find the minimum number of edges to cut the graph into two disconnected components.\n",
    "\n",
    "Specifically, let us consider cutting the network into two communities. Let $V_1$ and $V_2$ be the set of nodes in the two communities.\n",
    "Then, the cut is the number of edges between the two communities, which is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Cut}(V_1, V_2) = \\sum_{i \\in V_1} \\sum_{j \\in V_2} A_{ij}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, the community detection problem is translated into **an optimization problem**, with the goal of finding a cut $V_1, V_2$ that minimizes $\\text{Cut}(V_1, V_2)$.\n",
    "\n",
    "The description of this problem is not complete 😈. Let's find out what is missing by playing with the optimization problem.\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "Can you identify what is missing in the description of the graph cut problem? Without this, the best cut is trivial. {{ \"<a href='BASE_URL/vis/community-detection/index.html?scoreType=graphcut&numCommunities=2&randomness=1&dataFile=two-cliques.json'>Graph Cut Problem 🎮</a>\".replace('BASE_URL', base_url) }}\n",
    "\n",
    "```{dropdown} Click to reveal the answer!\n",
    "\n",
    "The missing element is a constraint: each community must contain at least one node. Without this, the trivial solution of placing all nodes in a single community would always yield a cut of zero.\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Modularity (Cont.)\n",
    "\n",
    ":::{figure-md} fig-modularity-game\n",
    "\n",
    "<img src=\"../figs/modularity.jpg\" alt=\"Single node failure\" width=\"100%\">\n",
    "\n",
    "Illustration of how modularity measures assortativity relative to a null model.\n",
    ":::\n",
    "\n",
    "Let's dive into the modularity formula! To put modularity into math terms, we need a few ingredients:\n",
    "- $m$: The total number of strings (edges) in our bag\n",
    "- $n$: The total number of balls (nodes) we have\n",
    "- $A_{ij}$: This tells us if ball $i$ and ball $j$ are connected by a string\n",
    "- $\\delta(c_i,c_j)$: This is our color-checker. It gives us a 1 if balls $i$ and $j$ are the same color (same community), and 0 if they're different.\n",
    "\n",
    "Now, the probability of pulling out a string out of $m$ string and finding matching colors on both ends is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\sum_{i=1}^n \\sum_{j=i+1}^n A_{ij} \\delta(c_i,c_j) = \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\delta(c_i,c_j)\n",
    "$$\n",
    "\n",
    "We set $A_{ii} = 0$ by assuming our network doesn't have any \"selfie strings\" (where a ball is connected to itself). Also, we changed our edge counting a bit. Instead of counting each string once (which gave us $m$), we're now counting each string twice (once from each end). That's why we use $2m$ in the equation.\n",
    "\n",
    "Now, imagine we've cut all the strings, and we're going to draw two balls at random with replacement.\n",
    "Here's how our new bag looks:\n",
    "- We have $2m$ balls in total ($1$ string has $2$ balls, and thus $m$ strings have $2m$ balls in total).\n",
    "- A node with $k$ edges correspond to the $k$ of $2m$ balls in the bag.\n",
    "- The color of each ball in our bag matches the color (or community) of its node in the network.\n",
    "\n",
    "Now, what's the chance of pulling out two balls of the same color?\n",
    "\n",
    "$$\n",
    "\\sum_{c=1}^C \\left( \\frac{1}{2m}\\sum_{i=1}^n k_i \\delta(c, c_i) \\right)^2\n",
    "$$\n",
    "\n",
    "where $k_i$ is the degree (i.e., the number of edges) of node $i$, and $C$ is the total number of communities (i.e., colors).\n",
    "\n",
    "Here's what it means in simple terms:\n",
    "- We look at each color ($c$) one by one (the outer sum).\n",
    "- For each color, we figure out how many balls of that color are in our bag ($\\frac{1}{2m}\\sum_{i=1}^n k_i \\delta(c, c_i)$).\n",
    "- We divide by $2m$ to get the probability of drawing a ball of that color.\n",
    "- We then calculate the chance of grabbing that color twice in a row ($\\left( \\frac{1}{2m}\\sum_{i=1}^n k_i \\delta(c, c_i) \\right)^2$).\n",
    "- Finally, we add up these chances for all $C$ colors.\n",
    "\n",
    "Putting altogether, the modularity is defined by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q &=\\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\delta(c_i,c_j) - \\sum_{c=1}^C \\left( \\frac{1}{2m}\\sum_{i=1}^n k_i \\delta(c, c_i) \\right)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Equivalently, a standard expression is given by\n",
    "\n",
    "$$\n",
    "Q =\\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\left[ A_{ij} -  \\frac{k_ik_j}{2m} \\right]\\delta(c_i,c_j)\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "\n",
    "Are the two forms of modularity the same formula? Let's see how we can transform one into the other:\n",
    "\n",
    "1. We start with our first form of modularity:\n",
    "\n",
    "   $$\n",
    "   Q =\\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\delta(c_i,c_j) - \\sum_{c=1}^C \\left( \\frac{1}{2m}\\sum_{i=1}^n k_i \\delta(c, c_i) \\right)^2\n",
    "   $$\n",
    "\n",
    "2. First, let's factor out $\\frac{1}{2m}$ from both terms:\n",
    "\n",
    "   $$\n",
    "   Q =\\frac{1}{2m} \\left[ \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\delta(c_i,c_j) - \\frac{1}{2m}\\sum_{c=1}^C \\left( \\sum_{i=1}^n k_i \\delta(c, c_i) \\right)^2 \\right]\n",
    "   $$\n",
    "\n",
    "3. Now, here's a neat trick: $(\\sum_i a_i)^2 = (\\sum_i a_i)( \\sum_j a_j)$. We can use this to expand the squared term:\n",
    "\n",
    "   $$\n",
    "   Q =\\frac{1}{2m} \\left[ \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\delta(c_i,c_j) - \\frac{1}{2m}\\sum_{c=1}^C \\left( \\sum_{i=1}^n k_i \\delta(c, c_i) \\right) \\left( \\sum_{j=1}^n k_j \\delta(c, c_j) \\right)\\right]\n",
    "   $$\n",
    "\n",
    "4. And here is another trick $(\\sum_i a_i)( \\sum_j a_j) = \\sum_i a_i \\sum_j a_j = \\sum_i \\sum_j a_ia_j$\n",
    "\n",
    "   $$\n",
    "   Q =\\frac{1}{2m} \\left[ \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\delta(c_i,c_j) - \\frac{1}{2m}\\sum_{c=1}^C \\left( \\sum_{i=1}^n \\sum_{j=1}^n k_i k_j  \\delta(c, c_i)  \\delta(c, c_j) \\right)\\right]\n",
    "   $$\n",
    "\n",
    "5. Here's yet another cool trick, $\\delta(c,c_i) \\delta(c, c_j) = \\delta(c_i,c_j)$. This means we can simplify our expression:\n",
    "\n",
    "   $$\n",
    "   Q =\\frac{1}{2m} \\left[ \\sum_{i=1}^n \\sum_{j=1}^n A_{ij} \\delta(c_i,c_j) -  \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n k_i k_j  \\delta(c_i,c_j) \\right]\n",
    "   $$\n",
    "\n",
    "6. Finally, we can factor out the common parts:\n",
    "\n",
    "   $$\n",
    "   Q =\\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\left[ A_{ij} -  \\frac{k_ik_j}{2m} \\right]\\delta(c_i,c_j)\n",
    "   $$\n",
    "```\n",
    "\n",
    "\n",
    "## Modularity Demo\n",
    "\n",
    "Let's learn how the modularity works by playing with a community detection game!\n",
    "\n",
    "```{admonition} Exercise 1\n",
    ":class: tip\n",
    "\n",
    "Find communities by maximizing the modularity. {{ \"<a href='BASE_URL/vis/community-detection/index.html?scoreType=modularity&numCommunities=2&randomness=1&dataFile=two-cliques.json'>Modularity maximization (two communities) 🎮</a>\".replace('BASE_URL', base_url) }}\n",
    "\n",
    "```\n",
    "\n",
    "One of the good things about modularity is that it can figure out how many communities there should be all by itself! 🕵️‍♀️ Let's have some fun with this idea. We're going to play the same game again, but this time, we'll start with a different number of communities. See how the modularity score changes as we move things around.\n",
    "\n",
    "```{admonition} Exercise 2\n",
    ":class: tip\n",
    "\n",
    "Find communities by maximizing the modularity. {{ \"<a href='BASE_URL/vis/community-detection/index.html?scoreType=modularity&numCommunities=4&randomness=1&dataFile=two-cliques.json'>Modularity maximization (four communities) 🎮</a>\".replace('BASE_URL', base_url) }}\n",
    "```\n",
    "\n",
    "Now, let's take our modularity maximization for a real-world example! 🥋 We're going to use the famous karate club network. This network represents friendships between members of a university karate club. It's a classic in the world of network science, and it's perfect for seeing how modularity works in practice.\n",
    "\n",
    "```{admonition} Exercise 3\n",
    ":class: tip\n",
    "\n",
    "Find communities by maximizing the modularity. {{ \"<a href='BASE_URL/vis/community-detection/index.html?scoreType=modularity&numCommunities=4&randomness=0.25&dataFile=net_karate.json'>Modularity maximization (four communities) 🎮</a>\".replace('BASE_URL', base_url) }}\n",
    "\n",
    "```\n",
    "\n",
    "## Limitation of Modularity\n",
    "\n",
    "Like many other community detection methods, modularity is not a silver bullet. Thanks to extensive research, we know many limitations of modularity. Let's take a look at a few of them.\n",
    "\n",
    "### Resolution limit\n",
    "\n",
    "The modularity finds two cliques connected by a single edge as two separate communities.\n",
    "But what if we add another community to this network?\n",
    "Our intuition tells us that, because communities are *local* structure, the two cliques should remain separated by the modularity. But is this the case?\n",
    "\n",
    "```{admonition} Exercise 4\n",
    ":class: tip\n",
    "\n",
    "Find communities by maximizing the modularity. {{ \"<a href='BASE_URL/vis/community-detection/index.html?scoreType=modularity&numCommunities=3&randomness=0.9&dataFile=two-cliques-big-clique.json'>Modularity maximization (four communities) 🎮</a>\".replace('BASE_URL', base_url) }}\n",
    "\n",
    "```{dropdown} Click here to see the solution\n",
    "\n",
    "The best modularity score actually comes from merging our two cliques into one big community. This behavior is what we call the **Resolution limit** {footcite}`fortunato2007resolution`. Modularity can't quite make out communities that are smaller than a certain size!\n",
    "\n",
    "Think of it like this: modularity is trying to see the big picture, but it misses the little details. In network terms, the number of edges $m_c$ in a community $c$ has to be bigger than a certain size. This size is related to the total number of edges $m$ in the whole network. We write this mathematically as ${\\cal O}(m)$.\n",
    "```\n",
    "\n",
    "### Spurious communities\n",
    "\n",
    "What if the network does not have any communities at all? Does the modularity find no communities? To find out, let's run the modularity on a random network, where each pair of nodes is connected randomly with the same probability.\n",
    "\n",
    "```{admonition} Exercise 5\n",
    ":class: tip\n",
    "\n",
    "Find communities by maximizing the modularity. {{ \"<a href='BASE_URL/vis/community-detection/index.html?scoreType=modularity&numCommunities=3&randomness=0.8&dataFile=random-net.json'>Modularity maximization (four communities) 🎮</a>\".replace('BASE_URL', base_url) }}\n",
    "\n",
    "```{dropdown} Click here to see the solution\n",
    "\n",
    "Surprise, surprise! 😮 Modularity finds communities even in our random network, and with a very high score too! It's like finding shapes in clouds - sometimes our brains (or algorithms) see patterns where there aren't any.\n",
    "\n",
    "The wild thing is that the modularity score for this random network is even higher than what we saw for our network with two clear cliques!\n",
    "\n",
    "This teaches us two important lessons:\n",
    "1. We can't compare modularity scores between different networks. It's like comparing apples and oranges! 🍎🍊\n",
    "2. A high modularity score doesn't always mean we've found communities.\n",
    "\n",
    "Interested readers can read more about this in [this tweet by Tiago Peixoto](https://twitter.com/tiagopeixoto/status/1466352013856358400) and the discussion [here](https://reticular.hypotheses.org/1924).\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" style=\"max-width: 550px;\"><p lang=\"en\" dir=\"ltr\">Modularity maximization is not a reliable method to find communities in networks. Here&#39;s a simple example showing why:<br><br>1. Generate an Erdős-Rényi random graph with N nodes and average degree &lt;k&gt;.<br><br>2. Find the maximum modularity partition. <a href=\"https://t.co/MTt5DdFXSX\">pic.twitter.com/MTt5DdFXSX</a></p>&mdash; Tiago Peixoto (@tiagopeixoto) <a href=\"https://twitter.com/tiagopeixoto/status/1466352013856358400?ref_src=twsrc%5Etfw\">December 2, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "```\n",
    "\n",
    "\n",
    "## So should we avoid modularity?\n",
    "\n",
    "The simple answer is no. Modularity is still a powerful tool for finding communities in networks. Like any other method, it has its limitations. And knowing these limitations is crucial for using it effectively. There is \"free lunch\" in community detection {footcite}`peel2017ground`.\n",
    "\n",
    "When these implicit assumptions are met, modularity is in fact a very powerful method for community detection. For example, it is in fact an \"optimal\" method for a certain class of networks {footcite}`nadakuditi2012graph`.\n",
    "\n",
    "So, keep modularity in your toolbox. Just remember to use it wisely!\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Modularity\n",
    "\n",
    "**Modularity** is by far the most widely used method for community detection.\n",
    "Modularity can be derived in many ways, but we will follow the one derived from assortativity.\n",
    "\n",
    "## Assortativity\n",
    "\n",
    "**Assortativity** is a measure of the tendency of nodes to connect with nodes of the same attribute.\n",
    "The attribute, in our case, is the community that the node belongs to, and we say that a network is assortative if nodes of the same community are more likely to connect with each other than nodes of different communities.\n",
    "\n",
    "Let's think about assortativity by using color balls and strings! 🎨🧵\n",
    "\n",
    "Imagine we're playing a game as follows:\n",
    "\n",
    "1. Picture each connection in our network as two colored balls joined by a piece of string. 🔴🟢--🔵🟡\n",
    "2. The color of each ball shows which community it belongs to.\n",
    "3. Now, let's toss all these ball-and-string pairs into a big bag.\n",
    "4. We'll keep pulling out strings with replacement and checking if the balls on each end match colors.\n",
    "\n",
    "The more color matches we find, the more assortative our network is. But, there's a catch!\n",
    "What if we got lots of matches just by luck? For example, if all our balls were the same color, we'd always get a match. But that doesn't tell us much about our communities.\n",
    "So, to be extra clever, we compare our results to a \"random\" version (null model):\n",
    "\n",
    "1. We snip all the strings and mix up all the balls.\n",
    "2. Then we draw pairs of balls at random *with replacement* and see how often the colors match.\n",
    "\n",
    "By comparing our original network to this mixed-up version, we can see if our communities are really sticking together more than we'd expect by chance.\n",
    "This comparison against the random version is the heart of modularity. Unlike graph cut methods that aim to maximize assortativity directly, modularity measures assortativity *relative* to *a null model*.\n",
    "\n",
    ":::{figure-md} fig-modularity-game\n",
    "\n",
    "<img src=\"../figs/modularity.jpg\" alt=\"Single node failure\" width=\"100%\">\n",
    "\n",
    "Illustration of how modularity measures assortativity relative to a null model.\n",
    ":::\n",
    "\n",
    "## Deriving Modularity\n",
    "\n",
    "Now, let's put on our math hats and make this colorful game a bit more precise.\n",
    "\n",
    "Let's introduce some helpful symbols to describe our network:\n",
    "- $N$: This is our total number of nodes (or balls in our game)\n",
    "- $M$: The number of edges (or strings) connecting our nodes\n",
    "- $A_{ij}$: Adjacency matrix. If $A_{ij} = 1$, it means node $i$ and node $j$ are connected. If $A_{ij} = 0$, they're not connected.\n",
    "- $k_i$: Degree of node $i$, i.e., how many edges a node has.\n",
    "- $c_i$: Community of node $i$, i.e., which community a node belongs to.\n",
    "- $\\delta(c_i, c_j)$: Kronecker delta function. It gives us 1 if nodes $i$ and $j$ are the same color, and 0 if they're different.\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "What is the probability of color matches for a given network? Derive the probability by using $\\sum, M, A_{ij}, \\delta(c_i, c_j)$.\n",
    "\n",
    "```{dropdown} Hint\n",
    "Let's think about our colorful bag of balls and strings! 🎨🧵\n",
    "First, ask yourself:\n",
    "1. How many strings do we have in total? (This is our M!)\n",
    "2. Now, out of all these strings, how many are the same color on both ends?\n",
    "```\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "What is the probability of color matches for the random version? Derive the probability by using $\\sum, M, \\delta(c_i, c_j), k_i,k_j$.\n",
    "\n",
    "```{dropdown} Hint\n",
    "1. Imagine a big bag full of colorful balls, but this time without any strings. 🔴🟢🔵🟡\n",
    "2. Now, think about picking one ball out of the bag. What are the chances of picking a specific color?\n",
    "3. Then, put that ball back and pick another one. What are the odds this second ball matches the color of the first one?\n",
    "\n",
    "```\n",
    "\n",
    "The full modularity formula is on the next page 😉.---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Community detection (pattern matching)\n",
    "\n",
    "Community detection is an abstract unsupervised problem. It is abstract because there is no clear-cut definition or ground truth to compare against. The concept of a community in a network is subjective and highly context-dependent.\n",
    "\n",
    "A classical approach to community detection is based on *pattern matching*.\n",
    "Namely, we first explicitly define a community by a specific connectivity pattern of its members. Then, we search for these communities in the network.\n",
    "\n",
    ":::{figure-md} clique\n",
    "\n",
    "<img src=\"https://pythonhosted.org/trustedanalytics/R_images/k-clique_201508281155.png\" alt=\"Clique graph\" width=\"80%\">\n",
    "\n",
    "Cliques of different sizes. Taken from [https://pythonhosted.org/trustedanalytics/python_api/graphs/graph-/kclique_percolation.html](https://pythonhosted.org/trustedanalytics/python_api/graphs/graph-/kclique_percolation.html)\n",
    ":::\n",
    "\n",
    "Perhaps, the strictest definition of a community is a *clique*: a group of nodes all connected to each other. Examples include triangles (3-node cliques) and fully-connected squares (4-node cliques).\n",
    "However, cliques are often too rigid for real-world networks. In social networks, for instance, large groups of friends rarely have every member connected to every other, yet we want to accept such \"in-perfect\" social circles as communities.\n",
    "This leads to the idea of relaxed versions of cliques, called **pseudo-cliques**.\n",
    "\n",
    "Pseudo-cliques are defined by relaxing at least one of the following three dimensions of strictness:\n",
    "\n",
    "1. Degree: Not all nodes need to connect to every other node.\n",
    "   - **$k$-plex**: each node connects to all but $k$ others in the group {footcite}`seidman1978graph`.\n",
    "   - **$k$-core**: each node connects to $k$ others in the group {footcite}`seidman1983network`.\n",
    "2. Density: The overall connection density can be lower.\n",
    "   - **$\\rho$-dense subgraphs**, with a minimum edge density of $\\rho$ {footcite}`goldberg1984finding`.\n",
    "3. Distance: Nodes can be further apart.\n",
    "   - **$n$-clique**, where all nodes are within n steps of each other {footcite}`luce1950connectivity`.\n",
    "4. Combination of the above:\n",
    "   - **n-clan** and **n-club** {footcite}`mokken1979cliques`\n",
    "   - **$k$-truss**, a maximal subgraph where all edges participate in at least $k-2$ triangles {footcite}`saito2008extracting,cohen2009graph,wang2010triangulation`.\n",
    "   - **$\\rho$-dense core**, a subgraph with minimum conductance $\\rho$ {footcite}`koujaku2016dense`.\n",
    "\n",
    ":::{figure-md} clique-pattern\n",
    "\n",
    "<img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S0378873315000520-gr1.jpg\" alt=\"Pseudo-clique patterns\" width=\"80%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "\n",
    "Illustation of different pseudo cliques. Taken from {footcite}`koujaku2016dense`.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```\n",
    "# Pen and Paper\n",
    "\n",
    "✍️ [Pen and Paper Exercise](./pen-and-paper/exercise.pdf) 🚢\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Balanced cut\n",
    "\n",
    "## Ratio Cut\n",
    "\n",
    "Graph cut often provide unbalanced communities, e.g., a community consisting of a single node, and another consisting of all other nodes. For example, if the network has a node with degree one (e.g., one edge), an optimal cut will be to place this node in its own community, resulting in a cut of one.\n",
    "\n",
    "**Ratio cut** addresses this issue by introducing a normalization factor to balance the cut.\n",
    "Suppose we cut the network into two communities $V_1$ and $V_2$, then the ratio cut is defined as\n",
    "\n",
    "$$\n",
    "\\text{Ratio cut}(V_1, V_2) = \\frac{1}{|V_1| \\cdot |V_2|} \\sum_{i \\in V_1} \\sum_{j \\in V_2} A_{ij}\n",
    "$$\n",
    "\n",
    "- $|V_1|$ (or |V_2|) is the number of nodes in the community $V_1$ (or $V_2$).\n",
    "\n",
    "The normalization factor $1/(|V_1| |V_2|)$ balances the community sizes. It's smallest when communities are equal ($|V_1| = |V_2|$) and largest when one community has only one node ($|V_1| = 1$ or $|V_2| = 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3145be",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Total number of nodes\n",
    "total_nodes = 100\n",
    "\n",
    "# Create an array of possible sizes for V1\n",
    "V1_sizes = np.arange(1, total_nodes)\n",
    "\n",
    "# Calculate corresponding sizes for V2\n",
    "V2_sizes = total_nodes - V1_sizes\n",
    "\n",
    "# Calculate the normalization factor\n",
    "normalization_factor = 1 / (V1_sizes * V2_sizes)\n",
    "\n",
    "# Create the plot\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "plt.plot(V1_sizes, normalization_factor)\n",
    "plt.title('Normalization Factor vs. Community Size')\n",
    "plt.xlabel('Size of V1')\n",
    "plt.ylabel('1 / (|V1| * |V2|)')\n",
    "plt.yscale('log')  # Use log scale for y-axis due to large range of values\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea9145",
   "metadata": {},
   "source": [
    "## Normalized cut\n",
    "\n",
    "**Normalized cut**{footcite}`shi2000normalized` balances communities based on edge count, unlike Ratio cut which uses node count. It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Normalized cut}(V_1, V_2) = \\frac{1}{|E_1| \\cdot |E_2|} \\sum_{i \\in V_1} \\sum_{j \\in V_2} A_{ij}\n",
    "$$\n",
    "\n",
    "- $|E_1|$ and $|E_2|$ are the number of edges in the communities $V_1$ and $V_2$, respectively.\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "Compute the ratio cut and normalized cut for the following network. The red edges should be cut.\n",
    "\n",
    "```{glue:figure} fig-graph-cut\n",
    ":name: fig-graph-cut\n",
    "\n",
    "```\n",
    "\n",
    "```{dropdown} Click here to reveal the answer\n",
    "\n",
    "The graph consists of two cliques, each with 5 nodes ($|V_1| = |V_2| = 5$).\n",
    "Each clique has 10 internal edges and 2 edges connecting to the other clique.\n",
    "Therefore, $|E_1| = |E_2| = 10 + 2 = 12$.\n",
    "We can now calculate:\n",
    "\n",
    "- **Ratio cut**: $2 / (5 \\times 5) = 0.08$.\n",
    "- **Normalized cut**: $2 / (12 \\times 12) = 0.01388889$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05690f8e",
   "metadata": {
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "from myst_nb import glue\n",
    "\n",
    "# Create two cliques of size 5\n",
    "G1 = ig.Graph.Full(5)\n",
    "G2 = ig.Graph.Full(5)\n",
    "\n",
    "# Combine the two cliques\n",
    "G = G1 + G2\n",
    "\n",
    "# Add an edge between the two cliques\n",
    "G.add_edge(0, 5)\n",
    "G.add_edge(1, 6)\n",
    "\n",
    "# Draw the graph\n",
    "layout = G.layout_fruchterman_reingold()\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Draw the connecting edge in red\n",
    "ig.plot(\n",
    "    G,\n",
    "    target=ax,\n",
    "    layout=layout,\n",
    "    vertex_color='lightblue',\n",
    "    vertex_size=20,\n",
    "    edge_color='gray',\n",
    "    edge_width=1\n",
    ")\n",
    "\n",
    "# Draw the connecting edge in red behind the graph\n",
    "ax.plot([layout[0][0], layout[5][0]], [layout[0][1], layout[5][1]], color='red', linewidth=2, zorder=0)\n",
    "ax.plot([layout[1][0], layout[6][0]], [layout[1][1], layout[6][1]], color='red', linewidth=2, zorder=0)\n",
    "\n",
    "ig.plot(\n",
    "    G,\n",
    "    target=ax,\n",
    "    layout=layout,\n",
    "    vertex_color='white',\n",
    "    vertex_size=20,\n",
    "    edge_color='black',\n",
    "    edge_width=1\n",
    ")\n",
    "\n",
    "# Add labels to the nodes\n",
    "for i, coords in enumerate(layout):\n",
    "    ax.annotate(str(i), coords, ha='center', va='center')\n",
    "\n",
    "plt.title(\"Two Cliques Connected by One Edge\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "\n",
    "glue(\"fig-graph-cut\", fig, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee63f69",
   "metadata": {},
   "source": [
    "## Cut into more than two communities\n",
    "\n",
    "Ratio cut and Normalized cut can be extended to cut into more than two communities. Specifically, we can extend them to cut into $k$ communities, i.e., $V_1, V_2, \\dots, V_k$ by defining\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Ratio cut}(V_1, V_2, \\dots, V_k) &= \\sum_{k=1}^K \\frac{1}{|V_k|} \\left(\\sum_{i \\in V_k} \\sum_{j \\notin V_{k}} A_{ij} \\right) \\\\\n",
    "\\text{Normalized cut}(V_1, V_2, \\dots, V_k) &= \\sum_{k=1}^K \\frac{1}{|E_k|} \\left(\\sum_{i \\in V_k} \\sum_{j \\notin V_{k}} A_{ij} \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "## Algorithms to find the best cut\n",
    "\n",
    "For both ratio and normalized cut, finding the best cut is a NP-hard problem. Yet, there are some heuristics to find a good cut.\n",
    "Interested students are encouraged to refer to [Ulrike von Luxburg \"A Tutorial on Spectral Clustering\"](https://arxiv.org/abs/0711.0189) for more details.\n",
    "\n",
    "## Issue of Ratio cut and Normalized cut\n",
    "\n",
    "While Ratio cut and Normalized cut methods are clever approaches, they do come with a couple of challenges we should be aware of.\n",
    "\n",
    "Firstly, these methods ask us to decide upfront how many communities we want to find. This can be tricky because, in real-world networks, we often don't know this number in advance. It requires us to make a guess on how many different groups of friends we have before actually looking at our social circle.\n",
    "\n",
    "Secondly, and perhaps more critically, these methods *favor* communities of roughly the same size.\n",
    "It's as if they're assuming all our friend groups should have about the same number of people.\n",
    "But as we know from real life, that's not always the case.\n",
    "Some of us might have a large group of college friends and a smaller group of childhood buddies.\n",
    "Research has shown that in many real-world networks, communities can indeed be quite different in size {footcite}`palla2005uncovering,clauset2004finding`.\n",
    "\n",
    "These limitations don't mean these methods should not be used, but they do remind us the importance of understanding the underlying assumptions and limitations of methods we use 😉.\n",
    "It's always good to keep these points in mind when we're working with network data. 🕸️💡\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Stochastic Block Model\n",
    "\n",
    "Let's talk about two ways to look at communities in networks.\n",
    "\n",
    "In modularity maximization, we are given a network and asked to find the best way to group its parts into communities.\n",
    "\n",
    "Let's flip that idea on its head! 🙃 Instead of starting with a network and looking for communities, we start with the communities and ask, *\"What kind of network would we get if the nodes form these communities?\"*. This is the idea of the **Stochastic Block Model (SBM)**.\n",
    "\n",
    "While modularity maximization is about finding hidden patterns, SBM is about imagining what a network would look like based on a given community structure. Two sides of the same coin, each giving us a unique perspective on community detection.\n",
    "\n",
    "## Model\n",
    "\n",
    "In stochastic block model, we describe a network using probabilities given a community structure. Specifically, let us consider two nodes $i$ and $j$ who belong to community $c_i$ and $c_j$. Then, the probability of an edge between $i$ and $j$ is given by their community membership.\n",
    "\n",
    "$$\n",
    "P(A_{ij}=1|c_i, c_j) = p_{c_i,c_j}\n",
    "$$\n",
    "\n",
    "where $p_{c_i,c_j}$ is the probability of an edge between nodes in community $c_i$ and $c_j$, respectively.\n",
    "Notice that the edge probability is fully specified by the community membership of the nodes.\n",
    "This means that nodes in a community are connected with the same probability irrespective of the nodes themselves, and the nodes in different two communities are also connected with the same probability.\n",
    "As a result, when plotting the adjacency matrix, we observe \"blocks\" of different edge densities, which is why we say that SBM is a \"block model\".\n",
    "\n",
    "```{code-cell} ipython3\n",
    ":tags: [hide-input]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import igraph\n",
    "\n",
    "# Generate SBM\n",
    "n, k = 900, 3\n",
    "\n",
    "# Create block sizes (equal for simplicity)\n",
    "block_sizes = [n // k] * k\n",
    "\n",
    "# Create diverse pref matrix\n",
    "pref_matrix = [\n",
    "    [0.3, 0.05, 0.1],\n",
    "    [0.05, 0.4, 0.02],\n",
    "    [0.1, 0.02, 0.35]\n",
    "]\n",
    "\n",
    "# Generate SBM using igraph\n",
    "g = igraph.Graph.SBM(n, pref_matrix, block_sizes)\n",
    "\n",
    "# Convert to adjacency matrix for visualization\n",
    "A = np.array(g.get_adjacency().data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(A, cmap='binary')\n",
    "plt.title(\"Adjacency Matrix of Stochastic Block Model\")\n",
    "plt.xlabel(\"Node Index\")\n",
    "plt.ylabel(\"Node Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Characterizing network structures with the SBM\n",
    "\n",
    "Stochastic Block Model is a flexible model that can be used to describe a wide range of network structures.\n",
    "\n",
    "Let's start with communities where nodes within a community are more likely to be connected to each other than nodes in different communities. We can describe this using SBM by:\n",
    "\n",
    "$$\n",
    "P_{c,c'} = \\begin{cases}\n",
    "    p_{\\text{in}} & \\text{if } c = c' \\\\\n",
    "    p_{\\text{out}} & \\text{if } c \\neq c'\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "-  $p_{\\text{in}}$ is the chance of a connection between nodes in the same community\n",
    "- $p_{\\text{out}}$ is the chance of a connection between nodes in different communities\n",
    "\n",
    "Usually, we set $p_{\\text{in}} > p_{\\text{out}}$, because nodes in the same community tend to be more connected.\n",
    "\n",
    "But, there's more SBM can do:\n",
    "\n",
    "1. **Disassortative communities**: What if we flip things around and set $p_{\\text{in}} < p_{\\text{out}}$? Now we have communities where nodes prefer to connect with nodes from other communities. This is not in line with the communities we have focused on so far. Yet, it is still a valid model of community structure, and SBM allows for this generalization of community structure easily.\n",
    "\n",
    "2. **Random networks**: If we make $p_{\\text{in}} = p_{\\text{out}}$, we get a completely random network where every node has an equal chance of connecting to any other node. This is what we call an Erdős-Rényi network.\n",
    "\n",
    "In sum, SBM has been used as a playground for network scientists. We can use it to create many interesting network structures and study how they behave.\n",
    "\n",
    "## Generating networks with SBM\n",
    "\n",
    "It is easy to generate networks with SBM using igraph.\n",
    "For example, the assortativity communities can be generated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c68d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "\n",
    "p_in = 0.1\n",
    "p_out = 0.001\n",
    "block_sizes = [100, 200, 300]\n",
    "n = sum(block_sizes)\n",
    "\n",
    "pref_matrix = [\n",
    "    [p_in, p_out, p_out],\n",
    "    [p_out, p_in, p_out],\n",
    "    [p_out, p_out, p_in]\n",
    "]\n",
    "\n",
    "g = igraph.Graph.SBM(n, pref_matrix, block_sizes)\n",
    "\n",
    "# Plot the network\n",
    "import seaborn as sns\n",
    "palette = sns.color_palette()\n",
    "\n",
    "community_colors = sum([[palette[i]] * block_sizes[i] for i in range(len(block_sizes))], [])\n",
    "igraph.plot(g, vertex_color=community_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07bdbff",
   "metadata": {},
   "source": [
    "- `pref_matrix` is the matrix of connection probabilities between communities. Its $(i,j)$th-element is the probability of a connection between nodes in community $i$ and $j$.\n",
    "\n",
    "\n",
    "## Detecting communities with SBM\n",
    "\n",
    "Imagine you're a detective trying to figure out how a network was created. You have a hunch about the community structure, and you want to know if it matches the network you see. That's exactly what we're going to do to find out communities!\n",
    "\n",
    "Here's how we can describe the probability of seeing a particular network, given a community structure:\n",
    "\n",
    "$$\n",
    "P(\\left\\{A_{ij}\\right\\}_{ij}) = \\prod_{i<j} P(A_{ij}=1|c_i, c_j)^{A_{ij}} (1-P(A_{ij}=1|c_i, c_j))^{1-A_{ij}}\n",
    "$$\n",
    "\n",
    "Let's break this down into simpler terms:\n",
    "\n",
    "- First, $\\left\\{A_{ij}\\right\\}_{ij}$ is just a fancy way of saying \"all the connections in our network\". Think of it as a big table showing who's connected to whom.\n",
    "\n",
    "- We use $\\prod_{i < j}$ instead of $\\prod_{i,j}$ because we're dealing with an undirected network. This means if Alice is friends with Bob, Bob is also friends with Alice. We only need to count this friendship once, not twice!\n",
    "\n",
    "- The last part, $P(A_{ij}=1|c_i, c_j)^A_{ij}(1-P(A_{ij}=1|c_i, c_j))^{1-A_{ij}}$, might look scary, but it's actually quite clever. It's a shorthand way of saying \"what's the chance of this connection existing or not existing?\" If the connection exists ($A_{ij}=1$), we use the first part. If it doesn't ($A_{ij}=0$), we use the second part. It's a two-in-one formula.\n",
    "\n",
    "Here's a neat trick we can use to make our lives easier. We can take the logarithm of both sides of our equation. This turns our big product (multiplication) into a simpler sum (addition).\n",
    "\n",
    "$$\n",
    "{\\cal L}=\\log P(\\left\\{A_{ij}\\right\\}_{ij}) = \\sum_{i<j} A_{ij} \\log P(A_{ij}=1|c_i, c_j) + (1-A_{ij}) \\log (1-P(A_{ij}=1|c_i, c_j))\n",
    "$$\n",
    "\n",
    "We call this the **likelihood function**. It tells us how likely we are to see this network given our community guess. We can play around with different community assignments and edge probabilities to see which one gives us the highest likelihood.\n",
    "To make this game easier, let's first figure out the best edge probabilities for a given community assignment.\n",
    "\n",
    "Our likelihood function has a special shape - it is *a concave function* with respect to $p_{c,c'}$. This means that the likelihood function is a hill with only one peak when we look at it in terms of edge probability $p_{c,c'}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32cd96",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def concave_function(x):\n",
    "    return -(x - 0.5)**2 + 0.25\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = concave_function(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, 'b-', linewidth=2)\n",
    "plt.title('Schematic of Likelihood Function (Concave)')\n",
    "plt.xlabel('Edge Probability p_c,c\\'')\n",
    "plt.ylabel('Likelihood')\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Maximum')\n",
    "plt.annotate('Global Maximum', xy=(0.5, 0.25), xytext=(0.6, 0.2),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e0115",
   "metadata": {},
   "source": [
    "So, what does this mean for us? The top of this hill (our maximum value) is flat, and there's only one flat spot on the whole hill. So if we can find a spot where the hill isn't sloping at all (that's what we mean by \"zero gradient\"), we've found the very top of the hill! 🏔️\n",
    "\n",
    "In math terms, we take the derivative of our likelihood function with respect to $p_{c,c'}$ and set it to zero, i.e., $\\partial {\\cal L}  / \\partial p_{cc'} = 0$. Here is what we get:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial {\\cal L}}{\\partial p_{c,c'}} &= 0 \\\\\n",
    "\\Rightarrow & \\sum_{i<j} \\left[A_{ij} \\frac{1}{p_{c_i,c_j}} \\delta(c_i,c)\\delta(c_j,c') -(1-A_{ij}) \\frac{1}{1-p_{c_i,c_j}}\\delta(c_i,c')\\delta(c_j,c') \\right] = 0 \\\\\n",
    "\\Rightarrow &\n",
    "\\frac{m_{cc'}}{p_{c_i,c_j}} - \\frac{\\sum_{i < j} \\delta(c_i,c)\\delta(c_j,c') }{1-p_{c_i,c_j}} = 0 & \\text{if } c \\neq  c' \\\\\n",
    "\\Rightarrow & p_{c,c'} = \\frac{m_{cc'}}{\\sum_{i < j} \\delta(c_i,c)\\delta(c_j,c')}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's break down these equations:\n",
    "\n",
    "- $m_{cc'}$ is the number of edges between nodes in community $c$ and those in community $c'$.\n",
    "- The derivative $\\partial \\log p_{cc} / \\partial p_{cc}$ is just $1/p_{cc}$.\n",
    "\n",
    "The denominator $\\sum_{i < j} \\delta(c_i,c)\\delta(c_j,c')$ is the total number of pairs of nodes that belong to communities $c$ and $c'$. It is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\sum_{i < j} \\delta(c_i,c)\\delta(c_j,c') =\n",
    "\\begin{cases}\n",
    "n_cn_{c'} & \\text{if } c \\neq c' \\\\\n",
    "\\frac{n_c (n_c - 1)}{2} & \\text{if } c = c'\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Why do we have two different equations for $p_{c,c'}$? It's because we are counting each pair of nodes only by once. It is easy to verify when looking at the adjacency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db2bb4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import igraph\n",
    "\n",
    "# Generate SBM\n",
    "n, k = 900, 3\n",
    "\n",
    "# Create block sizes (equal for simplicity)\n",
    "block_sizes = [n // k] * k\n",
    "\n",
    "# Create diverse pref matrix\n",
    "pref_matrix = [\n",
    "    [0.3, 0.05, 0.1],\n",
    "    [0.05, 0.4, 0.02],\n",
    "    [0.1, 0.02, 0.35]\n",
    "]\n",
    "\n",
    "# Generate SBM using igraph\n",
    "g = igraph.Graph.SBM(n, pref_matrix, block_sizes)\n",
    "\n",
    "# Convert to adjacency matrix for visualization\n",
    "A = np.array(g.get_adjacency().data)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot the adjacency matrix\n",
    "ax.matshow(A, cmap='binary')\n",
    "mask = np.triu(np.ones_like(A, dtype=bool), k=1)\n",
    "\n",
    "# Highlight the upper triangle with yellow overlay\n",
    "ax.matshow(np.ma.masked_array(np.ones_like(A), ~mask), cmap='Reds_r', alpha=0.3)\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Adjacency Matrix with Highlighted Upper Triangle\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41883257",
   "metadata": {},
   "source": [
    "The upper triangle of the adjacency matrix represents $i < j$ over which we take the sum.\n",
    "When $c=c'$ (the diagonal block), we count only the upper half of the block, resulting in $\\frac{n_c (n_c - 1)}{2}$. When $c \\neq c'$ (different communities), we count all connections between them, resulting in $n_cn_{c'}$.\n",
    "\n",
    "We have now obtaind the likelihood function based only on the community assignment. Maximizing ${\\cal L}$ with respect to the community assignment gives us the most likely community assignment for the network.\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# What is community?\n",
    "\n",
    "## Birds of a feather flock together\n",
    "\n",
    "![Birds of a feather](https://t4.ftcdn.net/jpg/08/10/89/17/360_F_810891701_xy4NsqgdqllMfKDfV6V27ycrw8FLFqrw.jpg)\n",
    "\n",
    "Birds of a feather flock together, and so do many other things.\n",
    "For instance, we have a group of friends with similar interests who hang out together frequently but may not interact as much with other groups.\n",
    "\n",
    "In networks, communities are groups of nodes that share similar connection patterns. These communities do not always mean densely-connected nodes. Sometimes, a community can be nodes that are not connected to each other, but connect similarly to other groups. For instance, in a user-movie rating network, a community might be users with similar movie tastes, even if they don't directly connect to each other.\n",
    "\n",
    "![Community structure in a social network](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Network_Community_Structure.svg/220px-Network_Community_Structure.svg.png)\n",
    "\n",
    "Communities reflect underlying mechanisms of network formation and underpin the dynamics of information propagation. Examples include:\n",
    "\n",
    "1. Homophily: The tendency of similar nodes to form connections.\n",
    "2. Functional groups: Nodes that collaborate for specific purposes.\n",
    "3. Hierarchical structure: Smaller communities existing within larger ones.\n",
    "4. Information flow: The patterns of information, influence, or disease propagation through the network.\n",
    "\n",
    "This is why network scientists are sooo obsessed with community structure in networks. See {footcite}`fortunato2010community,fortunato2016community,peixoto2019bayesian` for comprehensive reviews on network communities.\n",
    "\n",
    "## References\n",
    "\n",
    "```{footbibliography}\n",
    "```\n",
    "# Module 5: Clustering\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn community detection, one of the most widely-used yet controversial techniques in network analysis. We will learn:\n",
    "- What is community structure in networks?\n",
    "- How to operationalize community structure?\n",
    "- How to find communities in networks?\n",
    "- Limitations of community detection\n",
    "- **Keywords**: community detection, assortativity, modularity, resolution limit, rugged landscape, random graph, label switching algorithm, Louvain algorithm, stochastic block model, the configuration model.# Assignment\n",
    "\n",
    "We will compute the various centrality measures for airport networks.\n",
    "\n",
    "- **For students enrolled in SSIE 641**\n",
    "  - You will receive a dedicated link to the assignment repository from the instructor.\n",
    "- *For those who are not enrolled in SSIE 641*\n",
    "  - You can access the assignment repository at [Github](https://github.com/sk-classroom/adv-net-sci-centrality).\n",
    "  - This repository does not offer auto-grading. But you can grade the assignment by yourself by\n",
    "    - `bash grading-toolkit/grade_notebook.sh tests/test_01.py assignment/assignment.ipynb`\n",
    "    - `bash grading-toolkit/grade_notebook.sh tests/test_02.py assignment/assignment.ipynb`\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# What is centrality?\n",
    "\n",
    "Have you ever wondered who the most popular person in your school is? Or which idea is the most important in a subject? Or maybe which movie everyone's talking about right now?\n",
    "These questions are all about finding out what's important in a network of people, ideas, or things. In network science, we call this *centrality.*\n",
    "\n",
    "Centrality or *importance* is a question of how important a node is in a network.\n",
    "But the notion of *importance* is somewhat vague.\n",
    "In what sense we say a node is important?\n",
    "Answering this question needs a specific *context*, and there are many contexts in which the *importance* is defined.\n",
    "\n",
    "![](../figs/centrality.jpg)\n",
    "\n",
    "## Different centrality measures\n",
    "\n",
    "Here we will focus on several popular centrality measures. Let us denote by $c_i$ the centrality of node $i$ throughout this section.\n",
    "Here is a preview of the centrality measures we will cover in this section\n",
    "\n",
    "| Centrality          | Category            | Description                                                                 |\n",
    "|-------------------------|---------------------|-----------------------------------------------------------------------------|\n",
    "| Degree Centrality       | Degree              | Counts the number of edges connected to a node.                             |\n",
    "| Closeness Centrality    | Shortest Path  | Measures how close a node is to all other nodes in the network.             |\n",
    "| Eccentricity Centrality | Shortest Path  | Based on the maximum shortest path distance from a node to any other node.  |\n",
    "| Harmonic Centrality     | Shortest Path  | Adjusts closeness centrality to work even in disconnected networks.         |\n",
    "| Betweenness Centrality  | Shortest Path  | Measures the extent to which a node lies on paths between other nodes.      |\n",
    "| Eigenvector Centrality  | Walk           | Measures a node's influence based on the influence of its neighbors.        |\n",
    "| HITS (Hub and Authority) Centrality | Walk  | Measures the importance of nodes as hubs and authorities in a network.      |\n",
    "| Katz Centrality         | Walk           | Considers the total number of walks between nodes, with a damping factor.   |\n",
    "| PageRank                | Walk           | Measures the importance of nodes based on the structure of incoming links.  |\n",
    "\n",
    "\n",
    "### Degree centrality\n",
    "\n",
    "Perhaps the simplest form of cnetrality is *degree centrality*. It is just the count of the number of edges connected to a node (i.e., the number of neighbors, or *degree* in network science terminology). The most important node is thus the one with the highest degree.\n",
    "\n",
    "$$\n",
    "c_i = d_i = \\sum_{j} A_{ij}\n",
    "$$\n",
    "\n",
    "where $A_{ij}$ is the adjacency matrix of the network, and $d_i$ is the degree of node $i$.\n",
    "\n",
    "### Centrality based on shortest path\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Milliarium_Aureum_op_het_Forum_Romanum_te_Rome_Columna_Miliaria_in_Foro_Romano_%28titel_op_object%29_Antieke_monumenten_%28serietitel%29_Antiquae_Urbis_Splendor_%28serietitel%29%2C_RP-P-2016-345-28-1.jpg/1546px-thumbnail.jpg?20191217151048\" alt=\"Roma Foro Romano Miliarium Aureum\" width=\"80%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "\n",
    "Let's talk about an ancient Roman monument called the *Milliarium Aureum*, also known as the *Golden Milestone*.\n",
    "It was the starting point for measuring distances on all major roads in the Roman Empire.\n",
    "Emperor Augustus built it when Rome changed from a republic to an empire.\n",
    "The monument not only marked the distances but also represent a centralization of power, where Rome transitioned from a Republic to an Empire.\n",
    "Perhaps the Romans understood the importance of being central in terms of distance, and this concept can be applied to define *centrality* in networks.\n",
    "\n",
    "### Closeness centrality\n",
    "**Closenes centrality** is a measure of how close a node is to all other nodes in the network. A node is central if it is close to all other nodes, which is operationally defined as\n",
    "\n",
    "$$\n",
    "c_i = \\frac{N - 1}{\\sum_{j = 1}^N \\text{shortest path length from } j \\text{ to } i}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of nodes in the network. The numerator, $N - 1$, is the normalization factor to make the centrality have a maximum value of 1.\n",
    "\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "Create a graph where a node has the maximum closeness centrality of value 1.\n",
    "\n",
    "\n",
    "```{dropdown} Click to see the answer\n",
    "\n",
    "The simplest example is a star graph, where one node is connected to all other nodes. The node at the center has the highest closeness centrality.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Star_network_7.svg/1920px-Star_network_7.svg.png\" alt=\"Star graph S7\" width=\"50%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Harmonic centrality\n",
    "\n",
    "**Harmonic Centrality** is a measure that adjusts closeness centrality to work even in disconnected networks. The problem with closeness centrality is that it cannot handle disconnected networks. When a network is disconnected, some nodes can't reach others, making their distance infinite. This causes all centrality values to become zero, which isn't very helpful!\n",
    "\n",
    "To fix this, Beauchamp {footcite:p}`beauchamp1965improved` came up with a clever solution called *harmonic centrality*. It works even when the network is disconnected.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j\\neq i} \\frac{1}{\\text{shortest path length from } j \\text{ to } i}\n",
    "$$\n",
    "\n",
    "### Eccentricity centrality\n",
    "\n",
    "**Eccentricity centrality** is baesd on the farthest distance from a node to any other node. The eccentricity centrality is defined as\n",
    "\n",
    "$$\n",
    "c_i = \\frac{1}{\\max_{j} \\text{shortest path length from } i \\text{ to } j}\n",
    "$$\n",
    "\n",
    "\n",
    "These centrality measures provide different perspectives on the importance of nodes based on their accessibility and reachability within the network.\n",
    "\n",
    "A central node should be close to all other nodes.\n",
    "\n",
    "Closeness centrality captures the notion of \"centrality\" in the network. Namely, a node is *central* if it is close to all other nodes.\n",
    "\n",
    "$$\n",
    "c_i = \\frac{N - 1}{\\sum_{j = 1}^N \\text{shortest path length from } j \\text{ to } i}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of nodes in the network. The numerator, $N$, is the normalization factor to make the centrality to have the maximum value of 1.\n",
    "\n",
    "\n",
    "### Eccentricity centrality\n",
    "\n",
    "Eccentricity centrality is based on the shortest path distance between nodes, just like the closeness centrality, but it is based on the *maximum* distance as opposed to the average distance like in the closeness centrality.\n",
    "\n",
    "$$\n",
    "c_i = \\frac{1}{\\max_{j} \\text{shortest path length from } i \\text{ to } j}\n",
    "$$\n",
    "\n",
    "\n",
    "### Betweenness centrality\n",
    "\n",
    "Another notion of centrality is *betweenness centrality*. It considers that a node is important if it lies on many shortest paths between other nodes.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j < k} \\frac{\\sigma_{jk}(i)}{\\sigma_{jk}}\n",
    "$$\n",
    "\n",
    "where $\\sigma_{jk}$ is the number of shortest paths between nodes $j$ and $k$, and $\\sigma_{jk}(i)$ is the number of shortest paths between nodes $j$ and $k$ that pass through node $i$.\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Centralities based on centralities\n",
    "\n",
    "\"A man is known by the company he keeps\" is a quote from Aesop who lived in the ancient Greece, a further back in time from the Roman Empire.\n",
    "It suggests that a person's character is reflected by the people this person is friends with.\n",
    "This idea can be applied to define the *centrality* of a node in a network.\n",
    "\n",
    "## Eigenvector centrality\n",
    "\n",
    "One considers that a node is important if it is connected to other important nodes. Yes, it sounds like circular! But it is actually computable! Let us define it more precisely by the following equation.\n",
    "\n",
    "$$\n",
    "c_i = \\lambda \\sum_{j} A_{ij} c_j\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is a constant. It suggests that the centrality of a node ($c_i$) is the sum of the centralities of its neighbors ($A_{ij} c_j$; note that $A_{ij}=1$ if $j$ is a neighbor, and otherwise $A_{ij}=0$), normalized by $\\lambda$.\n",
    "Using vector notation, we can rewrite the equation as\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix} = \\lambda\n",
    "\\begin{bmatrix}\n",
    "A_{11} & A_{12} & \\cdots & A_{1n} \\\\\n",
    "A_{21} & A_{22} & \\cdots & A_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{n1} & A_{n2} & \\cdots & A_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "\\vdots \\\\\n",
    "c_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\lambda \\mathbf{A} \\mathbf{c}\n",
    "$$\n",
    "\n",
    "Okay, but how do we solve this? Well, this is actually the eigenvector equation! And the solution to this equation is the eigenvector of the adjacency matrix, $\\mathbf{A}$. But here's the tricky part - there are multiple eigenvectors. So which one should we choose?\n",
    "\n",
    "Let's think about it for a moment. We want our centrality measure to be positive. It wouldn't make much sense to have negative importance! So, we're looking for an eigenvector where all the elements are positive. And a good news is that there's a special eigenvector that fits the bill perfectly.\n",
    "Perron-Frobenius theorem guarantees that the eigenvector associted with the largest eigenvalue always has all positive elements.\n",
    "\n",
    "This centrality is called **Eigenvector centrality**.\n",
    "\n",
    "\n",
    "## Hyperlink-Induced Topic Search (HITS) centrality\n",
    "\n",
    "Eigenvector centrality has several problems. One is that it does not handle directed networks very well.\n",
    "A natural extension of eigenvector centrality to directed networks is the HITS centrality.\n",
    "It introduces two notions of importance: *hub* and *authority*. A node is an important hub if it points to many important *authorities*. A node is an important authority if it is pointed by many important *hubs*.\n",
    "\n",
    "Let's put on a math hat to concretely define the hub and authority centralities.\n",
    "We introduce two vectors, $x_i$ and $y_i$, to denote the hub and authority centralities of node $i$, respectively. Following the idea of eigenvector centrality, we can define the hub and authority centralities as follows:\n",
    "\n",
    "$$\n",
    "x_i = \\lambda_x \\sum_j A_{ji} y_j, \\quad y_i = \\lambda_y \\sum_j A_{ij} x_j\n",
    "$$\n",
    "\n",
    "Or equivalently,\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\lambda_x \\mathbf{A}^T \\mathbf{y}, \\quad \\mathbf{y} = \\lambda_y \\mathbf{A} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "Substituting $\\mathbf{y} = \\lambda_y \\mathbf{A} \\mathbf{x}$ into the first equation and similar for $\\mathbf{x}$, we get\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\lambda_x \\mathbf{A}^T \\mathbf{A} \\mathbf{x}, \\quad \\mathbf{y} = \\lambda_y \\mathbf{A} \\mathbf{A}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "Again, we obtain the eigenvector equations whose solutions are the eigenvectors of $\\mathbf{A}^T \\mathbf{A}$ and $\\mathbf{A} \\mathbf{A}^T$ for $\\mathbf{x}$ and $\\mathbf{y}$, respectively.\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "If the original network is *undirected*, is the HITS centrality equivalent to the eigenvector centrality? If so or not, explain why.\n",
    "\n",
    "```{dropdown} Click to see the answer\n",
    "If the graph is undirected, the hub and authority centralities are equivalent. And the solution is given by the eigenvector of $\\mathbf{A} \\mathbf{A}^T$. Now, let us consider the eigenvector equation for the adjacency matrix $\\mathbf{A}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\lambda \\mathbf{A} \\mathbf{c}\n",
    "$$\n",
    "\n",
    "By multiplying $\\mathbb{A}$ on the both sides, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{A} \\mathbf{c} &= \\lambda \\mathbf{A} \\mathbf{A} \\mathbf{c} \\\\\n",
    "\\iff \\mathbf{A}^\\top \\mathbf{A} \\mathbf{c} &= \\lambda \\mathbf{A}^\\top \\mathbf{c} \\\\\n",
    "\\iff \\mathbf{A}^\\top \\mathbf{A} \\mathbf{c} &= \\lambda^2 \\mathbf{c}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we used the fact that $\\mathbf{A}$ is symmetric. It suggests that the eigenvector of $\\mathbf{A}^\\top \\mathbf{A}$ is the same as that of $\\mathbf{A}$, and that the eigenvalue of $\\mathbf{A}^\\top \\mathbf{A}$ is the square of that of $\\mathbf{A}$.\n",
    "Thus, the eigenvector centrality is equivalent to the HITS centrality if the network is undirected.\n",
    "\n",
    "```\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "Consider the case where the graph is undirected and we normalize the hub centrality by the degree $d_j$ of the authority, namely\n",
    "\n",
    "$$\n",
    "x_i = \\sum_j \\frac{A_{ji}}{d_j} y_j,\\quad y_i = \\sum_j A_{ij} x_j\n",
    "$$\n",
    "\n",
    "\n",
    "Then we will get the hub centrality equivalent to the degree centrality. Confirm this by substituting $x_i = d_i$.\n",
    "```\n",
    "\n",
    "## Katz centrality\n",
    "\n",
    "Eigenvector centrality tends to pay too much attention to a small number of nodes that are well connected to the network while under-emphasizing the importance of the rest of the nodes. A solution is to add a little bit of score to all nodes.\n",
    "\n",
    "$$\n",
    "c_i = \\beta + \\lambda \\sum_{j} A_{ij} c_j\n",
    "$$\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: tip\n",
    "\n",
    "Derive the solution of the Katz centrality.\n",
    "\n",
    "```{dropdown} Click to see the answer\n",
    "\n",
    "The equation can be solved by\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\beta \\mathbf{1} + \\lambda \\mathbf{A} \\mathbf{c}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{1}$ is the vector of ones. By rewriting the equation, we get\n",
    "\n",
    "$$\n",
    "\\left( \\mathbf{I} - \\lambda \\mathbf{A} \\right) \\mathbf{c} = \\beta \\mathbf{1}\n",
    "$$\n",
    "\n",
    "By taking the inverse of $\\mathbf{I} - \\lambda \\mathbf{A}$, we get\n",
    "\n",
    "$$\n",
    "\\mathbf{c} = \\beta \\left( \\mathbf{I} - \\lambda \\mathbf{A} \\right)^{-1} \\mathbf{1}\n",
    "$$\n",
    "```\n",
    "\n",
    "## PageRank\n",
    "\n",
    "You've probably heard PageRank, a celebrated idea behind Google Search. It is like a cousin of Katz centrality.\n",
    "\n",
    "$$\n",
    "c_i = (1-\\beta) \\sum_j A_{ji}\\frac{c_j}{d^{\\text{out}}_j} + \\beta \\cdot \\frac{1}{N}\n",
    "$$\n",
    "\n",
    "where $d^{\\text{out}}_j$ is the out-degree of node $j$ (the number of edges pointing out from node $j$).\n",
    "The term $c_j/d^{\\text{out}}_j$ represents that the score of node $j$ is divided by the number of nodes to which node $j$ points. In Web, this is like a web page distributes its score to the web pages it points to. It is based on an idea of traffic, where the viewers of a web page are evenly transferred to the linked web pages. A web page is important if it has a high traffic of viewers.\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupyter:\n",
    "  jupytext:\n",
    "    text_representation:\n",
    "      extension: .md\n",
    "      format_name: markdown\n",
    "      format_version: '1.3'\n",
    "      jupytext_version: 1.16.3\n",
    "  kernelspec:\n",
    "    display_name: Python 3\n",
    "    language: python\n",
    "    name: python3\n",
    "---\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m06-centrality.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Computing centrality with Python\n",
    "\n",
    "## Network of university students\n",
    "\n",
    "Let's compute the centrality of the network using Python igraph.\n",
    "\n",
    "```python\n",
    "# Uncomment if you use Colab\n",
    "#!pip install igraph\n",
    "```\n",
    "\n",
    "```python\n",
    "import igraph\n",
    "names  = ['Sarah', 'Mike', 'Emma', 'Alex', 'Olivia', 'James', 'Sophia', 'Ethan', 'Ava', 'Noah', 'Lily', 'Lucas', 'Henry']\n",
    "edge_list = [(0, 1), (0, 2), (1, 2), (2, 3), (3, 4), (3, 5), (3, 6), (4, 5), (6, 7), (6, 8), (6, 9), (7, 8), (7, 9), (8, 9), (9, 10), (9, 11), (9, 12)]\n",
    "g = igraph.Graph()\n",
    "g.add_vertices(13)\n",
    "g.vs[\"name\"] = names\n",
    "g.add_edges(edge_list)\n",
    "igraph.plot(g, vertex_label=g.vs[\"name\"])\n",
    "```\n",
    "\n",
    "`igraph` offers a wide range of centrality measures as methods of the `igraph.Graph` class.\n",
    "\n",
    "- **Degree centrality**: `igraph.Graph.degree()`\n",
    "- **Closeness centrality**: `igraph.Graph.closeness()`\n",
    "- **Betweenness centrality**: `igraph.Graph.betweenness()`\n",
    "- **Harmonic centrality**: `igraph.Graph.harmonic_centrality()`\n",
    "- **Eccentricity**: `igraph.Graph.eccentricity()`\n",
    "- **Eigenvector centrality**: `igraph.Graph.eigenvector_centrality()`\n",
    "- **PageRank centrality**: `igraph.Graph.personalized_pagerank()`\n",
    "\n",
    "For example, the closeness centrality is computed by\n",
    "\n",
    "```python\n",
    "g.closeness()\n",
    "```\n",
    "\n",
    "### Computing Katz centrality\n",
    "\n",
    "Let's compute the Katz centrality without using igraph.\n",
    "Let us first define the adjacency matrix of the graph\n",
    "\n",
    "```python\n",
    "A = g.get_adjacency_sparse()\n",
    "```\n",
    "\n",
    "which is the scipy CSR sparse matrix. The Katz centrality is given by\n",
    "\n",
    "$$\n",
    "\n",
    "\\mathbf{c} = \\beta \\mathbf{1} + \\alpha \\mathbf{A} \\mathbf{c}\n",
    "\n",
    "$$\n",
    "\n",
    "So, how do we solve this? We can use a linear solver but here we will use a simple method:\n",
    "\n",
    "1. Initialize $\\mathbf{c}$ with a random vector.\n",
    "2. Compute the right hand side of the equation and update $\\mathbf{c}$.\n",
    "3. Repeat the process until $\\mathbf{c}$ converges.\n",
    "\n",
    "Let's implement this.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "alpha, beta = 0.1, 0.05 # Hyperparameters\n",
    "n_nodes = g.vcount() # number of nodes\n",
    "c = np.random.rand(n_nodes, 1) # column random vector\n",
    "\n",
    "for _ in range(100):\n",
    "    c_next = beta * np.ones((n_nodes, 1)) + alpha * A * c\n",
    "    if np.linalg.norm(c_next - c) < 1e-6:\n",
    "        break\n",
    "    c = c_next\n",
    "print(c)\n",
    "```\n",
    "\n",
    "- Does the centrality converge?\n",
    "- Change the hyperparameter and see how the result changes 😉\n",
    "If the centrality diverges, think about why it diverges.\n",
    "\n",
    "*Hint*: Katz centrality can be analytically computed by\n",
    "\n",
    "$$\n",
    "\n",
    "\\mathbf{c} = \\beta \\left(\\mathbf{I} -  \\alpha \\mathbf{A} \\right)^{-1} \\mathbf{1}\n",
    "\n",
    "$$\n",
    "\n",
    "### Exercise (Optional)\n",
    "\n",
    "Compute the PageRank centrality of this graph\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "## Network of ancient Roman roads\n",
    "\n",
    "### Load the data & construct the network\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "root = \"https://raw.githubusercontent.com/skojaku/adv-net-sci/main/data/roman-roads\"\n",
    "node_table = pd.read_csv(f\"{root}/node_table.csv\")\n",
    "edge_table = pd.read_csv(f\"{root}/edge_table.csv\")\n",
    "```\n",
    "\n",
    "The node table:\n",
    "\n",
    "```python\n",
    "node_table.head(3)\n",
    "```\n",
    "\n",
    "The edge table:\n",
    "\n",
    "```python\n",
    "edge_table.head(3)\n",
    "```\n",
    "\n",
    "Let's construct a network from the node and edge tables.\n",
    "\n",
    "```python\n",
    "import igraph\n",
    "\n",
    "g = igraph.Graph() # create an empty graph\n",
    "g.add_vertices(node_table[\"node_id\"].values) # add nodes\n",
    "g.add_edges(list(zip(edge_table[\"src\"].values, edge_table[\"trg\"].values))) # add edges\n",
    "```\n",
    "\n",
    "which looks like this:\n",
    "\n",
    "```python\n",
    "coord = list(zip(node_table[\"lon\"].values, -node_table[\"lat\"].values))\n",
    "igraph.plot(g, layout = coord, vertex_size = 5)\n",
    "```\n",
    "\n",
    "### Exercise 🏛️\n",
    "\n",
    "1. Compute the following centrality measures:\n",
    "    - Degree centrality 🔢\n",
    "    - Eigenvector centrality\n",
    "    - PageRank centrality\n",
    "    - Katz centrality\n",
    "    - Betweenness centrality\n",
    "    - Closeness centrality\n",
    "2. Plot the centrality measures on the map and see in which centrality Rome is the most important node. 🗺️🏛️ (as beautiful as possible!!)\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Pen and paper exercises\n",
    "\n",
    "-  [️️School ](./pen-and-paper/exercise.pdf)\n",
    "# Module 6: Centrality\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn centrality, one of the most widely-used yet controversial techniques in network analysis. We will learn:\n",
    "- What is centrality in networks?\n",
    "- How to operationalize centrality?\n",
    "- How to find centrality in networks?\n",
    "- Limitations of centrality\n",
    "- **Keywords**: degree centrality, closeness centrality, betweenness centrality, eigenvector centrality, PageRank, Katz centrality, HITS, random walk---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Ladder Lottery\n",
    "\n",
    "```{admonition} Ladder Lottery\n",
    ":class: tip\n",
    "\n",
    "Ladder Lottery is a fun East Asian game, also known as \"鬼腳圖\" (Guijiaotu) in Chinese, \"阿弥陀籤\" (Amida-kuzi) in Japanese, \"사다리타기\" (Sadaritagi) in Korean, and \"Ladder Lottery\" in English. The game is played as follows:\n",
    "1. A player is given a board with a set of vertical lines.\n",
    "2. The player chooses a line and starts to move along the line\n",
    "3. When hitting a horizontal line, the player must move along the horizontal line and then continue to move along the next vertical line.\n",
    "4. The player wins if the player can hit a marked line at the bottom of the board.\n",
    "5. You cannot see the horizontal lines in advance!\n",
    "\n",
    "Play the {{ '[Ladder Lottery Game! 🎮✨]( BASE_URL/vis/amida-kuji.html?)'.replace('BASE_URL', base_url) }} and try to answer the following questions:\n",
    "\n",
    "1. Is tehre a strategy to maximize the probability of winning?\n",
    "2. How does the probability of winning change as the number of horizontal lines increases?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/6/64/Amidakuji_2022-05-10.gif)\n",
    "\n",
    "```# Pen and paper exercises\n",
    "\n",
    "- [✍️ Pen and paper exercises](pen-and-paper/exercise.pdf)\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Random Walks in Python\n",
    "\n",
    "## Simulating Random Walks\n",
    "\n",
    "We will simulate random walks on a simple graph of five nodes as follows.\n",
    "\n",
    "```{code-cell} ipython3\n",
    "import numpy as np\n",
    "import igraph\n",
    "\n",
    "g = igraph.Graph()\n",
    "\n",
    "g.add_vertices([0, 1, 2, 3, 4])\n",
    "g.add_edges([(0, 1), (0, 2), (0, 3), (1, 3), (2, 3), (2, 4), (3, 4)])\n",
    "igraph.plot(g, vertex_size=20, vertex_label=g.vs[\"name\"])\n",
    "```\n",
    "\n",
    "A random walk is characterized by the transition probabilities between nodes.\n",
    "\n",
    "$$\n",
    "P_{ij} = \\frac{A_{ij}}{k_i}\n",
    "$$\n",
    "\n",
    "Let us first compute the transition probabilities and store them in a matrix, $\\mathbf{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1295aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = g.get_adjacency_sparse().toarray()\n",
    "k = np.array(g.degree())\n",
    "n_nodes = g.vcount()\n",
    "\n",
    "# A simple but inefficient way to compute P\n",
    "P = np.zeros((n_nodes, n_nodes))\n",
    "for i in range(n_nodes):\n",
    "    for j in range(n_nodes):\n",
    "        if k[i] > 0:\n",
    "            P[i, j] = A[i, j] / k[i]\n",
    "        else:\n",
    "            P[i, j] = 0\n",
    "\n",
    "# Alternative, more efficient way to compute P\n",
    "P = A / k[:, np.newaxis]\n",
    "\n",
    "# or even more efficiently\n",
    "P = np.einsum(\"ij,i->ij\", A, 1 / k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e779ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transition probability matrix:\\n\", P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91791047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(P, annot=True, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288bc16",
   "metadata": {},
   "source": [
    "Each row and column of $\\mathbf{P}$ corresponds to a node, with entries representing the transition probabilities from the row node to the column node.\n",
    "\n",
    "Now, let us simulate a random walk on this graph. We represent a position of the walker by a vector, $\\mathbf{x}$, with five elements, each of which represents a node. We mark the node that the walker is currently at by `1` and others as `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da281b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 0, 0, 0, 0])\n",
    "x[0] = 1\n",
    "print(\"Initial position of the walker:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca9048",
   "metadata": {},
   "source": [
    "This vector representation is convenient to get the probabilities of transitions to other nodes from the current node:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\mathbf{P}\n",
    "$$\n",
    "\n",
    "which is translated into the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = x @ P\n",
    "print(\"Position of the walker after one step:\\n\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e06f3",
   "metadata": {},
   "source": [
    "We can then draw the next node based on the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_node = np.random.choice(n_nodes, p=probs)\n",
    "x[:] = 0 # zero out the vector\n",
    "x[next_node] = 1 # set the next node to 1\n",
    "print(\"Position of the walker after one step:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ff84b",
   "metadata": {},
   "source": [
    "By repeating this process, we can simulate the random walk.\n",
    "\n",
    "### Exercise 01\n",
    "\n",
    "Write the following function to simulate the random walk for a given number of steps and return the $x$ for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceffdef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(A, n_steps):\n",
    "    \"\"\"\n",
    "    Simulate the random walk on a graph with adjacency matrix A.\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): The adjacency matrix of the graph.\n",
    "        x (np.ndarray): The initial position of the walker.\n",
    "        n_steps (int): The number of steps to simulate.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The position of the walker after each step.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ca78a",
   "metadata": {},
   "source": [
    "## Expected behavior of random walks\n",
    "\n",
    "What is the expected position of the walker after multiple steps? It is easy to compute the expected position of the walker after one step from initial position $x(0)$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x(1)] = x(0) P\n",
    "$$\n",
    "\n",
    "where $x(t)$ is the probability distribution of the walker at time $t$. In Python, the expected position of the walker at time $t=1$ is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array([1, 0, 0, 0, 0])\n",
    "x_1 = x_0 @ P\n",
    "print(\"Expected position of the walker after one step:\\n\", x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8a6a6",
   "metadata": {},
   "source": [
    "For the second step, the expected position of the walker is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x(2)] = \\mathbb{E}[x(1) P] = \\mathbb{E}[x(0) P] P = x(0) P^2\n",
    "$$\n",
    "\n",
    "In other words,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7383f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = x_1 @ P\n",
    "print(\"Expected position of the walker after two steps:\\n\", x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1bd046",
   "metadata": {},
   "source": [
    "Following the same argument, the expected position of the walker at time $t$ is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[x(t)] = x(0) P^t\n",
    "$$\n",
    "\n",
    "### Exercise 02\n",
    "\n",
    "Write a function to compute the expected position of the walker at time $t$ using the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_position(A, x_0, t):\n",
    "    \"\"\"\n",
    "    Compute the expected position of the walker at time t.\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): The adjacency matrix of the graph.\n",
    "        x_0 (np.ndarray): The initial position of the walker.\n",
    "        t (int): The number of steps to simulate.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2c530",
   "metadata": {},
   "source": [
    "### Exercise 03\n",
    "\n",
    "Plot each element of $x(t)$ as a function of $t$ for $t=0,1,2,\\ldots, 1000$. Try different initial positions and compare the results!\n",
    "\n",
    "Steps:\n",
    "1. Define the initial position of the walker.\n",
    "2. Compute the expected position of the walker at time $t$ using the function you wrote above.\n",
    "3. Draw a line for each element of $x(t)$, totalling 5 lines.\n",
    "4. Create multiple such plots for different initial positions and compare them.\n",
    "\n",
    "## Community structure\n",
    "\n",
    "Random walks can capture community structure of a network.\n",
    "To see this, let us consider a network of a ring of cliques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import igraph\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_cliques = 3\n",
    "n_nodes_per_clique = 5\n",
    "\n",
    "G = nx.ring_of_cliques(n_cliques, n_nodes_per_clique)\n",
    "g = igraph.Graph().Adjacency(nx.to_numpy_array(G).tolist()).as_undirected()\n",
    "membership = np.repeat(np.arange(n_cliques), n_nodes_per_clique)\n",
    "\n",
    "color_map = [sns.color_palette()[i] for i in membership]\n",
    "igraph.plot(g, vertex_size=20, vertex_color=color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea9ceab",
   "metadata": {},
   "source": [
    "Let us compute the expected position of the walker after 1 to 10 steps.\n",
    "\n",
    "**Compute the transition matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820711fb",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# Get the adjacency matrix and degree\n",
    "A = g.get_adjacency_sparse()\n",
    "k = np.array(g.degree())\n",
    "\n",
    "# This is an efficient way to compute the transition matrix\n",
    "# using scipy.sparse\n",
    "P = sparse.diags(1 / k) @ A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e952b",
   "metadata": {},
   "source": [
    "**Compute the expected position of the walker after 1 to 300 steps**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19f92d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "x_t = np.zeros(g.vcount())\n",
    "x_t[2] = 1\n",
    "x_list = [x_t]\n",
    "for t in range(300):\n",
    "    x_t = x_t @ P\n",
    "    x_list.append(x_t)\n",
    "x_list = np.array(x_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b50ff",
   "metadata": {},
   "source": [
    "**Plot the expected position of the walker at each step**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab5fc7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15,10), ncols = 3, nrows = 2)\n",
    "\n",
    "t_list = [0, 1, 3, 5, 10, 299]\n",
    "for i, t in enumerate(t_list):\n",
    "    igraph.plot(g, vertex_size=20, vertex_color=[cmap(x_list[t][j] / np.max(x_list[t])) for j in range(g.vcount())], target = axes[i//3][i%3])\n",
    "    axes[i//3][i%3].set_title(f\"$t$ = {t}\", fontsize = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241118c",
   "metadata": {},
   "source": [
    "where the color of each node represents the probability of the walker being at that node.\n",
    "\n",
    "An important observation is that the walker spends more time in the clique that it started from and then diffuse to others. Thus, the position of the walker before reaching the steady state tells us the community structure of the network.\n",
    "\n",
    "## Exercise 04\n",
    "\n",
    "1. Generate a network of 100 nodes with 4 communities using a stochastic block model, with inter-community edge probability $0.05$ and intra-community edge probability $0.2$. Then, compute the expected position of the walker starting from node zero after $x$ steps. Plot the results for $x = 0, 5, 10, 1000$.\n",
    "\n",
    "2. Increase the inter-community edge probability to $0.15$ and repeat the simulation. Compare the results with the previous simulation.\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Characteristics of Random Walks\n",
    "\n",
    "## Stationary State\n",
    "\n",
    "Let's dive into the math behind random walks in a way that's easy to understand.\n",
    "\n",
    "Imagine you're at node $i$ at time $t$. You randomly move to a neighboring node $j$. The probability of this move, called the transition probability $p_{ij}$, is:\n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{A_{ij}}{k_i},\n",
    "$$\n",
    "\n",
    "Here, $A_{ij}$ is an element of the adjacency matrix, and $k_i$ is the degree of node $i$. For a network with $N$ nodes, we can represent all transition probabilities in a transition probability matrix $P$:\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\begin{pmatrix}\n",
    "p_{11} & p_{12} & \\cdots & p_{1N} \\\\\n",
    "p_{21} & p_{22} & \\cdots & p_{2N} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "p_{N1} & p_{N2} & \\cdots & p_{NN}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This matrix $P$ encapsulates the entire random walk process. We can use it to calculate the probability of visiting each node after any number of steps. For instance:\n",
    "\n",
    "- After one step: $P_{ij} = p_{ij}$\n",
    "- After two steps: $\\left(\\mathbf{P}^{2}\\right)_{ij} = \\sum_{k} P_{ik} P_{kj}$\n",
    "- After $T$ steps: $\\left(\\mathbf{P}^{T}\\right)_{ij}$\n",
    "\n",
    "```{note}\n",
    "Let's explore why $\\mathbf{P}^2$ represents the transition probabilities after two steps.\n",
    "\n",
    "First, recall that $\\mathbf{P}_{ij}$ is the probability of moving from node $i$ to node $j$ in one step. Now, consider a two-step walk from $i$ to $j$. We can express this as:\n",
    "\n",
    "$$(\\mathbf{P}^2)_{ij} = \\sum_k \\mathbf{P}_{ik} \\mathbf{P}_{kj}$$\n",
    "\n",
    "This equation encapsulates a key idea: to go from $i$ to $j$ in two steps, we must pass through some intermediate node $k$. Let's break this down step by step:\n",
    "\n",
    "1. The probability of the first step ($i$ to $k$) is $\\mathbf{P}_{ik}$.\n",
    "2. The probability of the second step ($k$ to $j$) is $\\mathbf{P}_{kj}$.\n",
    "3. The probability of this specific path ($i$ → $k$ → $j$) is the product $\\mathbf{P}_{ik} \\mathbf{P}_{kj}$.\n",
    "4. We sum over all possible intermediate nodes $k$ to get the total probability.\n",
    "\n",
    "Likewise, for three steps, we have:\n",
    "\n",
    "$$(\\mathbf{P}^3)_{ij} = \\sum_k \\left( \\mathbf{P}\\right)^2_{ik} \\mathbf{P}_{kj}$$\n",
    "\n",
    "where:\n",
    "1. The probability of going from $i$ to $k$ in two steps is $\\left( \\mathbf{P}\\right)^2_{ik}$.\n",
    "2. The probability of going from $k$ to $j$ in one step is $\\mathbf{P}_{kj}$.\n",
    "3. The probability of this specific path ($i$ →...→$k$ → $j$) is the product $\\left( \\mathbf{P}\\right)^2_{ik} \\mathbf{P}_{kj}$.\n",
    "4. We sum over all possible intermediate nodes $k$ to get the total probability.\n",
    "\n",
    "And we can extend this reasoning for any number of steps $t$.\n",
    "\n",
    "In summary, for any number of steps $t$, $\\left( \\mathbf{P}^t \\right)_{ij}$ gives the probability of being at node $j$ after $t$ steps, starting from node $i$.\n",
    "\n",
    "```\n",
    "\n",
    "As $T$ becomes very large, the probability distribution of being at each node, $\\mathbf{x}(t)$, approaches a constant value:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t+1) =\\mathbf{x}(t) \\mathbf{P}\n",
    "$$\n",
    "\n",
    "This is an eigenvector equation. The solution, given by the Perron-Frobenius theorem, is called the stationary distribution:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(\\infty) = \\mathbb{\\pi}, \\; \\mathbf{\\pi} = [\\pi_1, \\ldots, \\pi_N]\n",
    "$$\n",
    "\n",
    "For undirected networks, this stationary distribution always exists and is proportional to the degree of each node:\n",
    "\n",
    "$$\n",
    "\\pi_j = \\frac{k_j}{\\sum_{\\ell} k_\\ell} \\propto k_j\n",
    "$$\n",
    "\n",
    "This means the probability of being at node $j$ in the long run is proportional to the degree of node $j$. The normalization ensures that the sum of all probabilities is 1, i.e., $\\sum_{j=1}^N \\pi_j = 1$.\n",
    "\n",
    "\n",
    "## Experiment\n",
    "\n",
    "Let us demonstrate the above math by using a small network using Python. Let us consider a small network of 5 nodes, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f861e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "edge_list = []\n",
    "for i in range(5):\n",
    "    for j in range(i+1, 5):\n",
    "        edge_list.append((i, j))\n",
    "        edge_list.append((i+5, j+5))\n",
    "edge_list.append((0, 6))\n",
    "\n",
    "g = ig.Graph(edge_list)\n",
    "ig.plot(g, vertex_size=20, vertex_label=np.arange(g.vcount()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64238a1",
   "metadata": {},
   "source": [
    "The transition probability matrix $P$ is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca292bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "A = g.get_adjacency_sparse()\n",
    "deg = np.array(A.sum(axis=1)).flatten()\n",
    "Dinv = sparse.diags(1/deg)\n",
    "P = Dinv @ A\n",
    "P.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bb6f3",
   "metadata": {},
   "source": [
    "Let us compute the stationary distribution by using the power method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd208a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "x = np.zeros(g.vcount())\n",
    "x[1] = 1 # Start from node 1\n",
    "T = 100\n",
    "xt = []\n",
    "for t in range(T):\n",
    "    x = x.reshape(1, -1) @ P\n",
    "    xt.append(x)\n",
    "\n",
    "xt = np.vstack(xt) # Stack the results vertically\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "palette = sns.color_palette().as_hex()\n",
    "for i in range(g.vcount()):\n",
    "    sns.lineplot(x=range(T), y=xt[:, i], label=f\"Node {i}\", ax=ax, color=palette[i])\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_title(\"Stationary distribution of a random walk\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41cc53",
   "metadata": {},
   "source": [
    "We see that the distributions of the walker converges, and there are three characteristic features in the convergence:\n",
    "1. The distribution of the walker occilates with a decying amplitude and eventually converges.\n",
    "2. Nodes of the same degree converge to the same stationary probability.\n",
    "3. Nodes with higher degree converge to the higher stationary probability.\n",
    "\n",
    "To validate the last two observation, let us compare the stationary distribution of a random walker with the expected stationary distribution, which is proportional to the degree of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73818505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "n_edges = np.sum(deg) / 2\n",
    "expected_stationary_dist = deg / (2 * n_edges)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Expected stationary distribution\": expected_stationary_dist,\n",
    "    \"Stationary distribution of a random walk\": xt[-1].flatten()\n",
    "}).style.format(\"{:.4f}\").set_caption(\"Comparison of Expected and Observed Stationary Distributions\").background_gradient(cmap='cividis', axis = None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac8ee3",
   "metadata": {},
   "source": [
    "## Time to reach the stationary state\n",
    "\n",
    "Let's explore how quickly a random walker reaches its stationary state. The convergence speed is influenced by two main factors: edge density and community structure. In sparse networks, the walker needs more steps to explore the entire network. Additionally, the walker tends to remain within its starting community for some time.\n",
    "\n",
    "The mixing time, denoted as $t_{\\text{mix}}$, is defined as the minimum number of steps required for a random walk to get close to the stationary distribution, regardless of the starting point, with the maximum error less than $\\epsilon$:\n",
    "\n",
    "$$t_{\\text{mix}} = \\min\\{t : \\max_{{\\bf x}(0)} \\|{\\bf x}(t) - {\\bf \\pi}\\|_{1} \\leq \\epsilon\\}$$\n",
    "\n",
    "where $\\|{\\bf x}(t) - {\\bf \\pi}\\|_{1} = 2\\max_{i} |x_i(t) - \\pi_i|$ represents the L1 distance between two probability distributions. The choice of $\\epsilon$ is arbitrary.\n",
    "\n",
    "We know that the distribution of a walker after $t$ steps is given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) =  \\mathbf{x}(0) \\mathbf{P} ^{t}\n",
    "$$\n",
    "\n",
    "To find this distribution, we need to compute $\\mathbf{P}^t$. However, we face a challenge: $\\mathbf{P}$ is not diagonalizable.\n",
    "\n",
    "A diagonalizable matrix $\\mathbf{S}$ can be written as $\\mathbf{S} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}$, where $\\mathbf{\\Lambda}$ is a diagonal matrix and $\\mathbf{Q}$ is an orthogonal matrix. Visually, it looks like this:\n",
    "\n",
    "![](../figs/diagonalizable.jpg)\n",
    "\n",
    "It is useful because we can then compute the power of the matrix as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{S}^t = \\mathbf{Q} \\mathbf{\\Lambda}^t \\mathbf{Q}^{-1}\n",
    "$$\n",
    "\n",
    "And it is easy to find ${\\bf Q}$ and ${\\bf \\Lambda}$ by using eigenvalue decomposition if ${\\bf S}$ is symmetric and consists only of real values. Namely, the eigenvectors form ${\\cal Q}$ and the eigenvalues form the diagonal matrix ${\\cal \\Lambda}$.\n",
    "\n",
    "```{note}\n",
    "Let us demonstrate the above relation by calculating $\\mathbf{S}^2$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{S}^2 &= \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1} \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1} \\\\\n",
    "&= \\mathbf{Q} \\mathbf{\\Lambda}^2 \\mathbf{Q}^{-1}.\n",
    "\\end{align}\n",
    "$$\n",
    "(Note that $\\mathbf{Q} \\mathbf{Q}^{-1} = {\\bf I}$.)\n",
    "\n",
    "![](../figs/diagonalizable-squared.jpg)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "$\\mathbf{P}$ is also diagonalizable but not symmetric like $\\mathbf{\\overline A}$ so that we cannot use the above relation directly. So we do a trick by rewriteing $\\mathbf{P}$ as:\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\mathbf{D}^{-1} \\mathbf{A} = \\mathbf{D}^{-\\frac{1}{2}} \\overline {\\bf A} \\mathbf{D}^{\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "where $\\overline{\\bf A} = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}$ is the normalized adjacency matrix.\n",
    "\n",
    "The advantage is that $\\overline{\\bf A}$ is diagonalizable: $\\overline{\\bf A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top$. Using this, we can compute $\\mathbf{P}^t$:\n",
    "\n",
    "$$\n",
    "\\mathbf{P}^t = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{Q} \\mathbf{\\Lambda}^t \\mathbf{Q}^\\top \\mathbf{D}^{\\frac{1}{2}} = \\mathbf{Q}_L \\mathbf{\\Lambda}^t \\mathbf{Q}_R ^\\top\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Q}_L = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{Q}$ and $\\mathbf{Q}_R = \\mathbf{D}^{\\frac{1}{2}} \\mathbf{Q}$.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Let us demonstrate the above relation by calculating $\\mathbf{P}^2$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{P}^2 &= \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} \\mathbf{D}^{\\frac{1}{2}} \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} \\mathbf{D}^{\\frac{1}{2}}\\\\\n",
    "&=  \\mathbf{D}^{-\\frac{1}{2}} \\overline{\\bf A} ^2 \\mathbf{D}^{\\frac{1}{2}}\\\\\n",
    "&= \\mathbf{Q}_L \\mathbf{\\Lambda}^2 \\mathbf{Q}_R ^\\top\n",
    "\\end{align}\n",
    "```\n",
    "\n",
    "The probability distribution after $t$ steps is then:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t) = \\mathbf{x}(0) \\mathbf{Q}_L \\mathbf{\\Lambda}^t \\mathbf{Q}_R ^\\top\n",
    "$$\n",
    "\n",
    "We can rewrite this in a more intuitive form:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x_1(t) \\\\\n",
    "x_2(t) \\\\\n",
    "\\vdots \\\\\n",
    "x_N(t)\n",
    "\\end{pmatrix}\n",
    " =\n",
    " \\sum_{\\ell=1}^N\n",
    " \\left[\n",
    " \\lambda_\\ell^t\n",
    " \\begin{pmatrix}\n",
    " q^{(L)}_{\\ell 1} \\\\\n",
    " q^{(L)}_{\\ell 2} \\\\\n",
    " \\vdots \\\\\n",
    " q^{(L)}_{\\ell N}\n",
    " \\end{pmatrix}\n",
    " \\langle\\mathbf{q}^{(R)}_{\\ell},  \\mathbf{x}(0) \\rangle\n",
    " \\right]\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "Visualize the above equation by using the following figure.\n",
    "\n",
    "![](../figs/diagonalizable-sum.jpg)\n",
    "\n",
    "```\n",
    "\n",
    "The term $\\lambda_\\ell^t$ represents the contribution of each eigenvalue to the stationary distribution over time. As $t$ increases, all terms decay exponentially except for the largest eigenvalue ($\\lambda_1 = 1$). This explains how the random walk converges to the stationary distribution:\n",
    "\n",
    "$$\n",
    "\\pi_i = \\lim_{t\\to\\infty} x_i(t) = \\begin{pmatrix} q^{(L)}_{1 1} \\\\ q^{(L)}_{1 2} \\\\ \\vdots \\\\ q^{(L)}_{1 N} \\end{pmatrix} \\langle\\mathbf{q}^{(R)}_{1},  \\mathbf{x}(0) \\rangle\n",
    "$$\n",
    "\n",
    "The second largest eigenvalue primarily determines the convergence speed to the stationary distribution. A larger second eigenvalue leads to slower convergence. Thus, the mixing time is closely related to the second largest eigenvalue.\n",
    "\n",
    "Levin-Peres-Wilmer theorem states that the mixing time is bounded by the relaxation time as\n",
    "\n",
    "$$\n",
    "t_{\\text{mix}} < \\tau \\log \\left( \\frac{1}{\\epsilon \\min_{i} \\pi_i} \\right), \\quad \\tau = \\frac{1}{\\lambda_2}\n",
    "$$\n",
    "\n",
    "where $\\lambda_2$ is the second largest eigenvalue of the normalized adjacency matrix. The mixing time is known to be bounded by the relaxation time as\n",
    "\n",
    "More commonly, it is expressed using the second smallest eigenvalue $\\mu$ of the normalized laplacian matrix as\n",
    "\n",
    "$$\n",
    "t_{\\text{mix}} \\leq \\frac{1}{1-\\mu}\n",
    "$$\n",
    "\n",
    "where $\\mu = 1-\\lambda_2$.\n",
    "\n",
    "\n",
    "### Compute the mixing time\n",
    "\n",
    "Let us demonstrate the above math by using the network of two cliques.\n",
    "\n",
    "#### Normalized Adjacency Matrix\n",
    "\n",
    "First, let us construct the normalized adjacency matrix $\\overline{\\bf A}$ of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578966d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dinv_sqrt = sparse.diags(1.0/np.sqrt(deg))\n",
    "A_norm = Dinv_sqrt @ A @ Dinv_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42399e50",
   "metadata": {},
   "source": [
    "Next, let us compute the eigenvalues and eigenvectors of the normalized adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eigh(A_norm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88506653",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`evals` and `evecs` are sorted in descending order of the eigenvalues. `evecs[:, 0]` is the eigenvector corresponding to the largest eigenvalue, which is always 1.\n",
    "```\n",
    "\n",
    "```{warning}\n",
    "There is a similar function called `np.linalg.eig` which returns the eigenvalues and eigenvectors. It can be used for any matrices, while `np.linalg.eigh` is specifically for symmetric matrices. `np.linalg.eigh` is faster and more stable and therefore preferred if your matrix is symmetric. `np.linalg.eig` is more susceptible to numerical errors and therefore less stable.\n",
    "```\n",
    "\n",
    "The eigenvalues and eigenvectors are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1062d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"Eigenvalue\": evals\n",
    "}).T.style.background_gradient(cmap='cividis', axis = 1).set_caption(\"Eigenvalues of the normalized adjacency matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"Eigenvector %i\" % i: evecs[:, i]\n",
    "    for i in range(10)\n",
    "}).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Eigenvectors of the normalized adjacency matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398bda1",
   "metadata": {},
   "source": [
    "Notice that the largest eigenvalue is 1, which is always true for a normalized adjacency matrix.\n",
    "The largest eigenvector (the leftmost one) is associated with the stationary distribution of the random walk.\n",
    "\n",
    "```{note}\n",
    "The sign of the eigenvector is indeterminate, which means we can choose the sign of the eigenvector arbitrarily. In fact, `np.linalg.eigh` returns the eigenvector whose sign can vary for a different run.\n",
    "```\n",
    "\n",
    "We decompose $\\overline{\\bf A}$ as\n",
    "\n",
    "$$\\overline {\\bf A} = {\\bf Q}{\\bf \\Lambda}{\\bf Q}^\\top$$\n",
    "\n",
    "where ${\\bf Q}$ corresponds to `eigvecs` and ${\\bf \\Lambda}$ corresponds to `np.diag(evals)` (since ${\\bf \\Lambda}$ is a diagonal matrix). Let's see if this is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc616d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(A_norm.toarray()).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Normalized Adjacency Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dedb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_norm_reconstructed = evecs @ np.diag(evals) @ evecs.T\n",
    "pd.DataFrame(A_norm_reconstructed).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Reconstruction of the Normalized Adjacency Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d79af",
   "metadata": {},
   "source": [
    "Notice that the reconstruction is not perfect due to the numerical error, although overall the structure is correct.\n",
    "\n",
    "#### Multi-step Transition Probability\n",
    "\n",
    "Let us first conform whether we can compute the transition probability after $t$ steps by using the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b258b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5\n",
    "x_0 = np.zeros(g.vcount())\n",
    "x_0[0] = 1\n",
    "\n",
    "# Compute x_t by using the eigenvalues and eigenvectors\n",
    "Q_L = np.diag(1.0/np.sqrt(deg)) @ evecs\n",
    "Q_R = np.diag(np.sqrt(deg)) @ evecs\n",
    "x_t = x_0 @ Q_L @ np.diag(evals**t) @ Q_R.T\n",
    "\n",
    "# Compute x_t by using the power iteration\n",
    "x_t_power = x_0.copy()\n",
    "for i in range(t):\n",
    "    x_t_power = x_t_power @ P\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Eigenvector\": x_t.flatten(),\n",
    "    \"Power iteration\": x_t_power.flatten()\n",
    "}).style.background_gradient(cmap='cividis', axis = None).set_caption(\"Comparison of Eigenvector and Power Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff301e",
   "metadata": {},
   "source": [
    "#### Relaxation Time and Mixing Time\n",
    "\n",
    "Let us measure the relaxation time of the random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eigh(A_norm.toarray())\n",
    "lambda_2 = -np.sort(-evals)[1]\n",
    "tau = 1 / lambda_2\n",
    "print(f\"The relaxation time of the random walk is {tau:.4f}.\")\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Random Walks\n",
    "\n",
    "Suppose you walk in a city. You are drunk and your feet have no idea where to go. You just take a step wherever your feet take you. At every intersection, you make a random decision and take a step. This is the core idea of a random walk.\n",
    "\n",
    "While your feet are taking you to a random street, after making many steps and looking back, you will realize that you have been to certain places more frequently than others. If you were to map the frequency of your visits to each street, you will end up with a distribution that tells you about salient structure of the street network. It is surprising that this seemingly random, brainless behavior can tell us something deep about the structure of the city.\n",
    "\n",
    "\n",
    "<img src=\"../figs/random-walk.png\" alt=\"Random walk on a network\" width=\"50%\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "## Random walks in a network\n",
    "\n",
    "A random walk in undirected networks is the following process:\n",
    "1. Start at a node $i$\n",
    "2. Randomly choose an edge to traverse to a neighbor node $j$\n",
    "3. Repeat step 2 until you have taken $T$ steps.\n",
    "\n",
    "\n",
    "```{figure-md} random-walk-example\n",
    "\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/a56ca795f324f75baab70bb3b49e0544c89e05f7/2-Figure1-1.png\" alt=\"Random walk example\" width=\"100%\">\n",
    "\n",
    "Random walk on a small network. The figure is taken from Li, Xing et al. “Representation Learning of Reconstructed Graphs Using Random Walk Graph Convolutional Network.” ArXiv abs/2101.00417 (2021).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e3226a",
   "metadata": {},
   "source": [
    "```{note}\n",
    "In case of directed networks, a random walker can only move along the edge direction, and it can be that the random walker is stuck in a so-called ``dead end'' that does not have any outgoing edges.\n",
    "```\n",
    "\n",
    "How does this simple process tell us something about the network structure? To get some insights, let us play with a simple interactive visualization.\n",
    "\n",
    "```{admonition} Random Walk Simulation\n",
    ":class: tip\n",
    "\n",
    "Play with the {{ '[Random Walk Simulator! 🎮✨]( BASE_URL/vis/random-walks/index.html?)'.replace('BASE_URL', base_url) }} and try to answer the following questions:\n",
    "\n",
    "1. When the random walker makes many steps, where does it tend to visit most frequently?\n",
    "2. When the walker makes only a few steps, where does it tend to visit?\n",
    "3. Does the behavior of the walker inform us about centrality of the nodes?\n",
    "3. Does the behavior of the walker inform us about communities in the network?\n",
    "\n",
    "```\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Random walks unify centrality and communities\n",
    "\n",
    "## Modularity: Interpretation from random walk perspective\n",
    "\n",
    "Modularity can be intepreted as a random walk perspective. Modularity is given by\n",
    "\n",
    "$$\n",
    "Q = \\frac{1}{2m} \\sum_{ij} \\left( A_{ij} - \\frac{d_i d_j}{2m} \\right) \\delta(c_i, c_j)\n",
    "$$\n",
    "\n",
    "where $m$ is the number of edges in the network, $A_{ij}$ is the adjacency matrix, $d_i$ is the degree of node $i$, $c_i$ is the community of node $i$, and $\\delta(c_i, c_j)$ is the Kronecker delta function (which is 1 if $c_i = c_j$ and 0 otherwise).\n",
    "\n",
    "We can rewrite the modularity using the language of random walks as follows.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q &= \\sum_{ij} \\left(\\frac{A_{ij}}{2m}  - \\frac{d_i}{2m} \\frac{d_j}{2m} \\right) \\delta(c_i, c_j) \\\\\n",
    "&= \\sum_{ij} \\left(\\pi_i P_{ij}  - \\pi_i \\pi_j \\right) \\delta(c_i, c_j)\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\pi_i$ is the stationary distribution of the random walk given by\n",
    "\n",
    "$$\n",
    "\\pi_i = \\frac{d_i}{2m}\n",
    "$$\n",
    "\n",
    "and $P_{ij}$ is the transition probability between nodes $i$ and $j$.\n",
    "\n",
    "```{note}\n",
    "Let's break down this derivation step by step:\n",
    "\n",
    "1. We start with the original modularity formula:\n",
    "\n",
    "   $$Q = \\frac{1}{2m} \\sum_{ij} \\left( A_{ij} - \\frac{d_i d_j}{2m} \\right) \\delta(c_i, c_j)$$\n",
    "\n",
    "2. First, we move the constant $1/(2m)$ to the inside of the parentheses:\n",
    "\n",
    "   $$Q = \\sum_{ij} \\left(\\frac{A_{ij}}{2m} - \\frac{d_i d_j}{2m^2} \\right) \\delta(c_i, c_j)$$\n",
    "\n",
    "3. Now, we recognize that $\\frac{A_{ij}}{2m}$ can be rewritten as:\n",
    "\n",
    "   $$\\frac{A_{ij}}{2m} = \\frac{d_i}{2m} \\cdot \\frac{A_{ij}}{d_i} = \\pi_i P_{ij}$$\n",
    "\n",
    "4. We also recognize that $\\frac{d_i}{2m}$ is the stationary distribution of the random walk, which we denote as $\\pi_i$:\n",
    "\n",
    "   $$\\frac{d_i}{2m} = \\pi_i$$\n",
    "\n",
    "5. Substituting these into our equation:\n",
    "\n",
    "   $$Q = \\sum_{ij} \\left(\\pi_i P_{ij} - \\pi_i \\pi_j \\right) \\delta(c_i, c_j)$$\n",
    "\n",
    "```\n",
    "\n",
    "The expression suggests that:\n",
    "\n",
    "1. The first term, $\\pi_i P_{ij} \\delta(c_i, c_j)$, represents the probability that a walker is at node $i$ and moves to node $j$ within the same community **by one step**.\n",
    "2. The second term, $\\pi_i \\pi_j$, represents the probability that a walker is at node $i$ and moves to another node $j$ within the same community **after long steps**.\n",
    "\n",
    "In summary, modularity compares short-term and long-term random walk probabilities. High modularity indicates that a random walker is more likely to stay within the same community after one step than after many steps.\n",
    "\n",
    "```{note}\n",
    "Building on this perspective from random walks, Delvenne et al. {footcite}`delvenne2010stability` extends the modularity by comparing multi-step and long-step transition probabilities of a random walk. This approach, known as \"Markov stability\", shows that the number of steps acts as a \"resolution parameter\" that determines the scale of detectable communities.\n",
    "```\n",
    "\n",
    "\n",
    "## PageRank: Interpretation from random walk perspective\n",
    "\n",
    "PageRank can be interpreted from a random walk perspective:\n",
    "\n",
    "$$\n",
    "c_i = (1-\\beta) \\sum_j P_{ji} c_j + \\beta \\cdot \\frac{1}{N}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $c_i$ is the PageRank of node $i$\n",
    "- $P_{ji}$ is the transition probability from node $j$ to node $i$\n",
    "- $\\beta$ is the teleportation probability\n",
    "- $N$ is the total number of nodes\n",
    "\n",
    "This equation represents a random walk where:\n",
    "1. With probability $(1-\\beta)$, the walker follows a link to the next node.\n",
    "2. With probability $\\beta$, the walker *teleports* to a random node in the network.\n",
    "\n",
    "The PageRank $c_i$ is the stationary distribution of this random walk, representing the long-term probability of finding the walker at node $i$.\n",
    "\n",
    "```{note}\n",
    "This sounds odd at first glance. But it makes sense when you think about what PageRank was invented for, i.e., Web search. It characterizes a web surfer as a random walker that chooses the next page by randomly jumping to a random page with probability $\\beta$ or by following a link to a page with probability $1-\\beta$. The web page with the largest PageRank means that the page is most likely to be visited by this random web surfer.\n",
    "```\n",
    "\n",
    "```{footbibliography}\n",
    "\n",
    "```# Module 7: Random Walks\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn random walks, one of the most fundamental techniques in network analysis. We will learn:\n",
    "- What is a random walk?\n",
    "- How to simulate a random walk on a network?\n",
    "- What is the behavior of a random walk on a network?\n",
    "- Implicit connections to community detection and network centralities\n",
    "- **Keywords**: random walk, community detection, network centralities---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Appendix\n",
    "\n",
    "## Spectral Embedding with the Adjacency Matrix\n",
    "\n",
    "The spectral embedding with the adjacency matrix is given by the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{U}} J(\\mathbf{U}),\\quad J(\\mathbf{U}) = \\| \\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top \\|_F^2\n",
    "$$\n",
    "\n",
    "We will approach the solution step by step based on the following steps:\n",
    "\n",
    "1. We start taking a derivative of $J(\\mathbf{U})$  with respect to $\\mathbf{U}$.\n",
    "2. We then set the derivative to zero (i.e., $\\nabla J(\\mathbf{U}) = 0$) and solve for $\\mathbf{U}$.\n",
    "\n",
    "1. Expand the Frobenius norm:\n",
    "\n",
    "   The Frobenius norm for any matrix $\\mathbf{M}$ is defined as:\n",
    "\n",
    "   $\\|\\mathbf{M}\\|_F^2 = \\sum_{i,j} M_{ij}^2 = \\text{Tr}(\\mathbf{M}\\mathbf{M}^\\top)$\n",
    "\n",
    "   Applying this to our problem:\n",
    "\n",
    "   $J(\\mathbf{U}) = \\|\\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top\\|_F^2 = \\text{Tr}[(\\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top)(\\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top)^\\top]$\n",
    "\n",
    "   Expanding this:\n",
    "\n",
    "   $= \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top - 2\\mathbf{A}\\mathbf{U}\\mathbf{U}^\\top + \\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}\\mathbf{U}^\\top)$\n",
    "\n",
    "2. Take the derivative with respect to $\\mathbf{U}$:\n",
    "\n",
    "   Using matrix calculus rules:\n",
    "\n",
    "   $\\frac{\\partial \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top)}{\\partial \\mathbf{U}} = 0$\n",
    "\n",
    "   $\\frac{\\partial \\text{Tr}(\\mathbf{A}\\mathbf{U}\\mathbf{U}^\\top)}{\\partial \\mathbf{U}} = 2\\mathbf{A}\\mathbf{U}$\n",
    "\n",
    "   $\\frac{\\partial \\text{Tr}(\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}\\mathbf{U}^\\top)}{\\partial \\mathbf{U}} = 4\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "   Combining these:\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial \\mathbf{U}} = -4\\mathbf{A}\\mathbf{U} + 4\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "   Simplifying:\n",
    "\n",
    "   $\\frac{\\partial J}{\\partial \\mathbf{U}} = -2\\mathbf{A}\\mathbf{U} + 2\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "3. Set the derivative to zero and solve:\n",
    "\n",
    "   $-2\\mathbf{A}\\mathbf{U} + 2\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U} = 0$\n",
    "\n",
    "   $\\mathbf{A}\\mathbf{U} = \\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}$\n",
    "\n",
    "4. This equation is satisfied when $\\mathbf{U}$ consists of eigenvectors of $\\mathbf{A}$:\n",
    "\n",
    "   Assume $\\mathbf{U}$ consists of eigenvectors of $\\mathbf{A}$:\n",
    "\n",
    "   $\\mathbf{A}\\mathbf{U} = \\mathbf{U}\\mathbf{\\Lambda}$\n",
    "\n",
    "   where $\\mathbf{\\Lambda}$ is a diagonal matrix of eigenvalues.\n",
    "\n",
    "   Since eigenvectors are orthonormal:\n",
    "\n",
    "   $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$\n",
    "\n",
    "   Therefore:\n",
    "\n",
    "   $\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{U}$\n",
    "\n",
    "   This shows our equation is satisfied when $\\mathbf{U}$ consists of eigenvectors of $\\mathbf{A}$.\n",
    "\n",
    "5. To minimize $J(\\mathbf{U})$, choose the eigenvectors corresponding to the $d$ largest eigenvalues.\n",
    "\n",
    "   To understand why, consider the trace of our objective function:\n",
    "\n",
    "   $J(\\mathbf{U}) = \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top) - 2\\text{Tr}(\\mathbf{A}\\mathbf{U}\\mathbf{U}^\\top) + \\text{Tr}(\\mathbf{U}\\mathbf{U}^\\top\\mathbf{U}\\mathbf{U}^\\top)$\n",
    "\n",
    "   Since $\\mathbf{U}$ is orthogonal ($\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$), and trace is invariant under cyclic permutations, we can simplify:\n",
    "\n",
    "   $J(\\mathbf{U}) = \\text{Tr}(\\mathbf{A}\\mathbf{A}^\\top) - \\text{Tr}(\\mathbf{U}^\\top\\mathbf{A}\\mathbf{U})$\n",
    "\n",
    "   Let $\\mathbf{U} = [\\mathbf{u}_1, ..., \\mathbf{u}_d]$ be the eigenvectors of $\\mathbf{A}$ with corresponding eigenvalues $\\lambda_1 \\geq ... \\geq \\lambda_d$. Then:\n",
    "\n",
    "   $\\text{Tr}(\\mathbf{U}^\\top\\mathbf{A}\\mathbf{U}) = \\sum_{i=1}^d \\lambda_i$\n",
    "\n",
    "   To minimize $J(\\mathbf{U})$, maximize $\\sum_{i=1}^d \\lambda_i$ by selecting the eigenvectors corresponding to the $d$ largest eigenvalues.\n",
    "\n",
    "The result is the collection of the $d$ eigenvectors corresponding to the $d$ largest eigenvalues, and it is one form of the spectral embedding.\n",
    "\n",
    "\n",
    "## The proof of the Laplacian Eigenmap\n",
    "\n",
    "The Laplacian Eigenmap is given by the following optimization problem:\n",
    "\n",
    "$$\n",
    "J_{LE}(\\mathbf{U}) = \\text{Tr}(\\mathbf{U}^\\top \\mathbf{L} \\mathbf{U})\n",
    "$$\n",
    "\n",
    "The step where we rewrite $J_{LE}(\\mathbf{U})$ as $\\text{Tr}(\\mathbf{U}^\\top \\mathbf{L} \\mathbf{U})$ is crucial for leveraging matrix derivatives. Let's break down this transformation step by step:\n",
    "\n",
    "1. First, we rewrite $\\mathbf{U}$ by column vectors:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{U} =\n",
    "   \\begin{bmatrix}\n",
    "   \\vert & \\vert & & \\vert \\\\\n",
    "   \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_d \\\\\n",
    "   \\vert & \\vert & & \\vert\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   where $\\mathbf{x}_i$ is the $i$-th column of $\\mathbf{U}$.\n",
    "\n",
    "2. We can expand the loss function $J_{LE}(\\mathbf{U})$:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\sum_{i} \\sum_{j} L_{ij} u_i^\\top u_j = \\sum_{i} \\sum_{j} \\sum_{d'} L_{ij} u_{i,d'} u_{j,d'}\n",
    "   $$\n",
    "\n",
    "3. Rearranging the order of summation:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\sum_{d'} \\sum_{i} \\sum_{j} L_{ij} u_{i,d'} u_{j,d'}\n",
    "   $$\n",
    "\n",
    "4. We can rewrite this as a matrix multiplication for each $d'$:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\sum_{d'} \\mathbf{x}_{d'}^\\top \\mathbf{L} \\mathbf{x}_{d'}\n",
    "   $$\n",
    "\n",
    "   where $\\mathbf{x}_{d'}$ is the $d'$-th column of $\\mathbf{U}$.\n",
    "\n",
    "5. Finally, we can express this as a trace:\n",
    "\n",
    "   $$\n",
    "   J_{LE}(\\mathbf{U}) = \\text{Tr}(\\mathbf{U}^\\top \\mathbf{L} \\mathbf{U})\n",
    "   $$\n",
    "\n",
    "This final form expresses our objective function in terms of matrix operations, which allows us to use matrix calculus to find the optimal solution. The trace representation is a useful technique to leverage matrix calculus.---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Graph embedding with word2vec\n",
    "\n",
    "How can we apply word2vec to graph data? There is a critical challenge: word2vec takes sequence of words as input, while graph data are discrete and unordered. A solution to fill this gap is *random walk*, which transforms graph data into a sequence of nodes. Once we have a sequence of nodes, we can treat it as a sequence of words and apply word2vec.\n",
    "\n",
    "\n",
    "## DeepWalk\n",
    "\n",
    "![](https://dt5vp8kor0orz.cloudfront.net/7c56c256b9fbf06693da47737ac57fae803a5a4f/1-Figure1-1.png)\n",
    "\n",
    "DeepWalk is one of the pioneering works to apply word2vec to graph data {footcite}`perozzi2014deepwalk`. It views the nodes as words and the nodes random walks on the graph as sentences, and applies word2vec to learn the node embeddings.\n",
    "\n",
    "More specifically, the method contains the following steps:\n",
    "\n",
    "1. Sample multiple random walks from the graph.\n",
    "2. Treat the random walks as sentences and feed them to word2vev to learn the node embeddings.\n",
    "\n",
    "\n",
    "There are some technical details that we need to be aware of, which we will learn by implementing DeepWalk in the following exercise.\n",
    "\n",
    "### Exercise 01: Implement DeepWalk\n",
    "\n",
    "In this exercise, we implement DeepWalk step by step.\n",
    "\n",
    "#### Step 1: Data preparation\n",
    "\n",
    "We will use the karate club network as an example.\n",
    "\n",
    "**Load the data**\n",
    "```{code-cell} ipython3\n",
    ":tags: [hide-input]\n",
    "\n",
    "import igraph\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "g = igraph.Graph.Famous(\"Zachary\")\n",
    "A = g.get_adjacency_sparse()\n",
    "\n",
    "# Add the community labels to the nodes for visualization\n",
    "g.vs[\"label\"] = np.unique([d[1]['club'] for d in nx.karate_club_graph().nodes(data=True)], return_inverse=True)[1]\n",
    "\n",
    "palette = sns.color_palette().as_hex()\n",
    "igraph.plot(g, vertex_color=[palette[label] for label in g.vs[\"label\"]], bbox=(300, 300))\n",
    "```\n",
    "\n",
    "#### Step 2: Generate random walks\n",
    "\n",
    "Next, we generate the training data for the word2vec model by generating multiple random walks starting from each node in the network.\n",
    "Let us first implement a function to sample random walks from a given network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1987093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(net, start_node, walk_length):\n",
    "    # Initialize the walk with the starting node\n",
    "    walk = [start_node]\n",
    "\n",
    "    # Continue the walk until the desired length is reached\n",
    "    while len(walk) < walk_length:\n",
    "        # Get the current node (the last node in the walk)\n",
    "        cur = walk[-1]\n",
    "\n",
    "        # Get the neighbors of the current node\n",
    "        cur_nbrs = list(net[cur].indices)\n",
    "\n",
    "        # If the current node has neighbors, randomly choose one and add it to the walk\n",
    "        if len(cur_nbrs) > 0:\n",
    "            walk.append(np.random.choice(cur_nbrs))\n",
    "        else:\n",
    "            # If the current node has no neighbors, terminate the walk\n",
    "            break\n",
    "\n",
    "    # Return the generated walk\n",
    "    return walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd198be7",
   "metadata": {},
   "source": [
    "Generate 10 random walks of length 50 starting from each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = g.vcount()\n",
    "n_walkers_per_node = 10\n",
    "walk_length = 50\n",
    "walks = []\n",
    "for i in range(n_nodes):\n",
    "    for _ in range(n_walkers_per_node):\n",
    "        walks.append(random_walk(A, i, walk_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f202318",
   "metadata": {},
   "source": [
    "#### Step 3: Train the word2vec model\n",
    "Then, we feed the random walks to the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbfcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(walks, vector_size=32, window=3, min_count=1, sg=1, hs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3d5c4",
   "metadata": {},
   "source": [
    "Here,\n",
    "\n",
    "- `vector_size` is the dimension of the embedding vectors.\n",
    "- `window` indicates the maximum distance between a word and its context words. For example, in the random walk `[0, 1, 2, 3, 4, 5, 6, 7]`, the context words of node 2 are `[0, 1, 3, 4, 5]` when `window=3`.\n",
    "- `min_count` is the minimum number of times a word must appear in the training data to be included in the vocabulary.\n",
    "\n",
    "Two parameters `sg=1` and `hs=1` indicate that we are using the skip-gram model with negative sampling. Let us understand what they mean in detail as follows.\n",
    "\n",
    "- **Skip-gram model**: it trains word2vec by predicting context words given a target word. For example, given the sentence \"The quick brown fox jumps over the lazy dog\", in the skip-gram model, given the target word \"fox\", the model will try to predict the context words \"quick\", \"brown\", \"jumps\", and \"over\". If `sg=0`, the input and output are swapped: the model will predict the target word from the context words, e.g., given the context words \"quick\", \"brown\", \"jumps\", and \"over\", the model will predict the target word \"fox\".\n",
    "\n",
    "- **Hierarchical softmax**: To understand hierarchical softmax better, let's break down how the word2vec model works. The goal of word2vec is to predict context words given a target word. For example, if our target word is $w_t$ and our context word is $w_c$, we want to find the probability of $w_c$ given $w_t$. This probability is calculated using the softmax function:\n",
    "\n",
    "    $$\n",
    "    P(w_c | w_t) = \\frac{\\exp(\\mathbf{v}_{w_c} \\cdot \\mathbf{v}_{w_t})}{\\sum_{w \\in V} \\exp(\\mathbf{v}_w \\cdot \\mathbf{u}_{w_t})}\n",
    "   $$\n",
    "\n",
    "    Here, $\\mathbf{v}_w$ and $\\mathbf{u}_w$ represent the vector for word $w$ as context and target respectively, and $V$ is the entire vocabulary. The tricky part is the denominator, which requires summing over all words in the vocabulary. If we have a large vocabulary, this can be very computationally expensive. Imagine having to compute 100,000 exponentials and their sum for each training example if our vocabulary size is 100,000!\n",
    "\n",
    "    Hierarchical softmax helps us solve this problem. Instead of calculating the probability directly, it organizes the vocabulary into a binary tree, where each word is a leaf node. To find the probability of a word, we calculate the product of probabilities along the path from the root to the leaf node. This method significantly reduces the computational complexity. Instead of being proportional to the vocabulary size, it becomes proportional to the logarithm of the vocabulary size. This makes it much more efficient, especially for large vocabularies.\n",
    "\n",
    "    ![](https://lh5.googleusercontent.com/proxy/_omrC8G6quTl2SGarwFe57qzbIs-PtGkEA5yODFE5I0Ny2IHGiJwsUhMrcuUqg5o-R2nD9hkgMuZsQJKoCggP29zXtj-Vz-X8BE)\n",
    "\n",
    "\n",
    "By using the skip-gram model with hierarchical softmax, we can efficiently learn high-quality word embeddings even when dealing with large vocabularies.\n",
    "\n",
    "Now, we extract the node embeddings from the word2vec model. In the word2vec model, the embeddings are stored in the `wv` attribute. The embedding of node $i$ is given by `model.wv[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af74c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = []\n",
    "for i in range(n_nodes):\n",
    "    embedding.append(model.wv[i])\n",
    "embedding = np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cccd9e",
   "metadata": {},
   "source": [
    "`embedding` is the matrix of node embeddings. It has the same number of rows as the number of nodes in the network, and the number of columns is the embedding dimension.\n",
    "\n",
    "**Print the first 3 nodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf454e09",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "embedding[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61687d",
   "metadata": {},
   "source": [
    "Let's visualize the node embeddings using UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a55628",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, metric=\"cosine\")\n",
    "xy = reducer.fit_transform(embedding)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Calculate the degree of each node\n",
    "degrees = A.sum(axis=1).A1\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    size=np.sqrt(degrees / np.max(degrees)) * 30,\n",
    "    community=[palette[label] for label in g.vs[\"label\"]]\n",
    "))\n",
    "\n",
    "p = figure(title=\"Node Embeddings from Word2Vec\", x_axis_label=\"X\", y_axis_label=\"Y\")\n",
    "\n",
    "p.scatter('x', 'y', size='size', source=source, line_color=\"black\", color=\"community\")\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b136b4c",
   "metadata": {},
   "source": [
    "#### Step 4: Clustering\n",
    "\n",
    "One of the interesting applications with node embeddings is clustering. While we have good community detection methods, like the modularity maximization and stochastic block model, we can use clustering methods from machine learning, such as $K$-means and Gaussian mixture model. Let's see what we can get from the node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2684a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Determine the optimal number of clusters using the silhouette score\n",
    "def Kmeans_with_silhouette(embedding, n_clusters_range=(2, 10)):\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Iterate over a range of cluster numbers from 2 to 9\n",
    "    for n_clusters in range(*n_clusters_range):\n",
    "        # Create a KMeans object with the current number of clusters\n",
    "        kmeans = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "        # Fit the KMeans model to the embedding data\n",
    "        kmeans.fit(embedding)\n",
    "\n",
    "        # Calculate the silhouette score for the current clustering\n",
    "        score = silhouette_score(embedding, kmeans.labels_)\n",
    "\n",
    "        # Append the number of clusters and its corresponding silhouette score to the list\n",
    "        silhouette_scores.append((n_clusters, score))\n",
    "\n",
    "    # Find the number of clusters that has the highest silhouette score\n",
    "    optimal_n_clusters = max(silhouette_scores, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Create a KMeans object with the optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_n_clusters)\n",
    "\n",
    "    # Fit the KMeans model to the embedding data with the optimal number of clusters\n",
    "    kmeans.fit(embedding)\n",
    "\n",
    "    # Return the labels (cluster assignments) for each data point\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ab77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "labels = Kmeans_with_silhouette(embedding)\n",
    "cmap = sns.color_palette().as_hex()\n",
    "igraph.plot(g, vertex_color=[cmap[label] for label in labels], bbox=(500, 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8f41fb",
   "metadata": {},
   "source": [
    "## node2vec\n",
    "\n",
    "node2vec is a sibling of DeepWalk proposed by {footcite}`grover2016node2vec`. Both use word2vec trained on random walks on networks. So, it appears that they are very similar. However, the following two components make them very different.\n",
    "\n",
    "- **Biased random walk**: node2vec uses biased random walks that can move in different directions. The bias walk is parameterized by two parameters, $p$ and $q$:\n",
    "\n",
    "    $$\n",
    "    P(v_{t+1} = x | v_t = v, v_{t-1} = t) \\propto\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{p} & \\text{if } d(v,t) = 0 \\\\\n",
    "    1 & \\text{if } d(v,t) = 1 \\\\\n",
    "    \\frac{1}{q} & \\text{if } d(v,t) = 2 \\\\\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "    where $d(v,x)$ is the shortest path distance between node $v$ and $x$. A smaller $p$ leads to more biased towards the previous node, $v_{t-1} = t$. A smaller $q$ leads to more biased towards the nodes that are further away from the previous node, $v_{t-1} = t$.\n",
    "\n",
    "    By adjusting the parameters $p$ and $q$, we can influence the random walk to behave more like either breadth-first sampling (BFS) or depth-first sampling (DFS).\n",
    "\n",
    "    - **Breadth-First Sampling (BFS)**: This type of sampling explores all the neighbors of a node before moving on to the next level of neighbors. It is useful for capturing community structures within the graph. When we set the parameters to favor BFS, the resulting embeddings will reflect these community structures.\n",
    "\n",
    "    - **Depth-First Sampling (DFS)**: This type of sampling goes deep into the graph, exploring as far as possible along each branch before backtracking. It is useful for capturing structural equivalence, where nodes that have similar roles in the graph (even if they are not directly connected) are represented similarly. When we set the parameters to favor DFS, the resulting embeddings will reflect these structural equivalences.\n",
    "\n",
    "    ![](https://www.researchgate.net/publication/354654762/figure/fig3/AS:1069013035655173@1631883977008/A-biased-random-walk-procedure-of-node2vec-B-BFS-and-DFS-search-strategies-from-node-u.png)\n",
    "\n",
    "    The embeddings generated by node2vec can capture different aspects of the graph depending on the sampling strategy used. With BFS, we capture community structures, and with DFS, we capture structural equivalence.\n",
    "\n",
    "    ![](https://miro.medium.com/v2/resize:fit:1138/format:webp/1*nCyF5jFSU5uJVdAPdf-0HA.png)\n",
    "\n",
    "- **Negative sampling**: node2vec uses negative sampling, instead of hierarchical softmax. This difference appears to be minor, but it has significant consequences on the characteristics of the embeddings. This is beyond the scope of this lecture, but you can refer to {footcite}`kojaku2021neurips` and {footcite}`dyer2014notes` for more details.\n",
    "\n",
    "\n",
    "### Exercise 02: Implement node2vec\n",
    "\n",
    "Let's implement the biased random walk for node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab8c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec_random_walk(net, start_node, walk_length, p, q):\n",
    "    \"\"\"\n",
    "    Sample a random walk starting from start_node.\n",
    "    \"\"\"\n",
    "    # Initialize the walk with the start_node\n",
    "    walk = [start_node]\n",
    "\n",
    "    # Continue the walk until it reaches the desired length\n",
    "    while len(walk) < walk_length:\n",
    "        # Get the current node in the walk\n",
    "        cur = walk[-1]\n",
    "        # Get the neighbors of the current node\n",
    "        cur_nbrs = list(net[cur].indices)\n",
    "        # Check if the current node has any neighbors\n",
    "        if len(cur_nbrs) > 0:\n",
    "            # If the walk has just started, randomly choose the next node from the neighbors\n",
    "            if len(walk) == 1:\n",
    "                walk.append(np.random.choice(cur_nbrs))\n",
    "            else:\n",
    "                # Get the previous node in the walk\n",
    "                prev = walk[-2]\n",
    "                # Use the alias sampling method to choose the next node based on the bias parameters p and q\n",
    "                next_node = alias_sample(net, cur_nbrs, prev, p, q)\n",
    "                # Append the chosen next node to the walk\n",
    "                walk.append(next_node)\n",
    "        else:\n",
    "            # If the current node has no neighbors, terminate the walk\n",
    "            break\n",
    "\n",
    "    return walk\n",
    "\n",
    "def alias_sample(net, neighbors, prev, p, q):\n",
    "    \"\"\"\n",
    "    Helper function to sample the next node in the walk.\n",
    "    \"\"\"\n",
    "    # Implement the logic to sample the next node based on the bias parameters p and q\n",
    "    # You can use the formula provided in the instructions to calculate the probabilities\n",
    "    # and then sample the next node accordingly.\n",
    "    # Initialize an empty list to store the unnormalized probabilities for each neighbor\n",
    "    unnormalized_probs = []\n",
    "\n",
    "    # Iterate over each neighbor of the current node\n",
    "    for neighbor in neighbors:\n",
    "        # If the neighbor is the same as the previous node in the walk\n",
    "        if neighbor == prev:\n",
    "            # Append the probability 1/p to the unnormalized probabilities list\n",
    "            unnormalized_probs.append(1 / p)\n",
    "        # If the neighbor is connected to the previous node in the walk\n",
    "        elif neighbor in net[prev].indices:\n",
    "            # Append the probability 1 to the unnormalized probabilities list\n",
    "            unnormalized_probs.append(1)\n",
    "        # If the neighbor is not connected to the previous node in the walk\n",
    "        else:\n",
    "            # Append the probability 1/q to the unnormalized probabilities list\n",
    "            unnormalized_probs.append(1 / q)\n",
    "\n",
    "    # Calculate the normalization constant by summing all unnormalized probabilities\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "\n",
    "    # Normalize the probabilities by dividing each unnormalized probability by the normalization constant\n",
    "    normalized_probs = [float(prob) / norm_const for prob in unnormalized_probs]\n",
    "\n",
    "    # Randomly choose the next node from the neighbors based on the normalized probabilities\n",
    "    next_node = np.random.choice(neighbors, size=1, p=normalized_probs)[0]\n",
    "\n",
    "    # Return the chosen next node\n",
    "    return next_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a83f6b",
   "metadata": {},
   "source": [
    "Now, let's set up the word2vec model for node2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = []\n",
    "p = 1\n",
    "q = 0.1\n",
    "for i in range(n_nodes):\n",
    "    for _ in range(n_walkers_per_node):\n",
    "        walks.append(node2vec_random_walk(A, i, walk_length, p, q))\n",
    "model = Word2Vec(walks, vector_size=32, window=3, min_count=1, sg=1, hs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174599e",
   "metadata": {},
   "source": [
    "where `hs=0` indicates that we are using negative sampling.\n",
    "Notice that we set `sg=1` and `hs=1` instead of `sg=1` and `hs=0` in DeepWalk. This is because node2vec uses the skip-gram model with negative sampling.\n",
    "\n",
    "Now, we extract the node embeddings from the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb305f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = []\n",
    "for i in range(n_nodes):\n",
    "    embedding.append(model.wv[i])\n",
    "embedding = np.array(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42eb621",
   "metadata": {},
   "source": [
    "Let's visualize the node embeddings from node2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792db97a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, metric=\"cosine\")\n",
    "xy = reducer.fit_transform(embedding)\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "# Calculate the degree of each node\n",
    "degrees = A.sum(axis=1).A1\n",
    "\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x=xy[:, 0],\n",
    "    y=xy[:, 1],\n",
    "    size=np.sqrt(degrees / np.max(degrees)) * 30,\n",
    "    community=[palette[label] for label in g.vs[\"label\"]],\n",
    "    name = [str(i) for i in range(n_nodes)]\n",
    "))\n",
    "\n",
    "p = figure(title=\"Node Embeddings from Word2Vec\", x_axis_label=\"X\", y_axis_label=\"Y\")\n",
    "\n",
    "p.scatter('x', 'y', size='size', source=source, line_color=\"black\", color=\"community\")\n",
    "\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"Name\", \"@name\"),\n",
    "    (\"Community\", \"@community\")\n",
    "]\n",
    "p.add_tools(hover)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ad79b",
   "metadata": {},
   "source": [
    "The results for clustering are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "labels = Kmeans_with_silhouette(embedding)\n",
    "\n",
    "\n",
    "cmap = sns.color_palette().as_hex()\n",
    "igraph.plot(g, vertex_color=[cmap[label] for label in labels], bbox=(500, 500), vertex_label=[\"%d\" %  d for d in  np.arange(n_nodes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1cd610",
   "metadata": {},
   "source": [
    "## LINE\n",
    "\n",
    "LINE {footcite}`tang2015line` is another pioneering work to learn node embeddings by directly optimizing the graph structure.\n",
    "It is equivalent to node2vec with $p=1$, $q=1$, and window size 1.\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "\n",
    "```# Pen and paper exercises\n",
    "\n",
    "- [✍️ Pen and paper exercises](pen-and-paper/exercise.pdf)\n",
    "\n",
    "# Software for Network Embedding\n",
    "\n",
    "There are various software packages for network embeddings. But due to technical complexity, some of them do not faithfully implement the algorithms in the paper. We provide a list of software packages for network embeddings below.\n",
    "\n",
    "- [fastnode2vec](https://github.com/louisabraham/fastnode2vec). This is a very fast implementation of node2vec. However, it uses a uniform probability distribution for the negative sampling, which is different from the original node2vec paper that uses a different distribution. This leads to some degeneracy of the embedding quality in community detection tasks.\n",
    "- [pytorch-geometric](https://github.com/pyg-team/pytorch_geometric). This is a very popular package for graph neural networks. It also uses a uniform probability distribution for the negative sampling, potentially having the same issue as `fastnode2vec`.\n",
    "- [gnn-tools](https://github.com/skojaku/gnn-tools). This is a collection of my experiments on network embedding methods.\n",
    "- [My collection](https://github.com/skojaku/graphvec). This is a lighter version of the `gnn-tools` collection.\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Spectral Embedding\n",
    "\n",
    "## Network compression\n",
    "\n",
    "Networks are a high-dimensional discrete data that can be difficult to analyze with traditional machine learning methods that assume continuous and smooth data. Spectral embedding is a technique to embed networks into low-dimensional spaces.\n",
    "\n",
    "Let us approach the spectral embedding from the perspective of network compression.\n",
    "Suppose we have an adjacency matrix $\\mathbf{A}$ of a network.\n",
    "The adjacency matrix is a high-dimensional data, i.e., a matrix has size $N \\times N$ for a network of $N$ nodes.\n",
    "We want to compress it into a lower-dimensional matrix $\\mathbf{U}$ of size $N \\times d$ for a user-defined small integer $d < N$.\n",
    "A good $\\mathbf{U}$ should preserve the network structure and thus can reconstruct the original data $\\mathbf{A}$ as closely as possible.\n",
    "This leads to the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{U}} J(\\mathbf{U}),\\quad J(\\mathbf{U}) = \\| \\mathbf{A} - \\mathbf{U}\\mathbf{U}^\\top \\|_F^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "1. $\\mathbf{U}\\mathbf{U}^\\top$ is the outer product of $\\mathbf{U}$ and represents the reconstructed network.\n",
    "2. $\\|\\cdot\\|_F$ is the Frobenius norm, which is the sum of the squares of the elements in the matrix.\n",
    "3. $J(\\mathbf{U})$ is the loss function that measures the difference between the original network $\\mathbf{A}$ and the reconstructed network $\\mathbf{U}\\mathbf{U}^\\top$.\n",
    "\n",
    "By minimizing the Frobenius norm with respect to $\\mathbf{U}$, we obtain the best low-dimensional embedding of the network.\n",
    "\n",
    "\n",
    "### An intuitive solution for the optimization problem\n",
    "\n",
    "Let us first understand the solution intuitively.\n",
    "Consider the spectral decomposition of $\\mathbf{A}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\sum_{i=1}^N \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ are weights and $\\mathbf{u}_i$ are column vectors. Each term $\\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$ is a rank-one matrix that captures a part of the network's structure. The larger the weight $\\lambda_i$, the more important that term is in describing the network.\n",
    "\n",
    "\n",
    "To compress the network, we can select the $d$ terms with the largest weights $\\lambda_i$. By combining the corresponding $\\mathbf{u}_i$ vectors into a matrix $\\mathbf{U}$, we obtain a good low-dimensional embedding of the network.\n",
    "\n",
    "![](../figs/spectral-decomposition.jpg)\n",
    "\n",
    "For a formal proof, please refer to the [Appendix section](./appendix.md).\n",
    "\n",
    "### An example for the spectral embedding\n",
    "\n",
    "Let us demonstrate the results with a simple example as follows.\n",
    "\n",
    "```{code-cell} ipython3\n",
    ":tags: [hide-input]\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a small example network\n",
    "G = nx.karate_club_graph()\n",
    "A = nx.adjacency_matrix(G).toarray()\n",
    "labels = np.unique([d[1]['club'] for d in G.nodes(data=True)], return_inverse=True)[1]\n",
    "cmap = sns.color_palette()\n",
    "nx.draw(G, with_labels=False, node_color=[cmap[i] for i in labels])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ffb9f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the spectral decomposition\n",
    "eigvals, eigvecs = np.linalg.eig(A)\n",
    "\n",
    "# Find the top d eigenvectors\n",
    "d = 2\n",
    "sorted_indices = np.argsort(eigvals)[::-1][:d]\n",
    "eigvals = eigvals[sorted_indices]\n",
    "eigvecs = eigvecs[:, sorted_indices]\n",
    "\n",
    "# Plot the results\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\n",
    "ax.set_title('Spectral Embedding')\n",
    "ax.set_xlabel('Eigenvector 1')\n",
    "ax.set_ylabel('Eigenvector 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f214a6",
   "metadata": {},
   "source": [
    "Interestingly, the first eigenvector corresponds to the eigen centrality of the network, representing the centrality of the nodes.\n",
    "The second eigenvector captures the community structure of the network, clearly separating the two communities in the network.\n",
    "\n",
    "## Modularity embedding\n",
    "\n",
    "In a similar vein, we can use the modularity matrix to generate a low-dimensional embedding of the network.\n",
    "Namely, let us define the modularity matrix $\\mathbf{Q}$ as follows:\n",
    "\n",
    "$$\n",
    "Q_{ij} = \\frac{1}{2m}A_{ij} - \\frac{k_i k_j}{4m^2}\n",
    "$$\n",
    "\n",
    "where $k_i$ is the degree of node $i$, and $m$ is the number of edges in the network.\n",
    "\n",
    "We then compute the eigenvectors of $\\mathbf{Q}$ and use them to embed the network into a low-dimensional space just as we did for the adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e65129",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "deg = np.sum(A, axis=1)\n",
    "m = np.sum(deg) / 2\n",
    "Q = A - np.outer(deg, deg) / (2 * m)\n",
    "Q/= 2*m\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eig(Q)\n",
    "\n",
    "# Sort the eigenvalues and eigenvectors\n",
    "sorted_indices = np.argsort(-eigvals)[:d]  # Exclude the first eigenvector\n",
    "eigvals = eigvals[sorted_indices]\n",
    "eigvecs = eigvecs[:, sorted_indices]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\n",
    "ax.set_title('Modularity Embedding')\n",
    "ax.set_xlabel('Eigenvector 1')\n",
    "ax.set_ylabel('Eigenvector 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58848b6",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The modularity embedding can be used to bipartition the network into two communities using a simple algorithm: group nodes with the same sign of the second eigenvector {footcite}`newman2006modularity`.\n",
    "```\n",
    "\n",
    "## Laplacian Eigenmap\n",
    "\n",
    "Laplacian Eigenmap {footcite}`belkin2003laplacian` is another approach to compress a network into a low-dimensional space. The fundamental idea behind this method is to position connected nodes close to each other in the low-dimensional space. This approach leads to the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{U}} J_{LE}(\\mathbf{U}),\\quad J_{LE}(\\mathbf{U}) = \\frac{1}{2}\\sum_{i,j} A_{ij} \\| u_i - u_j \\|^2\n",
    "$$\n",
    "\n",
    "In this equation, $\\| u_i - u_j \\|^2$ represents the squared distance between nodes $i$ and $j$ in the low-dimensional space. The goal is to minimize this distance for connected nodes (where $A_{ij} = 1$). The factor $\\frac{1}{2}$ is included for mathematical convenience in later calculations.\n",
    "\n",
    "To solve this optimization problem, we rewrite $J_{LE}(\\mathbf{U})$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_{LE}(\\mathbf{U}) &= \\frac{1}{2}\\sum_{i}\\sum_{j} A_{ij} \\| u_i - u_j \\|^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{i}\\sum_{j} A_{ij} \\left( \\| u_i \\|^2 - 2 u_i^\\top u_j + \\| u_j \\|^2 \\right) \\\\\n",
    "&= \\sum_{i}\\sum_{j} A_{ij} \\| u_i \\|^2 - \\sum_{i}\\sum_{j} A_{ij} u_i^\\top u_j\\\\\n",
    "&= \\sum_{i} k_i \\| u_i \\|^2 - \\sum_{i,j} A_{ij} u_i^\\top u_j\\\\\n",
    "&= \\sum_{i,j} L_{ij} u_i^\\top u_j\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "L_{ij} = \\begin{cases}\n",
    "k_i & \\text{if } i = j \\\\\n",
    "-A_{ij} & \\text{if } i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let us go through the derivation step by step.\n",
    "\n",
    "1. In the first step (i.e., the second line), we expand the squared norm using the vector identity $\\|a-b\\|^2 = \\|a\\|^2 - 2a^\\top b + \\|b\\|^2$.\n",
    "\n",
    "2. In the second step (i.e., the third line), we distribute the sum and the factor $\\frac{1}{2}$.\n",
    "The middle term gets a factor of 1 because it appears twice in the expansion (once for $i,j$ and once for $j,i$), canceling out the $\\frac{1}{2}$. Note that the term $A_{ij}$ is symmetric, i.e., $A_{ij} = A_{ji}$.\n",
    "\n",
    "3. In the third step (i.e., the fourth line), we recognize that $\\sum_j A_{ij}$ is the degree of node $i$, which we denote as $k_i$.\n",
    "\n",
    "4. Finally, we combine the terms by using the Laplacian matrix $\\mathbf{L}$.\n",
    "\n",
    "The minimization problem can be rewritten as:\n",
    "\n",
    "$$\n",
    "J_{LE}(\\mathbf{U}) = \\text{Tr}(\\mathbf{U}^\\top \\mathbf{L} \\mathbf{U})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{U} =\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{u}_1 ^\\top \\\\\n",
    "\\mathbf{u}_2 ^\\top \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{u}_N ^\\top \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "See the [Appendix section](./appendix.md) for the detailed derivation.\n",
    "\n",
    "By taking the derivative of $J_{LE}(\\mathbf{U})$ with respect to $\\mathbf{U}$ and set it to zero, we obtain the following equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{LE}}{\\partial \\mathbf{U}} = 0 \\implies \\mathbf{L} \\mathbf{U} = \\lambda \\mathbf{U}\n",
    "$$\n",
    "\n",
    "The solution is the $d$ eigenvectors associated with the $d$ smallest eigenvalues of $\\mathbf{L}$.\n",
    "\n",
    "It is important to note that the eigenvector corresponding to the smallest eigenvalue (which is always zero for connected graphs) is trivial - it's the all-one vector. Therefore, in practice, we typically compute the $d+1$ smallest eigenvectors and discard the one corresponding to the zero eigenvalue.\n",
    "\n",
    "\n",
    "### An example for the Laplacian Eigenmap\n",
    "\n",
    "Let us first compute the Laplacian matrix and its eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbddefc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "D = np.diag(np.sum(A, axis=1))\n",
    "L = D - A\n",
    "\n",
    "eigvals, eigvecs = np.linalg.eig(L)\n",
    "\n",
    "# Sort the eigenvalues and eigenvectors\n",
    "sorted_indices = np.argsort(eigvals)[1:d+1]  # Exclude the first eigenvector\n",
    "eigvals = eigvals[sorted_indices]\n",
    "eigvecs = eigvecs[:, sorted_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b808e3b",
   "metadata": {},
   "source": [
    "The eigenvectors corresponding to the $d$ smallest eigenvalues are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25143d5e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.scatterplot(x = eigvecs[:, 0], y = eigvecs[:, 1], hue=labels, ax=ax)\n",
    "ax.set_title('Laplacian Eigenmap')\n",
    "ax.set_xlabel('Eigenvector 2')\n",
    "ax.set_ylabel('Eigenvector 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab8120a",
   "metadata": {},
   "source": [
    "```{footbibliography}\n",
    "```\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Spectral vs Neural Embedding\n",
    "\n",
    "We have learned two types of graph embedding methods: spectral methods and neural embedding methods. But which one is better than the other? We will compare the two types of methods from multiple aspects as follows.\n",
    "\n",
    "\n",
    "1. **Analytical Tractability**: Spectral methods are more analytically tractable and thus are easier to understand using linear algebra. It is even possible to derive the capability and limitation of the spectral methods. For example, spectral methods based on adjacency matrices and normalized laplacian matrices are shown to be optimal for detecting communities in the stochastic block model {footcite}`nadakuditi2012graph`. Neural embedding methods are less analytically tractable. But still possible to analyze the theoretical properties by using an equivalence between a spectral embedding and a neural embedding under a very specific condition {footcite}`qiu2018network,kojaku2023network`. These theoretical results have demonstrated that DeepWalk, node2vec, and LINE are in fact an optimal embedding methods for community detection for the stochatic block model.\n",
    "\n",
    "2. **Scalability**: A key limitation of the spectral embedding is the computational cost. While efficient methods exist like randomized singular value decomposition (implemented in scikit learn package as `TruncatedSVD`), they might be unstable depending on the spectrum distribution of the matrix to be decomposed. Neural embedding methods are often more stable and scalable.\n",
    "\n",
    "3. **Flexibility**: Neural embeddings are more flexible than spectral embeddings. It is easy to change the objective functions of neural embeddings using the same training procedure. For example, the proximity of nodes in both embedding spaces are inherently dot similarity, but one can train neural embeddings to optimize for other metrics to embed the network in a non-Euclidean space. An interesting example of this is the Poincaré embeddings {footcite}`nickel2017poincare` for embedding networks in hyperbolic space.\n",
    "\n",
    "   ![](https://pbs.twimg.com/media/DUUj0sxU8AACV50.jpg)\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```\n",
    "# Module 8: Embedding\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn how to embed networks into low-dimensional spaces. We will learn:\n",
    "- Spectral embedding\n",
    "- Neural embedding\n",
    "- **Keywords**: Laplacian EigenMap, Normalized Spectral Embedding, DeepWalk, Node2Vec---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# word2vec\n",
    "\n",
    "In this section, we will introduce *word2vec*, a powerful technique for learning word embeddings. word2vec is a neural network model that learns words embeddings in a continuous vector space. It was introduced by Tomas Mikolov and his colleagues at Google in 2013 {footcite}`mikolov2013distributed`.\n",
    "\n",
    "## How it works\n",
    "\n",
    "\"You shall know a word by the company it keeps\" {footcite}`church1988word` is a famous quote in linguistics. It means that you can understand the meaning of a word by looking at the words that appear in the same context.\n",
    "word2vec operates on the same principle.\n",
    "word2vec identifies a word's context by examining the words within a fixed window around it. For example, in the sentence:\n",
    "\n",
    "> The quick brown fox jumps over a lazy dog\n",
    "\n",
    "The context of the word *fox* includes *quick*, *brown*, *jumps*, *over*, and *lazy*. word2vec is trained to predict which words are likely to appear as the context of an input word.\n",
    "\n",
    "```{note}\n",
    "There are two main architectures for word2vec:\n",
    "1. **Continuous Bag of Words (CBOW)**: Predicts the target word (center word) from the context words (surrounding words).\n",
    "2. **Skip-gram**: Predicts the context words (surrounding words) from the target word (center word).\n",
    "```\n",
    "\n",
    "So how are word embeddings learned? word2vec is a neural network model that looks like a bow tie. It has two layers of the vocabulary size coupled with a much smaller hidden layer.\n",
    "\n",
    "![](../figs/word2vec.png)\n",
    "\n",
    "- **Input layer**: The input layer consists of $N$ neurons, where $N$ is the size of the vocabulary (i.e., the number of unique words in the corpus). Each neuron corresponds to a unique word in the vocabulary. When a word is inputted, its corresponding neuron is activated and the other neurons are inhibited. Thus, the input layer is essentially a lookup mechanism that transforms the input word into a corresponding one-hot vector.\n",
    "\n",
    "- **Output layer**: The output layer also consists of $N$ neurons, each corresponding to a unique word in the vocabulary. Unlike the input layer, multiple neurons can be activated for a single input. The strength of the activation of each neuron (with a normalization by the softmax function) represents the probability of the corresponding word being the input word's context.\n",
    "\n",
    "- **Hidden layer**: The hidden layer is much smaller than the input and output layers. Multiple neurons in the hidden layer can be activated for a single input, and this activation pattern represents the word's *embedding*.\n",
    "\n",
    "We can consider word2vec as a *dimensionality reduction* technique that reduces the dimensionality of the input layer to the hidden layer based on the co-occurrence of words within a short distance. The distance is named the *window size*, which is a user-defined hyperparameter.\n",
    "\n",
    "## What's special about word2vec?\n",
    "\n",
    "With word2vec, words are represented as dense vectors, enabling us to explore their relationships using simple linear algebra. This is in contrast to traditional natural language processing (NLP) methods, such as bag-of-words and topic modeling, which represent words as discrete units or high-dimensional vectors.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)\n",
    "\n",
    "To showcase the effectiveness of word2vec, let's walk through an example using the `gensim` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Load pre-trained word2vec model from Google News\n",
    "model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143a978",
   "metadata": {},
   "source": [
    "Our first example is to find the words most similar to *king*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4028d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "word = \"king\"\n",
    "similar_words = model.most_similar(word)\n",
    "print(f\"Words most similar to '{word}':\")\n",
    "for similar_word, similarity in similar_words:\n",
    "    print(f\"{similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c5000",
   "metadata": {},
   "source": [
    "A cool (yet controversial) application of word embeddings is analogy solving. Let us consider the following puzzle:\n",
    "\n",
    "> *man* is to *woman* as *king* is to ___ ?\n",
    "\n",
    "We can use word embeddings to solve this puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e1823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We solve the puzzle by\n",
    "#\n",
    "#  vec(king) - vec(man) + vec(woman)\n",
    "#\n",
    "# To solve this, we use the model.most_similar function, with positive words being \"king\" and \"woman\" (additive), and negative words being \"man\" (subtractive).\n",
    "#\n",
    "model.most_similar(positive=['woman', \"king\"], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d2168",
   "metadata": {},
   "source": [
    "The last example is to visualize the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d05a9a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "countries = ['Germany', 'France', 'Italy', 'Spain', 'Portugal', 'Greece']\n",
    "capital_words = ['Berlin', 'Paris', 'Rome', 'Madrid', 'Lisbon', 'Athens']\n",
    "\n",
    "# Get the word embeddings for the countries and capitals\n",
    "country_embeddings = np.array([model[country] for country in countries])\n",
    "capital_embeddings = np.array([model[capital] for capital in capital_words])\n",
    "\n",
    "# Compute the PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings = np.vstack([country_embeddings, capital_embeddings])\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create a DataFrame for seaborn\n",
    "df = pd.DataFrame(embeddings_pca, columns=['PC1', 'PC2'])\n",
    "df['Label'] = countries + capital_words\n",
    "df['Type'] = ['Country'] * len(countries) + ['Capital'] * len(capital_words)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a scatter plot with seaborn\n",
    "scatter_plot = sns.scatterplot(data=df, x='PC1', y='PC2', hue='Type', style='Type', s=200, palette='deep', markers=['o', 's'])\n",
    "\n",
    "# Annotate the points\n",
    "for i in range(len(df)):\n",
    "    plt.text(df['PC1'][i], df['PC2'][i] + 0.08, df['Label'][i], fontsize=12, ha='center', va='bottom',\n",
    "             bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))\n",
    "\n",
    "# Draw arrows between countries and capitals\n",
    "for i in range(len(countries)):\n",
    "    plt.arrow(df['PC1'][i], df['PC2'][i], df['PC1'][i + len(countries)] - df['PC1'][i], df['PC2'][i + len(countries)] - df['PC2'][i],\n",
    "              color='gray', alpha=0.6, linewidth=1.5, head_width=0.02, head_length=0.03)\n",
    "\n",
    "plt.legend(title='Type', title_fontsize='13', fontsize='11')\n",
    "plt.title('PCA of Country and Capital Word Embeddings', fontsize=16)\n",
    "plt.xlabel('Principal Component 1', fontsize=14)\n",
    "plt.ylabel('Principal Component 2', fontsize=14)\n",
    "ax = plt.gca()\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ceec3",
   "metadata": {},
   "source": [
    "We can see that word2vec places the words representing countries close to each other and so do the words representing their capitals. The country-capital relationship is also roughly preserved, e.g., *Germany*-*Berlin* vector is roughly parallel to *France*-*Paris* vector.\n",
    "\n",
    "```{footbibliography}\n",
    "\n",
    "```\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Appendix\n",
    "\n",
    "## Bruna's Spectral GCN\n",
    "\n",
    "\n",
    "Let's first implement Bruna's spectral GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ffebd",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.sparse.linalg as slinalg\n",
    "\n",
    "class BrunaGraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Bruna's Spectral Graph Convolution Layer\n",
    "\n",
    "    This implementation follows the original formulation by Joan Bruna et al.,\n",
    "    using the eigendecomposition of the graph Laplacian for spectral convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, n_nodes):\n",
    "        \"\"\"\n",
    "        Initialize the Bruna Graph Convolution layer\n",
    "\n",
    "        Args:\n",
    "            in_features (int): Number of input features\n",
    "            out_features (int): Number of output features\n",
    "        \"\"\"\n",
    "        super(BrunaGraphConv, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Learnable spectral filter parameters\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.FloatTensor(in_features, out_features, n_nodes-1)\n",
    "        )\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize weights using Glorot initialization\"\"\"\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_laplacian_eigenvectors(adj):\n",
    "        \"\"\"\n",
    "        Compute eigendecomposition of the normalized graph Laplacian\n",
    "\n",
    "        Args:\n",
    "            adj: Adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            eigenvalues, eigenvectors of the normalized Laplacian\n",
    "        \"\"\"\n",
    "        # Compute normalized Laplacian\n",
    "        # Add self-loops\n",
    "        adj = adj + sp.eye(adj.shape[0])\n",
    "\n",
    "        # Compute degree matrix\n",
    "        deg = np.array(adj.sum(axis=1))\n",
    "        Dsqrt_inv = sp.diags(1.0 / np.sqrt(deg).flatten())\n",
    "\n",
    "        # Compute normalized Laplacian: D^(-1/2) A D^(-1/2)\n",
    "        laplacian = sp.eye(adj.shape[0]) - Dsqrt_inv @ adj @ Dsqrt_inv\n",
    "\n",
    "        # Compute eigendecomposition\n",
    "        # Using k=adj.shape[0]-1 to get all non-zero eigenvalues\n",
    "        eigenvals, eigenvecs = slinalg.eigsh(laplacian.tocsc(), k=adj.shape[0]-1,which='SM', tol=1e-6)\n",
    "\n",
    "        return torch.FloatTensor(eigenvals), torch.FloatTensor(eigenvecs)\n",
    "\n",
    "    def forward(self, x, eigenvecs):\n",
    "        \"\"\"\n",
    "        Forward pass implementing Bruna's spectral convolution\n",
    "\n",
    "        Args:\n",
    "            x: Input features [num_nodes, in_features]\n",
    "            eigenvecs: Eigenvectors of the graph Laplacian [num_nodes, num_nodes-1]\n",
    "\n",
    "        Returns:\n",
    "            Output features [num_nodes, out_features]\n",
    "        \"\"\"\n",
    "        # Transform to spectral domain\n",
    "        x_spectral = torch.matmul(eigenvecs.t(), x)  # [num_nodes-1, in_features]\n",
    "\n",
    "        # Initialize output tensor\n",
    "        out = torch.zeros(x.size(0), self.out_features, device=x.device)\n",
    "\n",
    "        # For each input-output feature pair\n",
    "        for i in range(self.in_features):\n",
    "            for j in range(self.out_features):\n",
    "                # Element-wise multiplication in spectral domain\n",
    "                # This is the actual spectral filtering operation\n",
    "                filtered = x_spectral[:, i] * self.weight[i, j, :]  # [num_spectrum]\n",
    "\n",
    "                # Transform back to spatial domain and accumulate\n",
    "                out[:, j] += torch.matmul(eigenvecs, filtered)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74bf48",
   "metadata": {},
   "source": [
    "Next, we will train the model on the karate club network to predict the given node labels indicating nodes' community memberships. We load the data by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecda0e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load karate club network\n",
    "G = nx.karate_club_graph()\n",
    "adj = nx.to_scipy_sparse_array(G)\n",
    "features = torch.eye(G.number_of_nodes())\n",
    "labels = torch.tensor([G.nodes[i]['club'] == 'Officer' for i in G.nodes()], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf0dce",
   "metadata": {},
   "source": [
    "We apply the convolution twice with ReLu activation in between. This can be implemented by preparing two independent `BrunaGraphConv` layers, applying them consecutively, and adding a ReLu activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928c668",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Define a simple GCN model\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, in_features, out_features, hidden_features, n_nodes):\n",
    "        super(SimpleGCN, self).__init__()\n",
    "        self.conv1 = BrunaGraphConv(in_features, hidden_features, n_nodes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = BrunaGraphConv(hidden_features, out_features, n_nodes)\n",
    "\n",
    "    def forward(self, x, eigenvecs):\n",
    "        x = self.conv1(x, eigenvecs)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, eigenvecs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3558f2",
   "metadata": {},
   "source": [
    "We then train the model by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea274af",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get eigenvectors of the Laplacian\n",
    "eigenvals, eigenvecs = BrunaGraphConv.get_laplacian_eigenvectors(adj)\n",
    "\n",
    "# Initialize the model\n",
    "hidden_features = 10\n",
    "input_features = features.shape[1]\n",
    "output_features = 2\n",
    "n_nodes = G.number_of_nodes()\n",
    "model = SimpleGCN(input_features, output_features, hidden_features, n_nodes)\n",
    "\n",
    "# Train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_idx, test_idx = train_test_split(np.arange(G.number_of_nodes()), test_size=0.2, random_state=42)\n",
    "train_features = features[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "test_features = features[test_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "\n",
    "n_train = 100\n",
    "for epoch in range(n_train):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_features, eigenvecs[train_idx, :])\n",
    "    loss = criterion(output, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    if epoch == 0 or (epoch+1) % 25 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(test_features, eigenvecs[test_idx, :])\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            accuracy = (predicted == test_labels).float().mean()\n",
    "            print(f'Epoch {epoch+1}/{n_train}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5f5c0",
   "metadata": {},
   "source": [
    "Observe that the accuracy increases as the training progresses. We can use the model to predict the labels.\n",
    "The model has a hidden layer, and let's visualize the data in the hidden space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f602937c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Visualize the learned embeddings\n",
    "embeddings = model.conv1(features, eigenvecs).detach().numpy()\n",
    "\n",
    "xy = TSNE(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.scatterplot(x = xy[:, 0].reshape(-1), y = xy[:, 1].reshape(-1), hue=labels.numpy(), palette='tab10', ax = ax)\n",
    "ax.set_title(\"Learned Node Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc2229",
   "metadata": {},
   "source": [
    "## ChebNet\n",
    "\n",
    "Let's implement the ChebNet layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41175eec",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.sparse as sp\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse(sparse_mx):\n",
    "    \"\"\"Convert scipy sparse matrix to torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo()\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
    "    )\n",
    "    values = torch.from_numpy(sparse_mx.data.astype(np.float32))\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse_coo_tensor(indices, values, shape)\n",
    "\n",
    "\n",
    "class ChebConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Chebyshev Spectral Graph Convolutional Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, K: int, bias: bool = True):\n",
    "        super(ChebConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.K = K\n",
    "\n",
    "        # Trainable parameters\n",
    "        self.weight = nn.Parameter(torch.Tensor(K, in_channels, out_channels))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "    def _normalize_laplacian(self, adj_matrix):\n",
    "        \"\"\"\n",
    "        Compute normalized Laplacian L = I - D^(-1/2)AD^(-1/2)\n",
    "        \"\"\"\n",
    "        # Convert to scipy if it's not already\n",
    "        if not sp.isspmatrix(adj_matrix):\n",
    "            adj_matrix = sp.csr_matrix(adj_matrix)\n",
    "\n",
    "        adj_matrix = adj_matrix.astype(float)\n",
    "\n",
    "        # Compute degree matrix D\n",
    "        rowsum = np.array(adj_matrix.sum(1)).flatten()\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5)\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "        # Compute L = I - D^(-1/2)AD^(-1/2)\n",
    "        n = adj_matrix.shape[0]\n",
    "        L = sp.eye(n) - d_mat_inv_sqrt @ adj_matrix @ d_mat_inv_sqrt\n",
    "        return L\n",
    "\n",
    "    def _scale_laplacian(self, L):\n",
    "        \"\"\"\n",
    "        Scale Laplacian eigenvalues to [-1, 1] interval\n",
    "        L_scaled = 2L/lambda_max - I\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Compute largest eigenvalue\n",
    "            eigenval, _ = sp.linalg.eigsh(L, k=1, which=\"LM\", return_eigenvectors=False)\n",
    "            lambda_max = eigenval[0]\n",
    "        except:\n",
    "            # Approximate lambda_max = 2 if eigenvalue computation fails\n",
    "            lambda_max = 2.0\n",
    "\n",
    "        n = L.shape[0]\n",
    "        L_scaled = (2.0 / lambda_max) * L - sp.eye(n)\n",
    "        return L_scaled\n",
    "\n",
    "    def chebyshev_basis(self, L_sparse: torch.sparse.Tensor, X: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute Chebyshev polynomials basis up to order K.\n",
    "        \"\"\"\n",
    "        # List to store Chebyshev polynomials\n",
    "        cheb_polynomials = []\n",
    "\n",
    "        # T_0(L) = I\n",
    "        cheb_polynomials.append(X)\n",
    "\n",
    "        if self.K > 1:\n",
    "            # T_1(L) = L\n",
    "            X_1 = torch.sparse.mm(L_sparse, X)\n",
    "            cheb_polynomials.append(X_1)\n",
    "\n",
    "        # Recurrence T_k(L) = 2L·T_{k-1}(L) - T_{k-2}(L)\n",
    "        for k in range(2, self.K):\n",
    "            X_k = (\n",
    "                2 * torch.sparse.mm(L_sparse, cheb_polynomials[k - 1])\n",
    "                - cheb_polynomials[k - 2]\n",
    "            )\n",
    "            cheb_polynomials.append(X_k)\n",
    "\n",
    "        return torch.stack(cheb_polynomials, dim=0)  # [K, num_nodes, in_channels]\n",
    "\n",
    "    def forward(self, X: torch.Tensor, adj_matrix: sp.spmatrix):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            X: Node features tensor of shape [num_nodes, in_channels]\n",
    "            adj_matrix: Adjacency matrix in scipy sparse format\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [num_nodes, out_channels]\n",
    "        \"\"\"\n",
    "        # Compute normalized and scaled Laplacian\n",
    "        L_norm = self._normalize_laplacian(adj_matrix)\n",
    "        L_scaled = self._scale_laplacian(L_norm)\n",
    "\n",
    "        # Convert to torch sparse tensor\n",
    "        L_scaled = sparse_mx_to_torch_sparse(L_scaled).to(X.device)\n",
    "\n",
    "        # Compute Chebyshev polynomials basis\n",
    "        Tx = self.chebyshev_basis(L_scaled, X)  # [K, num_nodes, in_channels]\n",
    "\n",
    "        # Perform convolution using learned weights\n",
    "        out = torch.einsum(\"kni,kio->no\", Tx, self.weight)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d55982",
   "metadata": {},
   "source": [
    "We stack the layers to form a simple GCN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a90bef",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class ChebNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ChebNet model for node classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: int,\n",
    "        out_channels: int,\n",
    "        K: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0.5,\n",
    "    ):\n",
    "        super(ChebNet, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(ChebConv(in_channels, hidden_channels, K))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(ChebConv(hidden_channels, hidden_channels, K))\n",
    "\n",
    "        # Output layer\n",
    "        self.convs.append(ChebConv(hidden_channels, out_channels, K))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, X: torch.Tensor, adj_matrix: sp.spmatrix):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers\n",
    "        \"\"\"\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            X = conv(X, adj_matrix)\n",
    "            X = self.activation(X)\n",
    "            X = self.dropout(X)\n",
    "\n",
    "        # Output layer\n",
    "        X = self.convs[-1](X, adj_matrix)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42d46d",
   "metadata": {},
   "source": [
    "Let's train the model on the karate club network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da33c8f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load karate club network\n",
    "G = nx.karate_club_graph()\n",
    "adj = nx.to_scipy_sparse_array(G)\n",
    "features = torch.eye(G.number_of_nodes())\n",
    "labels = torch.tensor(\n",
    "    [G.nodes[i][\"club\"] == \"Officer\" for i in G.nodes()], dtype=torch.long\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "hidden_features = 10\n",
    "input_features = features.shape[1]\n",
    "output_features = 2\n",
    "n_nodes = G.number_of_nodes()\n",
    "K = 3\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = ChebNet(\n",
    "    input_features, hidden_features, output_features, K, num_layers, dropout\n",
    ")\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train the model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(G.number_of_nodes()), test_size=0.2, random_state=42\n",
    ")\n",
    "train_features = features[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "test_features = features[test_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "\n",
    "n_train = 100\n",
    "for epoch in range(n_train):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss = criterion(output[train_idx], train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    if epoch == 0 or (epoch + 1) % 25 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(features, adj)\n",
    "            _, predicted = torch.max(output[test_idx], 1)\n",
    "            accuracy = (predicted == test_labels).float().mean()\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{n_train}, Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ebd79",
   "metadata": {},
   "source": [
    "Let's visualize the learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fa0fd",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get embeddings from the last hidden layer\n",
    "    X_hidden = features\n",
    "    for conv in model.convs[:-1]:\n",
    "        X_hidden = conv(X_hidden, adj)\n",
    "        X_hidden = model.activation(X_hidden)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "xy = TSNE(n_components=2).fit_transform(X_hidden.numpy())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.scatterplot(\n",
    "    x=xy[:, 0].reshape(-1),\n",
    "    y=xy[:, 1].reshape(-1),\n",
    "    hue=labels.numpy(),\n",
    "    palette=\"tab10\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Learned Node Embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d0f10",
   "metadata": {},
   "source": [
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# From Image to Graph\n",
    "\n",
    "## Analogy between image and graph data\n",
    "We can think of a convolution of an image from the perspective of networks.\n",
    "In the convolution of an image, a pixel is convolved with its *neighbors*. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.\n",
    "\n",
    "![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp)\n",
    "\n",
    "Building on this analogy, we can extend the idea of convolution to general graph data.\n",
    "Each node has a pixel value(s) (e.g., feature vector), which is convolved with the values of its neighbors in the graph.\n",
    "This is the key idea of graph convolutional networks.\n",
    "But, there is a key difference: while the number of neighbors for an image is homogeneous, the number of neighbors for a node in a graph can be heterogeneous. Each pixel has the same number of neighbors (except for the boundary pixels), but nodes in a graph can have very different numbers of neighbors. This makes it non-trivial to define the \"kernel\" for graph convolution.\n",
    "\n",
    "## Spectral filter on graphs\n",
    "Just like we can define a convolution on images in the frequency domain, we can also define a ''frequency domain'' for graphs.\n",
    "\n",
    "Consider a network of $N$ nodes, where each node has a feature variable ${\\mathbf x}_i \\in \\mathbb{R}$. We are interested in:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2,\n",
    "$$\n",
    "\n",
    "where $A_{ij}$ is the adjacency matrix of the graph. The quantity $J$ represents *the total variation* of $x$ between connected nodes; a small $J$ means that connected nodes have similar $x$ (low variation; low frequency), while a large $J$ means that connected nodes have very different $x$ (high variation; high frequency).\n",
    "\n",
    "We can rewrite $J$ as\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\\bf x}^\\top {\\bf L} {\\bf x},\n",
    "$$\n",
    "\n",
    "where ${\\bf L}$ is the Laplacian matrix of the graph given by\n",
    "\n",
    "$$\n",
    "L_{ij} = \\begin{cases}\n",
    "-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\n",
    "k_i & \\text{if } i = j \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "and ${\\bf x} = [x_1,x_2,\\ldots, x_N]^\\top$ is a column vector of feature variables.\n",
    "\n",
    "\n",
    "```{admonition} Detailed derivation\n",
    ":tag: note\n",
    ":class: dropdown\n",
    "\n",
    "The above derivation shows that the total variation of $x$ between connected nodes is proportional to ${\\bf x}^\\top {\\bf L} {\\bf x}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 \\\\\n",
    "&= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N \\underbrace{A_{ij}\\left( x_i^2 +x_j^2\\right)}_{\\text{symmetric}} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n",
    "&= \\sum_{i=1}^Nx_i^2\\underbrace{\\sum_{j=1}^N A_{ij}}_{\\text{degree of node } i, k_i} - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n",
    "&= \\sum_{i=1}^Nx_i^2 k_i - \\sum_{i=1}^N\\sum_{j=1}^N A_{ij}x_ix_j \\\\\n",
    "&= \\underbrace{[x_1,x_2,\\ldots, x_N]}_{{\\bf x}} \\underbrace{\\begin{bmatrix} k_1 & 0 & \\cdots & 0 \\\\ 0 & k_2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & k_N \\end{bmatrix}}_{{\\bf D}} \\underbrace{\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}}_{{\\bf x}} - 2\\underbrace{\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}}_{{\\bf x}^\\top {\\mathbf A} {\\bf x}} {\\bf x} \\\\\n",
    "&= {\\bf x}^\\top {\\bf D} {\\bf x} - {\\bf x}^\\top {\\mathbf A} {\\bf x} \\\\\n",
    "&= {\\bf x}^\\top {\\bf L} {\\bf x},\n",
    "\\end{aligned}\n",
    "$$\n",
    "```\n",
    "\n",
    "Let us showcase the analogy between the Fourier transform and the Laplacian matrix.\n",
    "In the Fourier transform, a signal is decomposed into sinusoidal basis functions. Similarly, for a graph, we can decompose the variation $J$ into eigenvector bases.\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^N \\lambda_i  {\\bf x}^\\top {\\mathbf u}_i {\\mathbf u}_i^\\top {\\bf x} = \\sum_{i=1}^N \\lambda_i  ||{\\bf x}^\\top {\\mathbf u}_i||^2.\n",
    "$$\n",
    "\n",
    "where ${\\mathbf u}_i$ is the eigenvector corresponding to the eigenvalue $\\lambda_i$.\n",
    "- The term $({\\bf x}^\\top {\\mathbf u}_i)$ is a dot-product between the feature vector ${\\bf x}$ and the eigenvector ${\\mathbf u}_i$, which measures how much ${\\bf x}$ *coheres* with eigenvector ${\\mathbf u}_i$, similar to how Fourier coefficients measure coherency with sinusoids.\n",
    "- Each $||{\\bf x}^\\top {\\mathbf u}_i||^2$ is the ''strength'' of ${\\bf x}$ with respect to the eigenvector ${\\mathbf u}_i$, and the total variation $J$ is a weighted sum of these strengths.\n",
    "\n",
    "Some eigenvectors correspond to low-frequency components, while others correspond to high-frequency components. For example, the total variation $J$ for an eigenvector ${\\mathbf u}_i$ is given by\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2} \\sum_{j}\\sum_{\\ell} A_{j\\ell}(u_{ij} - u_{i\\ell})^2 = {\\mathbf u}_i^\\top {\\mathbf L} {\\mathbf u}_i = \\lambda_i.\n",
    "$$\n",
    "\n",
    "This equation provides key insight into the meaning of eigenvalues:\n",
    "\n",
    "1. For an eigenvector ${\\mathbf u}_i$, its eigenvalue $\\lambda_i$ measures the total variation for ${\\mathbf u}_i$.\n",
    "2. Large eigenvalues mean large differences between neighbors (high frequency), while small eigenvalues mean small differences (low frequency).\n",
    "\n",
    "Thus, if ${\\bf x}$ aligns well with ${\\mathbf u}_i$ with a large $\\lambda_i$, then ${\\bf x}$ has a strong high-frequency component; if ${\\bf x}$ aligns well with ${\\mathbf u}_i$ with a small $\\lambda_i$, then ${\\bf x}$ has strong low-frequency component.\n",
    "\n",
    "### Spectral Filtering\n",
    "\n",
    "Eigenvalues $\\lambda_i$ can be thought of as a *filter* that controls which frequency components pass through. Instead of using the filter associated with the Laplacian matrix, we can design a filter $h(\\lambda_i)$ to control which frequency components pass through. This leads to the idea of *spectral filtering*. Two common filters are:\n",
    "\n",
    "1. **Low-pass Filter**:\n",
    "   $$h_{\\text{low}}(\\lambda) = \\frac{1}{1 + \\alpha\\lambda}$$\n",
    "   - Preserves low frequencies (small λ)\n",
    "   - Suppresses high frequencies (large λ)\n",
    "   - Results in smoother signals\n",
    "\n",
    "2. **High-pass Filter**:\n",
    "   $$h_{\\text{high}}(\\lambda) = \\frac{\\alpha\\lambda}{1 + \\alpha\\lambda}$$\n",
    "   - Preserves high frequencies\n",
    "   - Suppresses low frequencies\n",
    "   - Emphasizes differences between neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ead78",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "alpha = 1\n",
    "lambdas = np.linspace(0, 10, 100)\n",
    "h_low = 1 / (1 + alpha * lambdas)\n",
    "h_high = (alpha * lambdas) / (1 + alpha * lambdas)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.lineplot(x=lambdas, y=h_low, label=\"Low-pass filter\", ax=axes[0])\n",
    "axes[0].legend(frameon=False).remove()\n",
    "sns.lineplot(x=lambdas, y=h_high, label=\"High-pass filter\", ax=axes[1])\n",
    "axes[1].legend(frameon=False).remove()\n",
    "axes[0].set_title(\"Low-pass filter\")\n",
    "axes[1].set_title(\"High-pass filter\")\n",
    "fig.text(0.5, 0.01, \"Eigenvalue $\\lambda$\", ha=\"center\")\n",
    "axes[0].set_ylabel(\"Filter response $h(\\lambda)$\")\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fa8b4b",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let us showcase the idea of spectral filtering with a simple example with the karate club network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dbc7c3",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib as mpl\n",
    "\n",
    "G = ig.Graph.Famous(\"Zachary\")\n",
    "A = G.get_adjacency_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8b844",
   "metadata": {},
   "source": [
    "We will first compute the laplacian matrix and its eigendecomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a588971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Laplacian matrix\n",
    "deg = np.array(A.sum(axis=1)).reshape(-1)\n",
    "D = sparse.diags(deg)\n",
    "L = D - A\n",
    "\n",
    "# Compute eigendecomposition\n",
    "evals, evecs = np.linalg.eigh(L.toarray())\n",
    "\n",
    "# Sort eigenvalues and eigenvectors\n",
    "order = np.argsort(evals)\n",
    "evals = evals[order]\n",
    "evecs = evecs[:, order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa468f4",
   "metadata": {},
   "source": [
    "Now, let's create a low-pass and high-pass filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba04eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2\n",
    "L_low = evecs @ np.diag(1 / (1 + alpha * evals)) @ evecs.T\n",
    "L_high = evecs @ np.diag(alpha * evals / (1 + alpha * evals)) @ evecs.T\n",
    "\n",
    "print(\"Size of low-pass filter:\", L_low.shape)\n",
    "print(\"Size of high-pass filter:\", L_high.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94a687",
   "metadata": {},
   "source": [
    "Notice that the high-pass filter and low-pass filter are matrices of the same size as the adjacency matrix $A$, which defines a 'convolution' on the graph as follows:\n",
    "\n",
    "$$\n",
    "{\\bf x}' = {\\bf L}_{\\text{low}} {\\bf x} \\quad \\text{or} \\quad {\\bf x}' = {\\bf L}_{\\text{high}} {\\bf x}.\n",
    "$$\n",
    "\n",
    "where ${\\bf L}_{\\text{low}}$ and ${\\bf L}_{\\text{high}}$ are the low-pass and high-pass filters, respectively, and ${\\bf x}'$ is the convolved feature vector.\n",
    "\n",
    "Now, let's see how these filters work. Our first example is a random feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random feature vector\n",
    "x = np.random.randn(A.shape[0], 1)\n",
    "\n",
    "# Convolve with low-pass filter\n",
    "x_low = L_low @ x\n",
    "\n",
    "# Convolve with high-pass filter\n",
    "x_high = L_high @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbeaa0a",
   "metadata": {},
   "source": [
    "Let us visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712fc549",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "norm = mpl.colors.Normalize(vmin=-0.3, vmax=0.3)\n",
    "\n",
    "# Original\n",
    "values = x.reshape(-1)\n",
    "values /= np.linalg.norm(values)\n",
    "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[0])\n",
    "axes[0].set_title(\"Original\")\n",
    "\n",
    "# Low-pass filter applied\n",
    "values = L_low @ x\n",
    "values /= np.linalg.norm(values)\n",
    "values = values.reshape(-1)\n",
    "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[1])\n",
    "axes[1].set_title(\"Low-pass filter\")\n",
    "\n",
    "# High-pass filter applied\n",
    "values = L_high @ x\n",
    "values /= np.linalg.norm(values)\n",
    "values = values.reshape(-1)\n",
    "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[2])\n",
    "axes[2].set_title(\"High-pass filter\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf776ef",
   "metadata": {},
   "source": [
    "We observe that the low-pass filter results in smoother ${\\bf x}$ between connected nodes (i.e., neighboring nodes have similar ${\\bf x}$).\n",
    "The original ${\\bf x}$ and ${\\bf x}'_{\\text{low}}$ are very similar because random variables are high-frequency components. In contrast, when we apply the high-pass filter, ${\\bf x}'_{\\text{high}}$ is similar to ${\\bf x}$ because the high-frequency components are not filtered.\n",
    "\n",
    "Let's now use an eigenvector as our feature vector ${\\bf x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b756d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "eigen_centrality = np.array(G.eigenvector_centrality()).reshape(-1, 1)\n",
    "low_pass_eigen = L_low @ eigen_centrality\n",
    "high_pass_eigen = L_high @ eigen_centrality\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=-0, vmax=0.3)\n",
    "values = eigen_centrality.reshape(-1)# high_pass_random.reshape(-1)\n",
    "values /= np.linalg.norm(values)\n",
    "values = values.reshape(-1)\n",
    "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[0])\n",
    "axes[0].set_title(\"Original\")\n",
    "\n",
    "values = low_pass_eigen.reshape(-1)\n",
    "values /= np.linalg.norm(values)\n",
    "values = values.reshape(-1)\n",
    "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[1])\n",
    "axes[1].set_title(\"Low-pass filter\")\n",
    "\n",
    "values = high_pass_eigen.reshape(-1)\n",
    "values /= np.linalg.norm(values)\n",
    "ig.plot(G, vertex_color=[palette(norm(x)) for x in values], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[2])\n",
    "axes[2].set_title(\"High-pass filter\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733ff53",
   "metadata": {},
   "source": [
    "The high-pass filter increases the contrast of the eigenvector centrality, emphasizing the differences between nodes. On the other hand, the low-pass filter smooths out the eigenvector centrality.\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "\n",
    "# Graph Convolutional Networks\n",
    "We have seen that spectral filters give us a principled way to think about \"convolution\" on irregular graph structures, and controlling the frequency components brings out different aspects of the data. We now go one step further: instead of designing filters by hand, we can learn them from data for specific tasks.\n",
    "\n",
    "\n",
    "## Spectral Graph Convolutional Networks\n",
    "\n",
    "A simplest form of learnable spectral filter is given by\n",
    "\n",
    "$$\n",
    "{\\bf L}_{\\text{learn}} = \\sum_{k=1}^K \\theta_k {\\mathbf u}_k {\\mathbf u}_k^\\top,\n",
    "$$\n",
    "\n",
    "where ${\\mathbf u}_k$ are the eigenvectors and $\\theta_k$ are the learnable parameters. The variable $K$ is the number of eigenvectors used (i.e., the rank of the filter). The weight $\\theta_k$ is learned to maximize the performance of the task at hand.\n",
    "\n",
    "Building on this idea, {footcite}`bruna2014spectral` added a nonlinearity to the filter and proposed a spectral convolutional neural network (GCN) by\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(\\ell+1)} = h\\left( L_{\\text{learn}} {\\bf x}^{(\\ell)}\\right),\n",
    "$$\n",
    "\n",
    "where $h$ is an activation function, and ${\\bf x}^{(\\ell)}$ is the feature vector of the $\\ell$-th convolution. They further extend this idea to convolve on multidimensional feature vectors, ${\\bf X} \\in \\mathbb{R}^{N \\times f_{\\text{in}}}$ to produce new feature vectors of different dimensionality, ${\\bf X}' \\in \\mathbb{R}^{N \\times f_{\\text{out}}}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf X}^{(\\ell+1)}_i &= h\\left( \\sum_j L_{\\text{learn}}^{(i,j)} {\\bf X}^{(\\ell)}_j\\right),\\quad \\text{where} \\quad L^{(i,j)}_{\\text{learn}} = \\sum_{k=1}^K \\theta_{k, (i,j)} {\\mathbf u}_k {\\mathbf u}_k^\\top,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that the learnable filter $L_{\\text{learn}}^{(i,j)}$ is defined for each pair of input $i$ and output $j$ dimensions.\n",
    "\n",
    "\n",
    "```{note}\n",
    "Many GCNs simple when it comes to implementation despite the complicated formula. And this is one of my ways to learn GNNs. Check out the [Appendix for the Python implementation](appendix.md).\n",
    "\n",
    "```\n",
    "\n",
    "## From Spectral to Spatial\n",
    "\n",
    "Spectral GCNs are mathematically elegant but have two main limitations:\n",
    "1. **Computational Limitation**: Computing the spectra of the Laplacian is expensive ${\\cal O}(N^3)$ and prohibitive for large graphs\n",
    "2. **Spatial Locality**: The learned filters are not spatially localized. A node can be influenced by all other nodes in the graph.\n",
    "\n",
    "These two limitations motivate the development of spatial GCNs.\n",
    "\n",
    "### ChebNet\n",
    "\n",
    "ChebNet {footcite}`defferrard2016convolutional` is one of the earliest spatial GCNs that bridges the gap between spectral and spatial domains.\n",
    "The key idea is to leverage Chebyshev polynomials to approximate ${\\bf L}_{\\text{learn}}$ by\n",
    "\n",
    "$$\n",
    "{\\bf L}_{\\text{learn}} \\approx \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}), \\quad \\text{where} \\quad \\tilde{{\\bf L}} = \\frac{2}{\\lambda_{\\text{max}}}{\\bf L} - {\\bf I},\n",
    "$$\n",
    "\n",
    "where $\\tilde{{\\bf L}}$ is the scaled and normalized Laplacian matrix in order to have eigenvalues in the range of $[-1,1]$. The Chebyshev polynomials $T_k(\\tilde{{\\bf L}})$ transforms the eigenvalues $\\tilde{{\\bf L}}$ to the following recursively:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "T_0(\\tilde{{\\bf L}}) &= {\\bf I} \\\\\n",
    "T_1(\\tilde{{\\bf L}}) &= \\tilde{{\\bf L}} \\\\\n",
    "T_k(\\tilde{{\\bf L}}) &= 2\\tilde{{\\bf L}} T_{k-1}(\\tilde{{\\bf L}}) - T_{k-2}(\\tilde{{\\bf L}})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We then replace ${\\bf L}_{\\text{learn}}$ in the original spectral GCN with the Chebyshev polynomial approximation:\n",
    "\n",
    "$$\n",
    "{\\bf x}^{(\\ell+1)} = h\\left( \\sum_{k=0}^{K-1} \\theta_k T_k(\\tilde{{\\bf L}}){\\bf x}^{(\\ell)}\\right),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $T_k(\\tilde{{\\bf L}})$ applies the k-th Chebyshev polynomial to the scaled Laplacian matrix\n",
    "- $\\theta_k$ are the learnable parameters\n",
    "- K is the order of the polynomial (typically small, e.g., K=3)\n",
    "\n",
    "### Graph Convolutional Networks by Kipf and Welling\n",
    "\n",
    "While ChebNet offers a principled way to approximate spectral convolutions, Kipf and Welling (2017) {footcite}`kipf2017semi` proposed an even simpler and highly effective variant called **Graph Convolutional Networks (GCN)**.\n",
    "\n",
    "\n",
    "#### First-order Approximation\n",
    "\n",
    "The key departure is to use the first-order approximation of the Chebyshev polynomials.\n",
    "\n",
    "$$\n",
    "g_{\\theta'} * x \\approx \\theta'_0x + \\theta'_1(L - I_N)x = \\theta'_0x - \\theta'_1D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}x\n",
    "$$\n",
    "\n",
    "This is crude approximation but it leads to a much simpler form, leaving only two learnable parameters, instead of $K$ parameters in the original ChebNet.\n",
    "\n",
    "Additionally, they further simplify the formula by using the same $\\theta$ for both remaining parameters (i.e., $\\theta_0 = \\theta$ and $\\theta_1 = -\\theta$). The result is the following convolutional filter:\n",
    "\n",
    "$$\n",
    "g_{\\theta} * x \\approx \\theta(I_N + D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}})x\n",
    "$$\n",
    "\n",
    "While this is a very simple filter, one can stack multiple layers of convolutions to perform high-order graph convolutions.\n",
    "\n",
    "#### Deep GCNs can suffer from over-smoothing\n",
    "\n",
    "GCN models can be deep, and when they are too deep, they start suffering from an ill-posed problem called *gradient vanishing/exploding*, where the gradients of the loss function becomes too small or too large to update the model parameters. It is a common problem in deep learning.\n",
    "\n",
    "To facilitate the training of deep GCNs, the authors introduce a very simple trick called *renormalization*. The idea is to add self-connections to the graph:\n",
    "\n",
    "$$\n",
    "\\tilde{A} = A + I_N, \\quad \\text{and} \\quad \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}\n",
    "$$\n",
    "\n",
    "And use $\\tilde{A}$ and $\\tilde{D}$ to form the convolutional filter.\n",
    "\n",
    "Altogether, this leads to the following layer-wise propagation rule:\n",
    "\n",
    "$$X^{(\\ell+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X^{(\\ell)}W^{(\\ell)})$$\n",
    "\n",
    "where:\n",
    "- $X^{(\\ell)}$ is the matrix of node features at layer $\\ell$\n",
    "- $W^{(\\ell)}$ is the layer's trainable weight matrix\n",
    "- $\\sigma$ is a nonlinear activation function (e.g., ReLU)\n",
    "\n",
    "These simplifications offer several advantages:\n",
    "- **Efficiency**: Linear complexity in number of edges\n",
    "- **Localization**: Each layer only aggregates information from immediate neighbors\n",
    "- **Depth**: Fewer parameters allow building deeper models\n",
    "- **Performance**: Despite (or perhaps due to) its simplicity, it often outperforms more complex models\n",
    "\n",
    "```{admonition} Exercise\n",
    ":class: note\n",
    "\n",
    "Let's implement a simple GCN model for node classification.\n",
    "[Coding Exercise](../../../notebooks/exercise-m09-graph-neural-net.ipynb)\n",
    "```\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "```---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Preliminaries: Image Processing\n",
    "\n",
    "Graph Neural Networks are a type of neural network for graph data. node2vec and deepwalk stem from the idea of language modeling.\n",
    "In this module, we will focus on another branch of graph neural networks that stem from image processing.\n",
    "\n",
    "## Edge Detection Problem in Image Processing\n",
    "\n",
    "Edge detection is a classical problem in image processing. The goal is to identify the boundaries of objects in an image.\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20240616211411/Screenshot-(85).webp)\n",
    "\n",
    "To approach the problem, let us first remind that an image is a matrix of pixels. Each pixel has RGB values, each of which represents the intensity of red, green, and blue color. To simplify the problem, we focus on grayscale images, in which each pixel has only one value representing the brightness. In this case, an image can be represented as a 2D matrix, where each element in the matrix represents the brightness of a pixel.\n",
    "\n",
    "![](https://ai.stanford.edu/~syyeung/cvweb/Pictures1/imagematrix.png)\n",
    "\n",
    "### An example\n",
    "\n",
    "Human eyes are very sensitive to brightness changes. An edge in an image appears when there is a *significant brightness change between adjacent pixels*. To be more concrete, let's consider a small example consisting of 6x6 pixels, with a vertical line from the top to the bottom, where the brightness is higher than the neighboring pixels. This is an edge we want to detect.\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "10 & 10 & 80 & 10 & 10 & 10 \\\\\n",
    "10 & 10 & 80 & 10 & 10 & 10 \\\\\n",
    "10 & 10 & 80 & 10 & 10 & 10 \\\\\n",
    "10 & 10 & 80 & 10 & 10 & 10 \\\\\n",
    "10 & 10 & 80 & 10 & 10 & 10 \\\\\n",
    "10 & 10 & 80 & 10 & 10 & 10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's zoom on the pixel at (3, 3) and its surrounding pixels.\n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "10 & 80 & 10 \\\\\n",
    "\\textcolor{blue}{10} & \\textcolor{red}{80} & \\textcolor{purple}{10} \\\\\n",
    "10 & 80 & 10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where the central pixel is highlighted in red. Since we are interested in the edge which is a sudden change in brightness along the horizontal direction, we take a derivative at the central pixel by\n",
    "\n",
    "$$\n",
    "\\nabla Z_{22} = \\textcolor{blue}{Z_{2,1}} - \\textcolor{purple}{Z_{2,3}}\n",
    "$$\n",
    "\n",
    "Following the same process, we can compute the derivative at all pixels, which gives us the (horizontal) derivative of the image.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "- & -70 & 0 & 70 & 0 & - \\\\\n",
    "- & -70 & 0 & 70 & 0 & - \\\\\n",
    "- & -70 & 0 & 70 & 0 & - \\\\\n",
    "- & -70 & 0 & 70 & 0 & - \\\\\n",
    "- & -70 & 0 & 70 & 0 & -\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The symbol `-` indicates that the derivative is not defined because one of the neighboring pixels is out of the image boundary.\n",
    "We observe that the derivative is high at the edge and low elsewhere. This is a simple but effective way to detect edges in an image.\n",
    "\n",
    "We can consider a derivative operator along the vertical direction that computes the difference between the vertical neighboring pixels.\n",
    "\n",
    "$$\n",
    "\\nabla Z_{22} = Z_{1,2} - Z_{3,2}\n",
    "$$\n",
    "\n",
    "And, when applied to the entire image, the result is\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "- & - & - & - & -  & - \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "- & - & - & - & - & -\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The all entries are zero, meaning that there is no edge in the vertical direction.\n",
    "\n",
    "We can combine the horizontal and vertical derivatives to get the gradient of the image. For example,\n",
    "\n",
    "$$\n",
    "\\nabla Z_{22} = Z_{12} - Z_{32} + Z_{21} - Z_{23}\n",
    "$$\n",
    "\n",
    "When applied to the entire image, the result is the same as the horizontal derivative.\n",
    "\n",
    "### Convolution\n",
    "\n",
    "We observe that there is a repeated pattern in the derivative computation: we are taking addition and subtraction of neighbiring pixels. This motivates us to generalize the operation to a more general form.\n",
    "\n",
    "$$\n",
    "\\nabla Z_{22} = \\sum_{i=-1}^1 \\sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} Z_{2+i, 2+j}\n",
    "$$\n",
    "\n",
    "where $K$ is a $3 \\times 3$ matrix, and $w=h=3$ represent the width and height of the kernel.\n",
    "\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "K_{11} & K_{12} & K_{13} \\\\\n",
    "K_{21} & K_{22} & K_{23} \\\\\n",
    "K_{31} & K_{32} & K_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The matrix $K$ is called a **kernel**, and applying it to the image is called **convolution**.\n",
    "\n",
    "```{note}\n",
    "The index of the kernel is conventionally reversed. Namely, we reorder the entries of the kernel such that\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "K_{33} & K_{32} & K_{31} \\\\\n",
    "K_{23} & K_{22} & K_{21} \\\\\n",
    "K_{13} & K_{12} & K_{11}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, take the element-wise product with $Z$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Z_{11} K_{33} & Z_{12} K_{32} & Z_{13} K_{31} \\\\\n",
    "Z_{21} K_{23} & Z_{22} K_{22} & Z_{23} K_{21} \\\\\n",
    "Z_{31} K_{13} & Z_{32} K_{12} & Z_{33} K_{11}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and sum up all the elements to get the new pixel value $\\nabla Z_{22}$.\n",
    "Why do we reverse the kernel? This is to match with the mathematical definition of convolution, which will be introduced later.\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "In the previous example, we used a $3 \\times 3$ kernels called the Prewitt operator, which in terms of $K$ is\n",
    "\n",
    "$$\n",
    "K_h = \\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\quad \\text{or} \\quad\n",
    "K_v = \\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $K_h$ is the horizontal Prewitt operator and $K_v$ is the vertical Prewitt operator.\n",
    "```\n",
    "\n",
    "A kernel represents a local pattern we want to detect. The new pixel value after the convolution is maximized when the pattern is most similar to the kernel in terms of the inner product. This can be confirmed by:\n",
    "\n",
    "$$\n",
    "\\nabla Z_{22} = \\sum_{i=-1}^1 \\sum_{j=-1}^1 K_{h-(i+1),w-(j+1)} Z_{2+i, 2+j} = \\langle \\hat K, Z \\rangle\n",
    "$$\n",
    "\n",
    "where $\\langle \\cdot, \\cdot \\rangle$ is the inner product, and $\\hat K$ is the order-reversed kernel.\n",
    "\n",
    "```{note}\n",
    "Check out this awesome interactive demo to see how different kernels work: [Demo](https://setosa.io/ev/image-kernels/)\n",
    "```\n",
    "\n",
    "## Fourier Transform\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*D6iRfzDkz-sEzyjYoVZ73w.gif)\n",
    "\n",
    "Convolution computes the new pixel values by sliding a kernel over an image. How is the resulting image related to the original image?\n",
    "\n",
    "\n",
    "\n",
    "To answer this question, let us consider a row of an image and convolve it with a kernel $K$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X &= \\begin{bmatrix}\n",
    "X_1 & X_2 & X_3 & X_4 & X_5 & X_6\n",
    "\\end{bmatrix} \\\\\n",
    "K &= \\begin{bmatrix}\n",
    "K_1 & K_2 & K_3\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The convolution of $X$ and $K$ is\n",
    "\n",
    "$$\n",
    "X * K = \\begin{bmatrix}\n",
    "X_1 K_3 + X_2 K_2 + X_3 K_1 & X_2 K_3 + X_3 K_2 + X_4 K_1 & X_3 K_3 + X_4 K_2 + X_5 K_1 & X_4 K_3 + X_5 K_2 + X_6 K_1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "...which is complicated, right? 😅 So let's make it simple by using a useful theorem called **the convolution theorem**.\n",
    "\n",
    "The convolution theorem gives us a simpler way to think about convolution. Instead of doing the complex sliding window operation in the original domain (like pixel values), we can:\n",
    "\n",
    "1. Transform both signals to the frequency domain using Fourier transform\n",
    "2. Multiply them together (much simpler!)\n",
    "3. Transform back to get the same result\n",
    "\n",
    "Mathematically, the above steps can be written as:\n",
    "\n",
    "1. $\\mathcal{F}(X), \\mathcal{F}(K)$ - Transform both signals to frequency domain (Fourier transform)\n",
    "2. $\\mathcal{F}(X) \\cdot \\mathcal{F}(K)$ - Multiply the transformed signals\n",
    "3. $\\mathcal{F}^{-1}(\\mathcal{F}(X) \\cdot \\mathcal{F}(K))$ - Transform back to get $X * K$\n",
    "\n",
    "where $\\mathcal{F}^{-1}$ is the inverse Fourier transform that brings us back to the original domain. This is much easier than computing the convolution directly!\n",
    "\n",
    "For a discrete signal $x[n]$ with $N$ points, the Fourier transform $\\mathcal{F}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-2\\pi i \\frac{nk}{N}}\n",
    "$$\n",
    "\n",
    "where $i$ is the imaginary unit. Or equivalently,\n",
    "\n",
    "$$\n",
    "\\mathcal{F}(x)[k] = \\sum_{n=0}^{N-1} x[n] \\cdot \\left[ \\cos\\left(2\\pi \\frac{nk}{N}\\right) - i \\sin\\left(2\\pi \\frac{nk}{N}\\right) \\right]\n",
    "$$\n",
    "\n",
    "using Euler's formula $e^{ix} = \\cos(x) + i\\sin(x)$.\n",
    "\n",
    "```{note}\n",
    "Complex number can be thought of as a way to represent a 2D vector using a single value (which is a computer science perspective; mathematically, it is a bit more subtle). For example, $e^{i\\pi/2} = \\cos(\\pi/2) + i\\sin(\\pi/2)$ represents the 2D vector $(\\cos(\\pi/2), \\sin(\\pi/2))$. In the context of Fourier transform, we interpret $e^{-2\\pi i \\frac{nk}{N}}$ as two *base waves*, i.e., sine and cosine, with phase $\\frac{2\\pi k}{N}$.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Euler%27s_formula.svg/360px-Euler%27s_formula.svg.png)\n",
    "```\n",
    "\n",
    "In simple terms, $\\mathcal{F}$ takes a signal (like our row of pixel values) and breaks it down into sine and cosine waves of different frequencies. Each frequency component $k$ tells us \"how much\" of that frequency exists in our original signal.\n",
    "Don't worry too much about the complex math. The key idea is that $\\mathcal{F}$ represents a signal as a sum of multiple waves with different frequencies, so we can understand the signal in terms of its frequencies rather than its original values.\n",
    "\n",
    "\n",
    "![](https://devincody.github.io/Blog/post/an_intuitive_interpretation_of_the_fourier_transform/img/FFT-Time-Frequency-View_hu24c1c8fe894ecd0dad24174b2bed08c9_99850_800x0_resize_lanczos_2.png)\n",
    "\n",
    "\n",
    "```{note}\n",
    "3Blue1Brown makes a beautiful video explaining Fourier transform: [Video](https://www.youtube.com/watch?v=spUNpyF58BY). Here is a great interactive demo on Fourier transform by Jez Swanson: [Demo](https://www.jezzamon.com/fourier/).\n",
    "```\n",
    "\n",
    "### An example for the Fourier transform\n",
    "\n",
    "Now, let's perform the convolution using the Fourier transform using an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a794ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([10, 10, 80, 10, 10, 10])\n",
    "K = np.array([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e100a3",
   "metadata": {},
   "source": [
    "Let us first perform the convolution directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99012489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad X with zeros on both sides to handle boundary\n",
    "n_conv = len(X) - len(K) + 1  # Now we get full length output\n",
    "XKconv = np.zeros(n_conv)\n",
    "\n",
    "for i in range(n_conv):\n",
    "    XKconv[i] = np.sum(X[i:(i+len(K))] * K[::-1]) # Reverse the kernel and take element-wise product and sum up\n",
    "XKconv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdfe72",
   "metadata": {},
   "source": [
    "Let us now perform the convolution using the Fourier transform. We compute the Fourier transform of $X$ and $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Transform X and K to frequency domain\n",
    "FX = np.fft.fft(X)\n",
    "# Pad K with zeros to match the length of X before FFT\n",
    "K_padded = np.pad(K, (0, len(X) - len(K)), 'constant') # [-1  0  1  0  0  0]\n",
    "FK = np.fft.fft(K_padded)\n",
    "print(\"FX:\", FX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c849df3",
   "metadata": {},
   "source": [
    "- We add zeros to $K$ to make it the same length as $X$ before applying the Fourier transform. This is necessary because the convolution theorem requires the signals to have the same length.\n",
    "- `FX` is the Fourier transform of $X$, which is a complex number. Each entry $FX[k]$ represents the weight of the cosine wave in its real part and the weight of the sine wave in its imaginary part, with phase $2\\pi k / N$. Similarly for `FK`.\n",
    "\n",
    "Next, we multiply the transformed signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc24f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FXKconv = FX * FK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9728e9",
   "metadata": {},
   "source": [
    "This is the convolution in the frequency domain. Finally, we transform back to get the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "XKconv_ft = np.real(np.fft.ifft(FXKconv))\n",
    "XKconv_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc080a5c",
   "metadata": {},
   "source": [
    "- We take the real part. The imaginary part is due to numerical artifacts that do not matter in practice.\n",
    "- The Fourier transform convolution produces a longer output than direct convolution because it includes partial overlaps between K and X at the boundaries. Since we only want the full overlaps, we need to truncate the first two elements of `XKconv_ft` (as K has length 3) to match the length of the direct convolution result.\n",
    "- For example, let's look at what happens at the beginning of the convolution:\n",
    "  - At position -2: Only the last element of K overlaps with X: `[0, 0, 10] * [-1, 0, 1] = 10`\n",
    "  - At position -1: Two elements of K overlap with X: `[0, 10, 10] * [-1, 0, 1] = 10`\n",
    "  - At position 0: Full overlap begins: `[10, 10, 80] * [-1, 0, 1] = 70`\n",
    "\n",
    "  The Fourier transform method gives us all these positions (-2, -1, 0, ...), but we only want the full overlaps starting from position 0, which is why we truncate the first two elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XKconv_ft = XKconv_ft[2:]\n",
    "XKconv_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fd089",
   "metadata": {},
   "source": [
    "This gives us the same result as the direct convolution up to numerical errors.\n",
    "\n",
    "## Fourier Transform of Images\n",
    "\n",
    "Let's extend the above example to an image which is a 2D matrix.\n",
    "The idea is the same: we take the Fourier transform of each row and column of the image, and then multiply them together to get the convolution in the frequency domain.\n",
    "More specifically, for an image $X$ with size $H \\times W$, the Fourier transform of $X$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{F}(X)[h, w] &= \\sum_{k=0}^{H-1} \\sum_{\\ell=0}^{W-1} X[k, \\ell] \\cdot e^{-2\\pi i \\frac{hk}{H}} \\cdot e^{-2\\pi i \\frac{w\\ell}{W}} \\\\\n",
    "&= \\sum_{k=0}^{H-1} \\sum_{\\ell=0}^{W-1} X[k, \\ell] e^{-2\\pi i \\left(\\frac{hk}{H} + \\frac{w\\ell}{W}\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Comparing with the 1D case, we see that the 2D Fourier transform is *functionally* the same as the 1D Fourier transform, except that we now have two indices $h$ and $w$ to represent the frequency in both dimensions.\n",
    "The basis waves are 2D waves as shown below.\n",
    "\n",
    "**Cosine waves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0204c36",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def basis_function(img_size=256, u=0, v=0):\n",
    "  '''\n",
    "  img_size : square size of image f(x,y)\n",
    "  u,v : spatial space indice\n",
    "  '''\n",
    "  N = img_size\n",
    "  x = np.linspace(0, N-1, N)\n",
    "  y = np.linspace(0, N-1, N)\n",
    "  x_, y_ = np.meshgrid(x, y)\n",
    "  bf = np.exp(-1j*2*np.pi*(u*x_/N+v*y_/N))\n",
    "  if u == 0 and v == 0:\n",
    "    bf = np.round(bf)\n",
    "  real = np.real(bf) # The cosine part\n",
    "  imag = np.imag(bf) # The sine part\n",
    "  return real, imag\n",
    "\n",
    "size = 16\n",
    "bf_arr_real = np.zeros((size*size,size,size))\n",
    "bf_arr_imag = np.zeros((size*size,size,size))\n",
    "ind = 0\n",
    "for col in range(size):\n",
    "  for row in range(size):\n",
    "    re,imag = basis_function(img_size=size, u=row, v=col)\n",
    "    bf_arr_real[ind] = re\n",
    "    bf_arr_imag[ind] = imag\n",
    "    ind += 1\n",
    "\n",
    "# real part\n",
    "_, axs = plt.subplots(size, size, figsize=(7, 7))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(bf_arr_real, axs):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a7799",
   "metadata": {},
   "source": [
    "**Sine waves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4d00c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# imaginary part\n",
    "_, axs = plt.subplots(size, size, figsize=(7, 7))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(bf_arr_imag, axs):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b121f",
   "metadata": {},
   "source": [
    "The Fourier transform of an image is a decomposition of an image into the sum of these basis waves.\n",
    "\n",
    "### An example of Fourier transform\n",
    "\n",
    "Let us apply the Fourier transform to an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b6f4bb",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read image from URL\n",
    "def read_jpeg_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # Convert to RGB mode if needed (in case it's RGBA)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "def image_to_numpy(img):\n",
    "    return np.array(img)\n",
    "\n",
    "def to_gray_scale(img_np):\n",
    "    return np.mean(img_np, axis=2)\n",
    "\n",
    "# URL of the image\n",
    "url = \"https://www.binghamton.edu/news/images/uploads/features/20180815_peacequad02_jwc.jpg\"\n",
    "\n",
    "img = read_jpeg_from_url(url)\n",
    "img_np = image_to_numpy(img)\n",
    "img_gray = to_gray_scale(img_np)\n",
    "\n",
    "plt.imshow(img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4191a18",
   "metadata": {},
   "source": [
    "Take the Fourier transform of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_img_gray = np.fft.fft2(img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb281b6b",
   "metadata": {},
   "source": [
    "This decomposes the image into a sum of basis waves. Let's see the weights of the basis waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d4d7a",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "weight = np.abs(ft_img_gray)\n",
    "\n",
    "# real part\n",
    "fig1, ax1 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "ax1.imshow(weight, cmap='gray', norm=matplotlib.colors.LogNorm(), aspect='equal')\n",
    "cbar = fig1.colorbar(ax1.images[0], ax=ax1, orientation='horizontal')\n",
    "cbar.set_label('Fourier transform magnitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f9d61",
   "metadata": {},
   "source": [
    "The corresponding basis waves look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c9b1b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "size = 16\n",
    "bf_arr_real = np.zeros((size*size,size,size))\n",
    "bf_arr_imag = np.zeros((size*size,size,size))\n",
    "ind = 0\n",
    "for col in range(-size//2, size//2):\n",
    "  for row in range(-size//2, size//2):\n",
    "    re,imag = basis_function(img_size=size, u=row, v=col)\n",
    "    bf_arr_real[ind] = re\n",
    "    bf_arr_imag[ind] = imag\n",
    "    ind += 1\n",
    "\n",
    "# real part\n",
    "fig, axs = plt.subplots(size, size, figsize=(7, 7))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(bf_arr_real, axs):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img,cmap='gray')\n",
    "\n",
    "fig.suptitle('Real Part of Basis Functions')\n",
    "\n",
    "\n",
    "# imaginary part\n",
    "fig, axs = plt.subplots(size, size, figsize=(7, 7))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(bf_arr_imag, axs):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img,cmap='gray')\n",
    "\n",
    "fig.suptitle('Imaginary Part of Basis Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d80a9",
   "metadata": {},
   "source": [
    "Now, let's see the convolution of the image with a Prewitt operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e182cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]) # Prewitt operator\n",
    "\n",
    "K_padd = np.zeros((img_gray.shape[0], img_gray.shape[1]))\n",
    "K_padd[:K.shape[0], :K.shape[1]] = K\n",
    "\n",
    "# convolution\n",
    "FK = np.fft.fft2(K_padd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740dc0d4",
   "metadata": {},
   "source": [
    "The Fourier transform of the Prewitt operator looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(FK), cmap='gray')\n",
    "cbar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe95594",
   "metadata": {},
   "source": [
    "We can think of the frequency domain of the kernel as a **filter** that suppresses some frequencies and allows others to pass through. In the example of the Prewitt operator, the kernel `FK` has a low value around the center of the image. The product $FX \\cdot FK$ then suppresses the low-frequency components of the image, and we are left with the high-frequency components that correspond to the horizontal edges. We can think of this as a high-pass filter that only allows high-frequency components to pass through.\n",
    "\n",
    "Let's see the convolution result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31072cc8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "FX = np.fft.fft2(img_gray)\n",
    "conv_img_gray = np.real(np.fft.ifft2(FX * FK))\n",
    "plt.imshow(conv_img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7182d",
   "metadata": {},
   "source": [
    "We observe that the horizontal edges are highlighted.\n",
    "\n",
    "```{note}\n",
    "A widespread application of the 2D Fourier transform is JPEG format. Here's how it works:\n",
    "\n",
    "(1) It first breaks the image into small 8x8 squares.\n",
    "(2) It converts each square into frequencies using the Discrete Cosine Transform. The sine part is discarded for compression.\n",
    "(3) It keeps the important low frequencies that our eyes can see well.\n",
    "(4) It throws away most of the high frequencies that our eyes don't notice much.\n",
    "\n",
    "These steps make the file much smaller while still looking good to us.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## A key lesson from image processing\n",
    "\n",
    "We have seen an equivalence between convolution in the pixel (spatial) domain and multiplication in the frequency domain.\n",
    "Using the Fourier transform, an image is decomposed into a sum of basis waves.\n",
    "The *kernel* can be thought of as *a filter* that suppresses some basis waves and allows others to pass through.\n",
    "\n",
    "This idea is the key to understand graph convolutional networks we will see in the next page.\n",
    "\n",
    "# Pen and paper exercises\n",
    "\n",
    "- [✍️ Pen and paper exercises](pen-and-paper/exercise.pdf)\n",
    "---\n",
    "jupytext:\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3\n",
    "---\n",
    "\n",
    "# Popular Graph Neural Networks\n",
    "\n",
    "In this note, we will introduce three popular GNNs: GraphSAGE, Graph Attention Networks (GAT), and Graph Isomorphism Network (GIN).\n",
    "\n",
    "## GraphSAGE: Sample and Aggregate\n",
    "\n",
    "GraphSAGE {footcite}`hamilton2017graphsage` introduced a different GCN that can be ***generalized to unseen nodes*** (they called it \"inductive\"). While previous approaches like ChebNet and GCN operate on the entire graph, GraphSAGE proposes an inductive framework that generates embeddings by sampling and aggregating features from a node's neighborhood.\n",
    "\n",
    "![](https://theaisummer.com/static/02e23adc75fe68e5dd249a94f3c1e8cc/c483d/graphsage.png)\n",
    "\n",
    "### Key Ideas\n",
    "\n",
    "GraphSAGE involves two key ideas: (1) sampling and (2) aggregation.\n",
    "\n",
    "#### Neighborhood Sampling\n",
    "\n",
    "The key idea is the *neighborhood sampling*. Instead of using all neighbors, GraphSAGE samples a fixed-size set of neighbors for each node. This controls memory complexity, a key limitation of the previous GNNs.\n",
    "\n",
    "Another key advantage of neighborhood sampling is that it enables GraphSAGE to handle dynamic, growing networks. Consider a citation network where new papers (nodes) are continuously added. Traditional GCNs would need to recompute filters for the entire network with each new addition. In contrast, GraphSAGE can immediately generate embeddings for new nodes by simply sampling their neighbors, without any retraining or recomputation.\n",
    "\n",
    "#### Aggregation\n",
    "\n",
    "Another key idea is the *aggregation*. GraphSAGE makes a distinction between self-information and neighborhood information. While previous GNNs treat them equally and aggregate them, GraphSAGE treats them differently. Specifically, GraphSAGE introduces an additional step: it concatenates the self-information and the neighborhood information as the input of the convolution.\n",
    "\n",
    "$$\n",
    "Z_v = \\text{CONCAT}(X_v, X_{\\mathcal{N}(v)})\n",
    "$$\n",
    "\n",
    "where $X_v$ is the feature of the node itself and $X_{\\mathcal{N}(v)}$ is the aggregation of the features of its neighbors. GraphSAGE introduces different ways to aggregate information from neighbors:\n",
    "\n",
    "   $$X_{\\mathcal{N}(v)} = \\text{AGGREGATE}_k(\\{X_u, \\forall u \\in \\mathcal{N}(v)\\})$$\n",
    "\n",
    "   Common aggregation functions include:\n",
    "   - Mean aggregator: $\\text{AGGREGATE} = \\text{mean}(\\{h_u, \\forall u \\in \\mathcal{N}(v)\\})$\n",
    "   - Max-pooling: $\\text{AGGREGATE} = \\max(\\{\\sigma(W_{\\text{pool}}h_u + b), \\forall u \\in \\mathcal{N}(v)\\})$\n",
    "   - LSTM aggregator: Apply LSTM to randomly permuted neighbors\n",
    "\n",
    "The concatenated feature $Z_v$ is normalized by the L2 norm.\n",
    "\n",
    "$$\n",
    "\\hat{Z}_v = \\frac{Z_v}{\\|Z_v\\|_2}\n",
    "$$\n",
    "\n",
    "and then fed into the convolution.\n",
    "\n",
    "$$\n",
    "X_v^k = \\sigma(W^k \\hat{Z}_v + b^k)\n",
    "$$\n",
    "\n",
    "## Graph Attention Networks (GAT): Differentiate Individual Neighbors\n",
    "\n",
    "A key innovation of GraphSAGE is to treat the self and neighborhood information differently. But should all neighbors be treated equally? Graph Attention Networks (GAT) address this by letting the model learn which neighbors to pay attention to.\n",
    "\n",
    "\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_7.55.32_PM_vkdDcDx.png)\n",
    "\n",
    "The core idea is beautifully simple: instead of using fixed weights like GCN, let's learn attention weights $\\alpha_{ij}$ that determine how much node $i$ should attend to node $j$. These weights are computed dynamically based on node features:\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "where $e_{ij}$ represents the importance of the edge between node $i$ and node $j$. Variable $e_{ij}$ is a *learnable* parameter and can be negative, and the exponential function is applied to transform it to a non-negative value, with the normalization term $\\sum_{k \\in \\mathcal{N}(i)} \\exp(e_{ik})$ to ensure the weights sum to 1.\n",
    "\n",
    "How to compute $e_{ij}$? One simple choice is to use a neural network with a shared weight matrix $W$ and a LeakyReLU activation function. Specifically:\n",
    "\n",
    "1. Let's focus on computing $e_{ij}$ for node $i$ and its neighbor $j$.\n",
    "2. We use a shared weight matrix $W$ to transform the features of node $i$ and $j$.\n",
    "   $$\n",
    "   \\mathbf{\\tilde h}_i  = \\mathbf{h}_i, \\quad \\mathbf{\\tilde h}_j  = W\\mathbf{h}_j\n",
    "   $$\n",
    "3. We concatenate the transformed features and apply a LeakyReLU activation function.\n",
    "\n",
    "$$\n",
    "e_{ij} = \\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{\\tilde h}_i, \\mathbf{\\tilde h}_j])\n",
    "$$\n",
    "\n",
    "where $\\mathbf{a}$ is a trainable parameter vector that sums the two transformed features.\n",
    "\n",
    "Once we have these attention weights, the node update is straightforward - just a weighted sum of neighbor features:\n",
    "\n",
    "$$\\mathbf{h}'_i = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}{\\bf W}_{\\text{feature}}\\mathbf{h}_j\\right)$$\n",
    "\n",
    "where ${\\bf W}_{\\text{feature}}$ is a trainable weight matrix. To stabilize training, GAT uses multiple attention heads and concatenates their outputs:\n",
    "\n",
    "$$\\mathbf{h}'_i = \\parallel_{k=1}^K \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i) \\cup \\{i\\}} \\alpha_{ij}^k{\\bf W}^k_{\\text{feature}}\\mathbf{h}_j\\right)$$\n",
    "\n",
    "## Graph Isomorphism Network (GIN): Differentiate the Aggregation\n",
    "\n",
    "Graph Isomorphism Networks (GIN) is another popular GNN that born out of a question: what is the maximum discriminative power achievable by Graph Neural Networks? The answer lies in its theoretical connection to **the Weisfeiler-Lehman (WL) test**, a powerful algorithm for graph isomorphism testing.\n",
    "\n",
    "\n",
    "### Weisfeiler-Lehman Test\n",
    "\n",
    "Are two graphs structurally identical? Graph isomorphism testing determines if two graphs are structurally identical, with applications in graph classification, clustering, and other tasks.\n",
    "\n",
    "![](https://i.sstatic.net/j5sGu.png)\n",
    "\n",
    "While the general problem has no known polynomial-time solution, the WL test is an efficient heuristic that works well in practice. The WL test iteratively refines node labels by hashing the multiset of neighboring labels\n",
    "\n",
    "\n",
    "![](../figs/weisfeiler-lehman-test.jpg)\n",
    "\n",
    "The WL test works as follows:\n",
    "\n",
    "1. Assign all nodes the same initial label.\n",
    "2. For each node, collect the labels of all its neighbors and *aggregate them* into a hash (e.g., new label). For example, the top node gets {0} from its neighbors, resulting in a collection {0,0}. A new label is created via a hash function $h$ that maps {0, {0, 0}} to a new label 1.\n",
    "3. Repeat the process for a fixed number of iterations or until convergence.\n",
    "\n",
    "Here is the implementation of the WL test in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2efb1e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def weisfeiler_lehman_test(A, num_iterations):\n",
    "    n_nodes = A.shape[0]\n",
    "    labels = np.zeros(n_nodes, dtype=int)\n",
    "    color_map = {}\n",
    "    hash_fn = lambda x: color_map.setdefault(x, len(color_map))\n",
    "    for _ in range(num_iterations):\n",
    "\n",
    "        # Go through each node\n",
    "        labels_old = labels.copy()\n",
    "        for i in range(n_nodes):\n",
    "\n",
    "            # Collect the labels of all neighbors\n",
    "            neighbors = A[i].nonzero()[1]\n",
    "            neighbor_labels = labels_old[neighbors]\n",
    "\n",
    "            # Count the frequency of each label\n",
    "            unique, counts = np.unique(neighbor_labels, return_counts=True)\n",
    "\n",
    "            # Create a hash key by converting the frequency dictionary to a string\n",
    "            hash_key = str({unique[j]: counts[j] for j in range(len(unique))})\n",
    "\n",
    "            # Create a new label by hashing the frequency dictionary\n",
    "            label = hash_fn(hash_key)\n",
    "            labels[i] = label\n",
    "\n",
    "        # Check convergence\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        unique_old, counts_old = np.unique(labels_old, return_counts=True)\n",
    "        if np.array_equal(np.sort(counts), np.sort(counts_old)):\n",
    "            break\n",
    "    return labels\n",
    "\n",
    "\n",
    "edge_list = [(0, 1), (1, 2), (2, 0), (3, 4), (4, 5), (5, 3)]\n",
    "\n",
    "A = sparse.csr_matrix(\n",
    "    ([1] * len(edge_list), ([e[0] for e in edge_list], [e[1] for e in edge_list])),\n",
    "    shape=(6, 6),\n",
    ")\n",
    "A = A + A.T\n",
    "A.sort_indices()\n",
    "\n",
    "weisfeiler_lehman_test(A, A.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b5b798",
   "metadata": {},
   "source": [
    "After these iterations:\n",
    "- Nodes with the same label are structurally identical, meaning that they are indistinguishable unless we label them differently.\n",
    "- Two graphs are structurally identical if and only if they have the same node labels after the WL test.\n",
    "\n",
    "The WL test is a heuristic and can fail on some graphs. For example, it cannot distinguish regular graphs with the same number of nodes and edges.\n",
    "\n",
    "```{note}\n",
    "The WL test above is called the 1-WL test. There are higher-order WL tests that can distinguish more graphs, which are the basis of advanced GNNs.\n",
    "Check out [this note](https://www.moldesk.net/blog/weisfeiler-lehman-isomorphism-test/)\n",
    "```\n",
    "\n",
    "### GIN\n",
    "\n",
    "GIN {footcite}`xu2018how` is a GNN that is based on the WL test.\n",
    "The key idea is to focus on the parallel between the WL test and the GNN update rule.\n",
    "- In the WL test, we iteratively collect the labels of neighbors and aggregate them through a *hash function*.\n",
    "- In the GraphSAGE and GAT, the labels are the nodes' features, and the aggregation is some arithmetic operations such as mean or max.\n",
    "\n",
    "The key difference is that the hash function in the WL test always distinguishes different sets of neighbors' labels, while the aggregation in GraphSAGE and GAT does not always do so. For example, if all nodes have the same feature (e.g., all 1), the aggregation by the mean or max will result in the same value for all nodes, whereas the hash function in the WL test can still distinguish different sets of neighbors' labels by *the count of each label*.\n",
    "\n",
    "The resulting convolution update rule is:\n",
    "\n",
    "$$\n",
    "h_v^{(k+1)} = \\text{MLP}^{(k)}\\left((1 + \\epsilon^{(k)}) \\cdot h_v^{(k)} + \\sum_{u \\in \\mathcal{N}(v)} h_u^{(k)}\\right)\n",
    "$$\n",
    "\n",
    "where $\\text{MLP}^{(k)}$ is a multi-layer perceptron (MLP) with $k$ layers, and $\\epsilon^{(k)}$ is a fixed or trainable parameter.\n",
    "\n",
    "\n",
    "\n",
    "```{footbibliography}\n",
    "\n",
    "```\n",
    "# Module 9: Graph Neural Networks\n",
    "\n",
    "## What to learn in this module\n",
    "\n",
    "In this module, we will learn how to use neural networks to learn representations of graphs. We will learn:\n",
    "- Fourier transform on image\n",
    "- Fourier transform on graph\n",
    "- Spectral filters\n",
    "- Graph convolutional networks\n",
    "- Popular GNNs (GCN, GAT, GraphSAGE, and GIN)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}