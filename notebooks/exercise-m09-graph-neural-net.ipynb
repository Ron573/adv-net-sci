{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skojaku/adv-net-sci/blob/main/notebooks/exercise-m09-graph-neural-net.ipynb)\n",
    "\n",
    "\n",
    "# Exercise M09: Graph Neural Networks\n",
    "\n",
    "## Image Processing\n",
    "\n",
    "Let's perform the Fourier transform on an image.  \n",
    "For an image $X$ with size $H \\times W$, the Fourier transform of $X$ is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{F}(X)[h, w] &= \\sum_{k=0}^{H-1} \\sum_{\\ell=0}^{W-1} X[k, \\ell] \\cdot e^{-2\\pi i \\frac{hk}{H}} \\cdot e^{-2\\pi i \\frac{w\\ell}{W}} \\\\\n",
    "&= \\sum_{k=0}^{H-1} \\sum_{\\ell=0}^{W-1} X[k, \\ell] e^{-2\\pi i \\left(\\frac{hk}{H} + \\frac{w\\ell}{W}\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The exponential term $e^{-2\\pi i \\left(\\frac{hk}{H} + \\frac{w\\ell}{W}\\right)}$ represents a 2D wave with frequency $(h, w)$, which looks like the following: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def basis_function(img_size=256, u=0, v=0):\n",
    "  '''\n",
    "  img_size : square size of image f(x,y)\n",
    "  u,v : spatial space indice\n",
    "  '''\n",
    "  N = img_size\n",
    "  x = np.linspace(0, N-1, N)\n",
    "  y = np.linspace(0, N-1, N)\n",
    "  x_, y_ = np.meshgrid(x, y)\n",
    "  bf = np.exp(-1j*2*np.pi*(u*x_/N+v*y_/N))\n",
    "  if u == 0 and v == 0:\n",
    "    bf = np.round(bf)\n",
    "  real = np.real(bf) # The cosine part\n",
    "  imag = np.imag(bf) # The sine part\n",
    "  return real, imag\n",
    "\n",
    "size = 16\n",
    "bf_arr_real = np.zeros((size*size,size,size))\n",
    "bf_arr_imag = np.zeros((size*size,size,size))\n",
    "ind = 0\n",
    "for col in range(size):\n",
    "  for row in range(size):\n",
    "    re,imag = basis_function(img_size=size, u=row, v=col)\n",
    "    bf_arr_real[ind] = re\n",
    "    bf_arr_imag[ind] = imag\n",
    "    ind += 1\n",
    "\n",
    "# real part\n",
    "_, axs = plt.subplots(size, size, figsize=(4, 4))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(bf_arr_real, axs):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note ü§î**: It is common to reorder the basis functions such that the lowest frequency components are at the center, which looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_arr_real = np.zeros((size*size,size,size))\n",
    "bf_arr_imag = np.zeros((size*size,size,size))\n",
    "ind = 0\n",
    "for col in range(-size//2, size//2):\n",
    "  for row in range(-size//2, size//2):\n",
    "    re,imag = basis_function(img_size=size, u=row, v=col)\n",
    "    bf_arr_real[ind] = re\n",
    "    bf_arr_imag[ind] = imag\n",
    "    ind += 1\n",
    "\n",
    "# real part\n",
    "fig, axs = plt.subplots(size, size, figsize=(4, 4))\n",
    "axs = axs.flatten()\n",
    "for img, ax in zip(bf_arr_real, axs):\n",
    "  ax.set_axis_off()\n",
    "  ax.imshow(img,cmap='gray')\n",
    "\n",
    "fig.suptitle('Real Part of Basis Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, let's perform the Fourier transform on an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read image from URL\n",
    "def read_jpeg_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # Convert to RGB mode if needed (in case it's RGBA)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "def image_to_numpy(img):\n",
    "    return np.array(img)\n",
    "\n",
    "def to_gray_scale(img_np):\n",
    "    return np.mean(img_np, axis=2)\n",
    "\n",
    "# URL of the image. You can change this to any image you want.\n",
    "url = \"https://www.binghamton.edu/news/images/uploads/features/20180815_peacequad02_jwc.jpg\"\n",
    "\n",
    "img = read_jpeg_from_url(url)\n",
    "img_np = image_to_numpy(img)\n",
    "img_gray = to_gray_scale(img_np)\n",
    "\n",
    "plt.imshow(img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the Fourier transform of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_img_gray = np.fft.fft2(img_gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decomposes the image into a sum of basis waves. Let's see the weights of the basis waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "weight = np.abs(ft_img_gray)\n",
    "\n",
    "# real part\n",
    "fig1, ax1 = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "ax1.imshow(weight, cmap='gray', norm=matplotlib.colors.LogNorm(), aspect='equal')\n",
    "cbar = fig1.colorbar(ax1.images[0], ax=ax1, orientation='horizontal')\n",
    "cbar.set_label('Fourier transform magnitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brighter the pixel is, the more dominant the corresponding basis wave is in the image. We observed that there are some high frequency components, which correspond to the edges of the image.\n",
    "\n",
    "Now, let's see the convolution of the image with a Prewitt operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array(\n",
    "    [\n",
    "        [-1, -1, -1],\n",
    "        [0, 0, 0],\n",
    "        [1, 1, 1],\n",
    "    ]\n",
    ")  # Prewitt operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operator is used to detect the horizontal edges of the image. \n",
    "Let's compute the Fourier transform of the Prewitt operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_padd = np.zeros((img_gray.shape[0], img_gray.shape[1]))\n",
    "K_padd[:K.shape[0], :K.shape[1]] = K\n",
    "\n",
    "# convolution\n",
    "FK = np.fft.fft2(K_padd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note ü§î**: We have padded the operator to the same size as the image, which is the required to perform the convolution. Now, let's see the weights of the basis waves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(FK), cmap='gray')\n",
    "cbar = plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the low frequency components are suppressed, and the high frequency components are enhanced. This is because the Prewitt operator is a high-pass filter that only allows high-frequency components to pass through.\n",
    "\n",
    "\n",
    "We now perform the convolution of the image with the Prewitt operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FX = np.fft.fft2(img_gray)\n",
    "conv_img_gray = np.real(np.fft.ifft2(FX * FK))\n",
    "plt.imshow(conv_img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 üèãÔ∏è:\n",
    "\n",
    "Design your own kernel to:\n",
    "1. Detect the vertical edges of the image.\n",
    "2. Smooth the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Kernel to detect the vertical edges of the image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Kernel to smooth the image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Graph Convolutional Networks\n",
    "\n",
    "We can think of a convolution of an image from the perspective of networks.\n",
    "In the convolution of an image, a pixel is convolved with its *neighbors*. We can regard each pixel as a node, and each node is connected to its neighboring nodes (pixels) that are involved in the convolution.\n",
    "\n",
    "![](https://av-eks-lekhak.s3.amazonaws.com/media/__sized__/article_images/conv_graph-thumbnail_webp-600x300.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that each node has a variable $x_i \\in \\mathbb{R}$, just like each pixel has a value in a grey image. Consider a network of $N$ such nodes. \n",
    "\n",
    "In this example, we use the karate club network and set $x$ by a random gaussian variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib as mpl\n",
    "\n",
    "G = ig.Graph.Famous(\"Zachary\")\n",
    "A = G.get_adjacency_sparse()\n",
    "\n",
    "x = np.random.randn(G.vcount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We define the *total variation* of ${\\mathbf x}$ as the sum of the squared differences between connected nodes:\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N A_{ij}(x_i - x_j)^2 = {\\bf x}^\\top {\\bf L} {\\bf x},\n",
    "$$\n",
    "\n",
    "where ${\\bf L}$ is the Laplacian matrix of the graph given by\n",
    "\n",
    "$$\n",
    "L_{ij} = \\begin{cases}\n",
    "-1 & \\text{if } i \\text{ and } j \\text{ are connected} \\\\\n",
    "k_i & \\text{if } i = j \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "and ${\\bf x} = [x_1,x_2,\\ldots, x_N]^\\top$ is a column vector of feature variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the total variation of x by definition\n",
    "J = 0\n",
    "for i in range(G.vcount()):\n",
    "  for j in range(G.vcount()):\n",
    "    if A[i,j] != 0:\n",
    "      J += (x[i] - x[j])**2\n",
    "J = J / 2\n",
    "\n",
    "# Compute the total variation of x by using the Laplacian matrix\n",
    "deg = A.sum(axis = 1).A1\n",
    "L = sparse.diags(deg) - A\n",
    "J_by_laplacian = x.T @ L @ x\n",
    "\n",
    "print(f\"J: {J}, J_by_laplacian: {J_by_laplacian}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decompose the total variation into high-frequency and low-frequency components by using the eigenvectors ${\\bf u}_i$ and the eigenvalues $\\lambda_i$ of the Laplacian matrix:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^N \\lambda_i  {\\bf x}^\\top {\\mathbf u}_i {\\mathbf u}_i^\\top {\\bf x} = \\sum_{i=1}^N \\lambda_i  ||{\\bf x}^\\top {\\mathbf u}_i||^2.\n",
    "$$\n",
    "\n",
    "The term $({\\bf x}^\\top {\\mathbf u}_i)$ is a dot-product between the feature vector ${\\bf x}$ and the eigenvector ${\\mathbf u}_i$, measuring how much ${\\bf x}$ aligns with ${\\mathbf u}_i$, similar to Fourier coefficients with sinusoids. Each $||{\\bf x}^\\top {\\mathbf u}_i||^2$ represents the ''strength'' of ${\\bf x}$ with respect to ${\\mathbf u}_i$, making the total variation $J$ a weighted sum of these strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the eigenvalues and eigenvectors of the Laplacian matrix\n",
    "eigvals, eigvecs = np.linalg.eigh(L.toarray())\n",
    "\n",
    "# Sort the eigenvalues and eigenvectors\n",
    "sorted_indices = np.argsort(eigvals)\n",
    "eigvals = eigvals[sorted_indices]\n",
    "eigvecs = eigvecs[:, sorted_indices]\n",
    "\n",
    "strength = []\n",
    "for i in range(len(eigvals)):\n",
    "  strength.append(np.sum((x.T @ eigvecs[:,i])**2))\n",
    "\n",
    "J_by_eig = np.sum(strength * eigvals)\n",
    "print(f\"J: {J}, J_by_eig: {J_by_eig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the \"frequency\" of each component in increasing order of eigenvalues (frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x=range(len(eigvals)), y=strength)\n",
    "\n",
    "ax.set_xlabel(\"Eigenvalue index\")\n",
    "\n",
    "ax.set_ylabel(\"Strength\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues $\\lambda_i$ are then multipled by the strength of the corresponding component, and the total is the total variation $J$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x=range(len(eigvals)), y=strength * eigvals)\n",
    "\n",
    "ax.set_xlabel(\"Eigenvalue index\")\n",
    "\n",
    "ax.set_ylabel(\"Strength * Eigenvalue\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can consider \"eigenvals\" as a filter that controls which frequency components pass through üòâ. For example, if we want to keep the low-frequency components, we can set the high-frequency components to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigvals_filtered = eigvals.copy()\n",
    "eigvals_filtered[:10] *= 10\n",
    "eigvals_filtered[10:] = 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.barplot(x=range(len(eigvals)), y=strength * eigvals_filtered)\n",
    "\n",
    "ax.set_xlabel(\"Eigenvalue index\")\n",
    "\n",
    "ax.set_ylabel(\"Strength * Filtered Eigenvalue\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this filter, we reconstruct the Laplacian matrix.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_filtered = eigvecs @ np.diag(eigvals_filtered) @ eigvecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].set_title(\"Original Laplacian\")\n",
    "axes[1].set_title(\"Reconstructed Laplacian from filtered eigenvalues\")\n",
    "sns.heatmap(L.toarray(), cmap = 'viridis', ax=axes[0])\n",
    "sns.heatmap(L_filtered, cmap = 'viridis', ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new \"convolution\" matrix $L_{\\text{filtered}}$ with which to generate a new feature vector ${\\bf x}'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prime = L_filtered @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), )\n",
    "palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "norm = mpl.colors.Normalize(vmin=-0.3, vmax=0.3)\n",
    "\n",
    "ig.plot(G, vertex_color=[palette(norm(_x)) for _x in x], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[0])\n",
    "axes[0].set_title(\"Original\")\n",
    "\n",
    "ig.plot(G, vertex_color=[palette(norm(_x)) for _x in x_prime], bbox=(0, 0, 500, 500), vertex_size=20, target=axes[1])\n",
    "axes[1].set_title(\"High-pass filter\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the values of the nodes are smoothed out, since the high-frequency components are suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 üèãÔ∏è:\n",
    "\n",
    "Design your own filter to detect the high-frequency components of the graph. And apply the filter to:\n",
    "\n",
    "1. Random gaussian variables \n",
    "2. Eigenvector centrality of the graph.\n",
    "\n",
    "Then, compare the results with the original ones.\n",
    "\n",
    "**Construct your own filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the filter to random gaussian variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the filter to eigenvector centrality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Networks\n",
    "\n",
    "Let's implement a simple Graph Convolutional Network (GCN) by Kipf & Welling. We will use the karate club network again. To this end, we will use PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with multiple features for each node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.eye(G.vcount(), 5) # one-hot encoding of the node indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN updates node features by:\n",
    "\n",
    "$$\n",
    "x' = \\sigma\\left( \\tilde {\\mathbf A} X \\Theta\\right)\n",
    "$$\n",
    "\n",
    "where \n",
    "1. $X$ is the feature matrix of the graph,\n",
    "2. $\\sigma$ is an activation function, and \n",
    "3. $\\Theta$ is a learnable parameter matrix.\n",
    "4. $\\tilde {\\mathbf A}$ is the normalized adjacency matrix of the graph (with self-loops).\n",
    "\n",
    "Let's break down the formula. First, we will compute $\\tilde {\\mathbf A}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will multiply it by feature matrix $X$ and learnable parameter matrix $\\Theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat_norm_X = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the learnable parameters and apply it to the feature matrix. We initialize the learnable parameters by random gaussian variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta = torch.nn.Parameter(torch.randn(X.shape[1], 5), requires_grad=True)  # The new feature x' has 5 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And multiply it by $\\tilde {\\mathbf A} X$, and apply the activation function $\\sigma$. We will use the sigmoid function as the activation function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat_norm_X_Theta = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize the updated feature matrix. Each row is a new feature vector of a node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat_norm_X_Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the updated feature matrix in 2D space using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "xy = PCA(n_components=2).fit_transform(A_hat_norm_X_Theta.detach().numpy())\n",
    "\n",
    "\n",
    "# We color the nodes by the membership of the karate club members.\n",
    "import networkx as nx\n",
    "Gnx = nx.karate_club_graph()\n",
    "labels = np.unique([d[1]['club'] for d in Gnx.nodes(data=True)], return_inverse=True)[1]\n",
    "\n",
    "\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1], hue = labels,  ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the nodes are separated into two clusters, despite the fact that the GCN is **untrained!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the GCN to predict the membership of the karate club. Namely, the GCN is given the feature matrix $X$ and the adjacency matrix $A$, and it learns the parameter matrix $\\Theta$ to predict the membership of the karate club. We will split the nodes into the training and testing sets, and evaluate the accuracy of the model using the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the GCN model by putting the above code into a class.\n",
    "class GCN(torch.nn.Module):\n",
    "  def __init__(self, in_features, out_features, A):\n",
    "    super(GCN, self).__init__()\n",
    "    self.linear = torch.nn.Parameter(torch.randn(in_features, out_features), requires_grad=True)\n",
    "\n",
    "    A_hat = A + sparse.eye(A.shape[0])\n",
    "    deg = A_hat.sum(axis=1).A1\n",
    "    D_hat = sparse.diags(deg)\n",
    "    D_hat_inv = sparse.diags(1 / deg)\n",
    "    self.A_hat_norm = D_hat_inv @ A_hat\n",
    "\n",
    "  def forward(self, x):\n",
    "    Ax = torch.tensor(self.A_hat_norm @ x, dtype=torch.float32)\n",
    "    return torch.nn.functional.sigmoid(Ax @ self.linear)\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(in_features=A.shape[0], out_features=2, A = A)\n",
    "\n",
    "# Initialize the feature matrix\n",
    "X = torch.eye(A.shape[0])\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_idx, test_idx = train_test_split(np.arange(G.vcount()), test_size=0.2, random_state=42)\n",
    "labels = torch.tensor(labels)\n",
    "train_features = X[train_idx]\n",
    "train_labels = labels[train_idx]\n",
    "test_features = X[test_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "# Define the number of training epochs\n",
    "n_train = 200\n",
    "\n",
    "# Initialize the progress bar\n",
    "pbar = tqdm(range(n_train))\n",
    "\n",
    "# Initialize the loss history\n",
    "loss_history = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in pbar:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    loss = criterion(output[train_idx], train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        accuracy = (predicted[test_idx] == test_labels).float().mean()\n",
    "        loss_history.append(loss.item())\n",
    "        pbar.set_postfix(loss=loss.item(), accuracy=accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreases as the training progresses, which indicates that the model is learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.lineplot(x=range(n_train), y=loss_history, ax=ax)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = model(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "xy = PCA(n_components=2).fit_transform(xy.detach().numpy())\n",
    "\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1], hue = labels,  ax=ax, legend = False)\n",
    "ax.set_title(\"Learned feature matrix\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 üèãÔ∏è:\n",
    "\n",
    "1. Build a two-layer GCN and train it to predict the membership of the karate club.\n",
    "2. Compare the performance of the two-layer GCN with the one-layer GCN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advnetsci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
